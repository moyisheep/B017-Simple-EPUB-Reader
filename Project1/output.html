<!DOCTYPE html><html><head>
<title>2. Imaging Toolkit</title>
<meta/>
<link type="text/css" href="../styles/stylesheet.css"/>

<></>
<script type="text/javascript">var gPosition = 0;
var gProgress = 0;
var gCurrentPage = 0;
var gPageCount = 0;
var gClientHeight = null;

const kMaxFont = 0;

function getPosition()
{
	return gPosition;
}

function getProgress()
{
	return gProgress;
}

function getPageCount()
{
	return gPageCount;
}

function getCurrentPage()
{
	return gCurrentPage;
}

/**
 * Setup the columns and calculate the total page count;
 */

function setupBookColumns()
{
	var body = document.getElementsByTagName(&apos;body&apos;)[0].style;
	body.marginLeft = 0;
	body.marginRight = 0;
	body.marginTop = 0;
	body.marginBottom = 0;
	
    var bc = document.getElementById(&apos;book-columns&apos;).style;
    bc.width = (window.innerWidth * 2) + &apos;px !important&apos;;
	bc.height = (window.innerHeight-kMaxFont) + &apos;px !important&apos;;
    bc.marginTop = &apos;0px !important&apos;;
    bc.webkitColumnWidth = window.innerWidth + &apos;px !important&apos;;
    bc.webkitColumnGap = &apos;0px&apos;;
	bc.overflow = &apos;visible&apos;;

	gCurrentPage = 1;
	gProgress = gPosition = 0;
	
	var bi = document.getElementById(&apos;book-inner&apos;).style;
	bi.marginLeft = &apos;0px&apos;;
	bi.marginRight = &apos;0px&apos;;
	bi.padding = &apos;0&apos;;

	gPageCount = document.body.scrollWidth / window.innerWidth;

	// Adjust the page count to 1 in case the initial bool-columns.clientHeight is less than the height of the screen. We only do this once.2

	if (gClientHeight &lt; (window.innerHeight-kMaxFont)) {
		gPageCount = 1;
	}
}

/**
 * Columnize the document and move to the first page. The position and progress are reset/initialized
 * to 0. This should be the initial pagination request when the document is initially shown.
 */

function paginate()
{	
	// Get the height of the page. We do this only once. In setupBookColumns we compare this
	// value to the height of the window and then decide wether to force the page count to one.
	
	if (gClientHeight == undefined) {
		gClientHeight = document.getElementById(&apos;book-columns&apos;).clientHeight;
	}
	
	setupBookColumns();
}

/**
 * Paginate the document again and maintain the current progress. This needs to be used when
 * the content view changes size. For example because of orientation changes. The page count
 * and current page are recalculated based on the current progress.
 */

function paginateAndMaintainProgress()
{
	var savedProgress = gProgress;
	setupBookColumns();
	goProgress(savedProgress);
}

/**
 * Update the progress based on the current page and page count. The progress is calculated
 * based on the top left position of the page. So the first page is 0% and the last page is
 * always below 1.0.
 */

function updateProgress()
{
	gProgress = (gCurrentPage - 1.0) / gPageCount;
}

/**
 * Move a page back if possible. The position, progress and page count are updated accordingly.
 */

function goBack()
{
	if (gCurrentPage &gt; 1)
	{
		gCurrentPage--;
		gPosition -= window.innerWidth;
		window.scrollTo(gPosition, 0);
		updateProgress();
	}
}

/**
 * Move a page forward if possible. The position, progress and page count are updated accordingly.
 */

function goForward()
{
	if (gCurrentPage &lt; gPageCount)
	{
		gCurrentPage++;
		gPosition += window.innerWidth;
		window.scrollTo(gPosition, 0);
		updateProgress();
	}
}

/**
 * Move directly to a page. Remember that there are no real page numbers in a reflowed
 * EPUB document. Use this only in the context of the current document.
 */

function goPage(pageNumber)
{
	if (pageNumber &gt; 0 &amp;&amp; pageNumber &lt;= gPageCount)
	{
		gCurrentPage = pageNumber;
		gPosition = (gCurrentPage - 1) * window.innerWidth;
		window.scrollTo(gPosition, 0);
		updateProgress();
	}
}

/**
 * Go the the page with respect to progress. Assume everything has been setup.
 */

function goProgress(progress)
{
	progress += 0.0001;
	
	var progressPerPage = 1.0 / gPageCount;
	var newPage = 0;
	
	for (var page = 0; page &lt; gPageCount; page++) {
		var low = page * progressPerPage;
		var high = low + progressPerPage;
		if (progress &gt;= low &amp;&amp; progress &lt; high) {
			newPage = page;
			break;
		}
	}
		
	gCurrentPage = newPage + 1;
	gPosition = (gCurrentPage - 1) * window.innerWidth;
	window.scrollTo(gPosition, 0);
	updateProgress();		
}

//Set font family
function setFontFamily(newFont) {
	document.body.style.fontFamily = newFont + &quot; !important&quot;;
	paginateAndMaintainProgress();
}

//Sets font size to a relative size
function setFontSize(toSize) {
	document.getElementById(&apos;book-inner&apos;).style.fontSize = toSize + &quot;em !important&quot;;
	paginateAndMaintainProgress();
}

//Sets line height relative to font size
function setLineHeight(toHeight) {
	document.getElementById(&apos;book-inner&apos;).style.lineHeight = toHeight + &quot;em !important&quot;;
	paginateAndMaintainProgress();
}

//Enables night reading mode
function enableNightReading() {
	document.body.style.backgroundColor = &quot;#000000&quot;;
	var theDiv = document.getElementById(&apos;book-inner&apos;);
	theDiv.style.color = &quot;#ffffff&quot;;
	
	var anchorTags;
	anchorTags = theDiv.getElementsByTagName(&apos;a&apos;);
	
	for (var i = 0; i &lt; anchorTags.length; i++) {
		anchorTags[i].style.color = &quot;#ffffff&quot;;
	}
}</script>
<style></style>

</head>
<body>
<section>
<header>
<h2 id="ch2" class="CHAPTER">
<samp class="chapter-number"><span id="pg_11"><span id="kobo.1.1" class="koboSpan">2</span></span></samp>
<samp class="chapter-title"><span id="kobo.2.1" class="koboSpan">Imaging Toolkit</span></samp>
</h2>
</header>
<p class="noindent"><span id="kobo.3.1" class="koboSpan">In this chapter, our goal is to develop an understanding of the digital image formation model that is central to most current imaging devices. </span><span id="kobo.3.2" class="koboSpan">Building on the foundations of the image formation model and the various parameters associated with it allows us to understand the limitations of the conventional imaging pipeline. </span><span id="kobo.3.3" class="koboSpan">This becomes critical in subsequent chapters as we see how the computational imaging philosophy helps us go beyond what is possible with conventional imaging.</span></p>
<section>
<h3 id="sec1" class="head a-head"><a id="sec2-1"><span id="kobo.4.1" class="koboSpan">2.1 Optics</span></a></h3><a id="sec2-1">
</a><section><a id="sec2-1">
</a><h4 id="sec2" class="head b-head"><a id="sec2-1"></a><a id="sec2-1-1"><span id="kobo.5.1" class="koboSpan">2.1.1 Animal Eyes</span></a></h4><a id="sec2-1-1">
<p class="noindent"><span id="kobo.6.1" class="koboSpan">The human eye is a very sophisticated image-capturing device. </span><span id="kobo.6.2" class="koboSpan">It uses a lens to focalize the light reflected by an object onto the retina, which is made up of photosensitive cells called cones and rods. </span><span id="kobo.6.3" class="koboSpan">The eye’s functioning principle is similar to that of a modern camera. </span><span id="kobo.6.4" class="koboSpan">However, during its evolutionary development, the animal eye was not always this complex. </span><span id="kobo.6.5" class="koboSpan">An earlier anatomy, still found in animals such as the marine mollusk called a nautilus, is the rudimentary pinhole eye, which is simply a sphere with a tiny hole in front and a layer of photoreceptors on the opposite side.</span></p>
</a></section><a id="sec2-1-1">
</a><section><a id="sec2-1-1">
</a><h4 id="sec3" class="head b-head"><a id="sec2-1-1"></a><a id="sec2-1-2"><span id="kobo.7.1" class="koboSpan">2.1.2 Light, Waves, and Particles</span></a></h4><a id="sec2-1-2">
<p class="noindent"><span id="kobo.8.1" class="koboSpan">A light ray is modeled as a line describing the trace that a photon might leave behind. </span><span id="kobo.8.2" class="koboSpan">When capturing an image, each pixel captures the color of a ray of light. </span><span id="kobo.8.3" class="koboSpan">Therefore, the image allows us to detect the environment by mapping visible external points to points on the camera sensor.</span></p>
<p><span id="kobo.9.1" class="koboSpan">However, light rays are used not only for measuring the environment; they can also be used to investigate optical systems (e.g., the lens surface or its coating). </span><span id="kobo.9.2" class="koboSpan">The light is generally attenuated from source to destination. </span><span id="kobo.9.3" class="koboSpan">The </span><i><span id="kobo.10.1" class="koboSpan">reversibility</span></i><span id="kobo.11.1" class="koboSpan"> property of light rays means that the overall attenuation for a ray does not change when the source and destination are swapped.</span></p>
<p><span id="pg_12"><span id="kobo.12.1" class="koboSpan">Conceptually, light rays are infinitesimal in width and have an infinitesimal point of emergence. </span><span id="kobo.12.2" class="koboSpan">Therefore, measuring a single ray is challenging. </span><span id="kobo.12.3" class="koboSpan">To better understand light, we need to look at several models that describe it.</span></span></p>
<p><span id="kobo.13.1" class="koboSpan">Firstly, light can be described as an electromagnetic wave. </span><span id="kobo.13.2" class="koboSpan">Therefore all light frequencies, from low-frequency radio signals to high-frequency cosmic rays, propagate through a vacuum at a constant rate</span></p>
<p class="DIS-IMG"><span id="kobo.14.1" class="koboSpan"><img class="img1" src="../images/pg12-1.png"/></span></p>
<p class="noindent"><span id="kobo.15.1" class="koboSpan">The frequency </span><i><span><span id="kobo.16.1" class="koboSpan">ν</span></span></i><span id="kobo.17.1" class="koboSpan"> and wavelength </span><i><span><span id="kobo.18.1" class="koboSpan">λ</span></span></i><span id="kobo.19.1" class="koboSpan"> are linked via the equation</span></p>
<p class="DIS-IMG"><span id="kobo.20.1" class="koboSpan"><img class="img1" src="../images/pg12-2.png"/></span></p>
<p class="noindent"><span id="kobo.21.1" class="koboSpan">such that rays with high frequency, such as gamma rays, have a very small wavelength. </span><span id="kobo.21.2" class="koboSpan">It is important to point out that the light we typically measure propagates at speeds smaller than </span><i><span id="kobo.22.1" class="koboSpan">c</span></i><span id="kobo.23.1" class="koboSpan"> because it is obstructed by the surrounding matter particles.</span></p>
<p><span id="kobo.24.1" class="koboSpan">The wavelength has tremendous impact on the way the light wave propagates. </span><span id="kobo.24.2" class="koboSpan">For instance, when we drive underneath a bridge, we can always see our surroundings; however, the AM radio signal is likely to flicker. </span><span id="kobo.24.3" class="koboSpan">This is caused by the difference in wavelength between the two electromagnetic waves. </span><span id="kobo.24.4" class="koboSpan">The visible light has low wavelength compared to the bridge opening and therefore passes unobstructed. </span><span id="kobo.24.5" class="koboSpan">The wavelength of radio signals, however, is too large and therefore our antenna picks up only a noisy residue.</span></p>
<p><span id="kobo.25.1" class="koboSpan">Whereas light was initially viewed as a waveform, Albert Einstein showed for the first time that light can be quantized as a stream of photon particles. </span><span id="kobo.25.2" class="koboSpan">Modeling light as a wave is convenient on a macroscopic scale, but more complex processing, such as analyzing light interference caused by diffraction through lenses, requires using the particle light model. </span><span id="kobo.25.3" class="koboSpan">This is useful if we are trying to understand, for instance, the maximum image resolution achieved with a given lens and why this is dependent on the lens size. </span><span id="kobo.25.4" class="koboSpan">In this course, light is primarily viewed using the wave or ray models.</span></p>
<p><span id="kobo.26.1" class="koboSpan">In empty space, photons are well described by the </span><i><span id="kobo.27.1" class="koboSpan">ray model</span></i><span id="kobo.28.1" class="koboSpan">: single photon traces that do not interact with each other. </span><span id="kobo.28.2" class="koboSpan">Their wave-like behavior emerges in closed environments, such as when passing through a pinhole of a size comparable to the wavelength. </span><span id="kobo.28.3" class="koboSpan">As we will discuss subsequently, this causes diffraction, a wave-specific phenomenon.</span></p>
<p><span id="kobo.29.1" class="koboSpan">The energy transported by a single photon is measured</span></p>
<p class="DIS-IMG"><span id="kobo.30.1" class="koboSpan"><img class="img1" src="../images/pg12-3.png"/></span></p>
<p class="noindent"><span id="kobo.31.1" class="koboSpan">where </span><i><span id="kobo.32.1" class="koboSpan">h</span></i><span id="kobo.33.1" class="koboSpan"> = 6.62610 × 10</span><sup><span id="kobo.34.1" class="koboSpan">−34</span></sup><span id="kobo.35.1" class="koboSpan"> J · Hz</span><sup><span id="kobo.36.1" class="koboSpan">−1</span></sup><span id="kobo.37.1" class="koboSpan"> denotes Planck’s constant, </span><i><span><span id="kobo.38.1" class="koboSpan">ν</span></span></i><span id="kobo.39.1" class="koboSpan"> is the frequency, and </span><i><span><span id="kobo.40.1" class="koboSpan">λ</span></span></i><span id="kobo.41.1" class="koboSpan"> is the wavelength.</span></p>
<p><span id="kobo.42.1" class="koboSpan">This means that higher frequency photons carry significantly more energy. </span><span id="kobo.42.2" class="koboSpan">That is why higher frequency light (such as ultraviolet and x-ray) is more dangerous and can damage our bodies. </span><span id="kobo.42.3" class="koboSpan">One single photon carries an insignificant amount of energy (e.g., 4 × 10</span><sup><span id="kobo.43.1" class="koboSpan">−18</span></sup><span id="kobo.44.1" class="koboSpan"> J) </span><span id="pg_13"><span id="kobo.45.1" class="koboSpan">for visible light. </span><span id="kobo.45.2" class="koboSpan">Interestingly, when the human eyes are fully adapted to darkness, our rod cells can detect light bursts as small as eight to ten photons (Hecht et al., 1942).</span></span></p>
</a></section><a id="sec2-1-2">
</a><section><a id="sec2-1-2">
</a><h4 id="sec4" class="head b-head"><a id="sec2-1-2"></a><a id="sec2-1-3"><span id="kobo.46.1" class="koboSpan">2.1.3 Measuring Light with Rays</span></a></h4><a id="sec2-1-3">
<p class="noindent"><span id="kobo.47.1" class="koboSpan">The light is affected by a range of factors such as</span></p>
<ul class="bullet">
<li class="BL"><span id="kobo.48.1" class="koboSpan">the power transmitted,</span></li>
<li class="BL"><span id="kobo.49.1" class="koboSpan">direction of radiation,</span></li>
<li class="BL"><span id="kobo.50.1" class="koboSpan">area of real or imaginary surfaces,</span></li>
<li class="BL"><span id="kobo.51.1" class="koboSpan">wavelength, and</span></li>
<li class="BL"><span id="kobo.52.1" class="koboSpan">visibility.</span></li>
</ul>
<p><span id="kobo.53.1" class="koboSpan">The power transmitted is measured by the </span><i><span id="kobo.54.1" class="koboSpan">radiant flux</span></i> <span><span id="kobo.55.1" class="koboSpan">Φ</span></span><span id="kobo.56.1" class="koboSpan">, which is defined as the energy emitted, reflected, transmitted, or received, per unit time, and is measured in watts, or J/s.</span></p>
</a><p><a id="sec2-1-3"><span id="kobo.57.1" class="koboSpan">We introduce a simplified model of a light source called the ideal </span><i><span id="kobo.58.1" class="koboSpan">point source</span></i><span id="kobo.59.1" class="koboSpan"> light, which is infinitesimal in size and radiates light outward uniformly in all directions. </span><span id="kobo.59.2" class="koboSpan">The point source is described by a radiant flux </span><span><span id="kobo.60.1" class="koboSpan">Φ</span></span><span id="kobo.61.1" class="koboSpan">. </span><span id="kobo.61.2" class="koboSpan">Let us consider an imaginary sphere centered in the point source, shown in </span></a><a id="rfig2-1" href="chapter_2.xhtml#fig2-1"><span id="kobo.62.1" class="koboSpan">Figure 2.1</span></a><span id="kobo.63.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-1"><span id="kobo.64.1" class="koboSpan"><img class="img2" src="../images/Figure2-1.png"/></span>
</a><figcaption><a id="fig2-1"></a><p class="CAP"><a id="fig2-1"><span class="FIGN"></span></a><a href="#rfig2-1"><span id="kobo.65.1" class="koboSpan">Figure 2.1</span></a> <span class="FIG"><span id="kobo.66.1" class="koboSpan">The ideal point source: a point source with radiant flux </span><span><span id="kobo.67.1" class="koboSpan">Φ</span></span><span id="kobo.68.1" class="koboSpan"> and the irradiance/exitance for an imaginary sphere.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.69.1" class="koboSpan">The point source has the following properties:</span></p>
<ul class="bullet">
<li class="BL"><span id="kobo.70.1" class="koboSpan">All the rays in the point source arrive perpendicularly on the imaginary sphere at the same time.</span></li>
<li class="BL"><span id="pg_14"><span id="kobo.71.1" class="koboSpan">There is a one-to-one mapping between the sphere points and rays: every point on the sphere has a corresponding ray.</span></span></li>
<li class="BL"><span id="kobo.72.1" class="koboSpan">The rays therefore form a continuum and their number is uncountably infinite (similar to the number of real values).</span></li>
<li class="BL"><span id="kobo.73.1" class="koboSpan">The radiant flux is transmitted equally across the sphere’s surface.</span></li>
</ul>
<p class="noindent"><span id="kobo.74.1" class="koboSpan">Thus each ray emitted by the point source carries 0 W, and one can measure only a 2D beam containing an uncountably infinite number of rays.</span></p>
<p><span id="kobo.75.1" class="koboSpan">The </span><b><span id="kobo.76.1" class="koboSpan">irradiance</span></b><span id="kobo.77.1" class="koboSpan"> is subsequently introduced to define the radiant flux incident to an area on the sphere for an ideal point source</span></p>
<p class="DIS-IMG"><span id="kobo.78.1" class="koboSpan"><img class="img1" src="../images/pg14-1.png"/></span></p>
<p class="noindent"><span id="kobo.79.1" class="koboSpan">where </span><span><span id="kobo.80.1" class="koboSpan">Φ</span></span><span id="kobo.81.1" class="koboSpan"> is the radiant flux, and </span><i><span id="kobo.82.1" class="koboSpan">r</span></i><span id="kobo.83.1" class="koboSpan"> is the sphere radius. </span><span id="kobo.83.2" class="koboSpan">The irradiance is measured in W/m</span><sup><span id="kobo.84.1" class="koboSpan">2</span></sup><span id="kobo.85.1" class="koboSpan"> and is inversely proportional to the square of the sphere radius. </span><span id="kobo.85.2" class="koboSpan">For example, increasing the radius ten times leads to an irradiance 100 times smaller for the new sphere. </span><span id="kobo.85.3" class="koboSpan">Given that the irradiance describes a particular spatial area on the sphere, we say that it measures the </span><i><span id="kobo.86.1" class="koboSpan">spatial power density</span></i><span id="kobo.87.1" class="koboSpan">.</span></p>
<p><span id="kobo.88.1" class="koboSpan">The irradiance can be introduced in a more general context, where the surface is not necessarily a sphere. </span><span id="kobo.88.2" class="koboSpan">The value at a point on the surface is</span></p>
<p class="DIS-IMG"><span id="kobo.89.1" class="koboSpan"><img class="img1" src="../images/pg14-2.png"/></span></p>
<p><span id="kobo.90.1" class="koboSpan">For the radiance leaving the surface of interest we introduce the </span><b><span id="kobo.91.1" class="koboSpan">exitance</span></b> <i><span id="kobo.92.1" class="koboSpan">M</span></i><span id="kobo.93.1" class="koboSpan">, measured in W/m</span><sup><span id="kobo.94.1" class="koboSpan">2</span></sup><span id="kobo.95.1" class="koboSpan">.</span></p>
<p><span id="kobo.96.1" class="koboSpan">In addition to the spatial power density, a thorough description of light requires introducing a way to measure the </span><i><span id="kobo.97.1" class="koboSpan">angular power density</span></i><span id="kobo.98.1" class="koboSpan">. </span><span id="kobo.98.2" class="koboSpan">In other words, we need to describe the radiant flux inside a beam of light. </span><span id="kobo.98.3" class="koboSpan">A 3D beam of light requires introducing a generalization of the 2D angle known as the </span><i><span id="kobo.99.1" class="koboSpan">solid angle</span></i><span id="kobo.100.1" class="koboSpan">.</span></p>
<p><span id="kobo.101.1" class="koboSpan">Assume that we have a cone-shaped beam of light. </span><span id="kobo.101.2" class="koboSpan">The unit measure for the solid angle is the </span><i><span id="kobo.102.1" class="koboSpan">steradian</span></i><span id="kobo.103.1" class="koboSpan">, which is defined by a cone with the vertex in the center of a sphere of radius </span><i><span id="kobo.104.1" class="koboSpan">r</span></i><span id="kobo.105.1" class="koboSpan"> whose base delimits a spherical </span><i><span id="kobo.106.1" class="koboSpan">cap</span></i><span id="kobo.107.1" class="koboSpan"> of area </span><i><span id="kobo.108.1" class="koboSpan">r</span></i><sup><span id="kobo.109.1" class="koboSpan">2</span></sup><span id="kobo.110.1" class="koboSpan">. </span><span id="kobo.110.2" class="koboSpan">The diagram of a steradian is depicted in </span><a id="rfig2-2" href="chapter_2.xhtml#fig2-2"><span id="kobo.111.1" class="koboSpan">Figure 2.2</span></a><span id="kobo.112.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-2"><span id="kobo.113.1" class="koboSpan"><img class="img3" src="../images/Figure2-2.png"/></span>
</a><figcaption><a id="fig2-2"></a><p class="CAP"><a id="fig2-2"><span class="FIGN"></span></a><a href="#rfig2-2"><span id="kobo.114.1" class="koboSpan">Figure 2.2</span></a> <span class="FIG"><span id="kobo.115.1" class="koboSpan">The steradian and the solid angle of a cone-shaped beam.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.116.1" class="koboSpan">We now introduce the general solid angle </span><span><span id="kobo.117.1" class="koboSpan">Ω</span></span></p>
<p class="DIS-IMG"><span id="kobo.118.1" class="koboSpan"><img class="img1" src="../images/pg14-3.png"/></span></p>
<p class="noindent"><span id="kobo.119.1" class="koboSpan">where sr stands for steradians and </span><i><span><span id="kobo.120.1" class="koboSpan">α</span></span></i><span id="kobo.121.1" class="koboSpan"> represents the half of the top planar angle of a cross-section of the solid angle, shown in </span><a href="chapter_2.xhtml#fig2-2"><span id="kobo.122.1" class="koboSpan">Figure 2.2</span></a><span id="kobo.123.1" class="koboSpan">.</span></p>
<p><span id="kobo.124.1" class="koboSpan">We point out that the solid angle corresponding to a whole sphere is </span><span><span id="kobo.125.1" class="koboSpan">Ω</span></span><span id="kobo.126.1" class="koboSpan"> = 2</span><i><span><span id="kobo.127.1" class="koboSpan">π</span></span></i><span id="kobo.128.1" class="koboSpan">(1 − cos (</span><i><span><span id="kobo.129.1" class="koboSpan">π</span></span></i><span id="kobo.130.1" class="koboSpan">)) = 4</span><i><span><span id="kobo.131.1" class="koboSpan">π</span></span></i><span id="kobo.132.1" class="koboSpan"> sr. </span><span id="kobo.132.2" class="koboSpan">We can now introduce the </span><b><span id="kobo.133.1" class="koboSpan">radiant intensity</span></b><span id="kobo.134.1" class="koboSpan">, which measures the angular power density. </span><span id="kobo.134.2" class="koboSpan">For an ideal cone-shaped beam of light covering solid angle </span><span><span id="kobo.135.1" class="koboSpan">Ω</span></span><span id="kobo.136.1" class="koboSpan"> with uniformly </span><span id="pg_15"><span id="kobo.137.1" class="koboSpan">spread radiant flux </span><span><span id="kobo.138.1" class="koboSpan">Φ</span></span><span id="kobo.139.1" class="koboSpan">, the radiant intensity satisfies</span></span></p>
<p class="DIS-IMG"><span id="kobo.140.1" class="koboSpan"><img class="img1" src="../images/pg15-1.png"/></span></p>
<p class="noindent"><span id="kobo.141.1" class="koboSpan">For example, a beam covering a whole sphere has a very low intensity </span><span id="kobo.142.1" class="koboSpan"><img class="inline" src="../images/pg15-in-1.png"/></span><span id="kobo.143.1" class="koboSpan">, so that one steradian contains a small part of the incoming light power. </span><span id="kobo.143.2" class="koboSpan">However, if we focus the same power in a beam covering the 1,000th part of a sphere, the intensity is significantly larger at </span><i><span id="kobo.144.1" class="koboSpan">I</span></i><span id="kobo.145.1" class="koboSpan"> = 80</span><span><span id="kobo.146.1" class="koboSpan">Φ</span></span><span id="kobo.147.1" class="koboSpan">.</span></p>
<p><span id="kobo.148.1" class="koboSpan">The irradiance and radiant intensity allow us to model lighting phenomena such as the difference in heat between noon and dusk. </span><span id="kobo.148.2" class="koboSpan">At any time a beam of light from the sun, approximated here as a beam of parallel rays, illuminates an area on the ground that is proportional to 1/cos (</span><i><span><span id="kobo.149.1" class="koboSpan">α</span></span></i><span id="kobo.150.1" class="koboSpan">) where </span><i><span><span id="kobo.151.1" class="koboSpan">α</span></span></i><span id="kobo.152.1" class="koboSpan"> is the beam incidence angle, as shown in </span><a id="rfig2-3" href="chapter_2.xhtml#fig2-3"><span id="kobo.153.1" class="koboSpan">Figure 2.3</span></a><span id="kobo.154.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-3"><span id="kobo.155.1" class="koboSpan"><img class="img2" src="../images/Figure2-3.png"/></span>
</a><figcaption><a id="fig2-3"></a><p class="CAP"><a id="fig2-3"><span class="FIGN"></span></a><a href="#rfig2-3"><span id="kobo.156.1" class="koboSpan">Figure 2.3</span></a> <span class="FIG"><span id="kobo.157.1" class="koboSpan">The area illuminated by a parallel beam as a function of the incident angle.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.158.1" class="koboSpan">Therefore a given fixed area </span><i><span id="kobo.159.1" class="koboSpan">A</span></i><span id="kobo.160.1" class="koboSpan"> on the ground is characterized by an irradiance that changes with the incident angle </span><i><span><span id="kobo.161.1" class="koboSpan">α</span></span></i></p>
<p class="DIS-IMG"><span id="kobo.162.1" class="koboSpan"><img class="img1" src="../images/pg15-2.png"/></span></p>
<p class="noindent"><span id="kobo.163.1" class="koboSpan">where </span><span><span id="kobo.164.1" class="koboSpan">Φ</span></span><span id="kobo.165.1" class="koboSpan"> is the uniformly spread radiant flux of the beam of light covering the corresponding area. </span><span id="kobo.165.2" class="koboSpan">In this case we point out that the irradiance reaches a peak value when the incidence angle is 0, and the irradiance vanishes when the sun moves behind the horizon, corresponding to an incidence angle </span><i><span><span id="kobo.166.1" class="koboSpan">α</span></span></i><span id="kobo.167.1" class="koboSpan"> = </span><i><span><span id="kobo.168.1" class="koboSpan">π</span></span><span id="kobo.169.1" class="koboSpan">/</span></i><span id="kobo.170.1" class="koboSpan">2.</span></p>
<p><span id="pg_16"><span id="kobo.171.1" class="koboSpan">In a real-life application the objective is not to capture the irradiance or the radiance intensity but to capture the </span><b><span id="kobo.172.1" class="koboSpan">radiance</span></b> <i><span id="kobo.173.1" class="koboSpan">L</span></i><span id="kobo.174.1" class="koboSpan">, which represents the ray strength, measuring the combined angular and spatial power densities.</span></span></p>
<p><span id="kobo.175.1" class="koboSpan">The radiance is described by the equation</span></p>
<p class="DIS-IMG"><span id="kobo.176.1" class="koboSpan"><img class="img1" src="../images/pg16-1.png"/></span></p>
<p class="noindent"><span id="kobo.177.1" class="koboSpan">where </span><span><span id="kobo.178.1" class="koboSpan">Ω</span></span><span id="kobo.179.1" class="koboSpan"> is the solid angle, </span><i><span id="kobo.180.1" class="koboSpan">A</span></i><span id="kobo.181.1" class="koboSpan"> is the area, and </span><i><span><span id="kobo.182.1" class="koboSpan">α</span></span></i><span id="kobo.183.1" class="koboSpan"> is the incidence angle.</span></p>
</section>
<section>
<h4 id="sec5" class="head b-head"><a id="sec2-1-4"><span id="kobo.184.1" class="koboSpan">2.1.4 Pinhole Model</span></a></h4><a id="sec2-1-4">
<p class="noindent"><span id="kobo.185.1" class="koboSpan">The principle underlying the biological eye is also the functioning principle of the first man-made cameras, called pinhole cameras (Young, 1989). </span><span id="kobo.185.2" class="koboSpan">The pinhole camera is based on a box with a half-millimeter hole and a photosensitive layer on the opposite side.</span></p>
</a><p><a id="sec2-1-4"><span id="kobo.186.1" class="koboSpan">The functioning principle of the pinhole camera, which is identical to that of a camera obscura, is shown in </span></a><a id="rfig2-4" href="chapter_2.xhtml#fig2-4"><span id="kobo.187.1" class="koboSpan">Figure 2.4a</span></a><span id="kobo.188.1" class="koboSpan">. </span><span id="kobo.188.2" class="koboSpan">In a camera obscura, which is a dark room with only a tiny hole in one of its walls, the light is projected upside down on the opposite wall, called a projection plane. </span><span id="kobo.188.3" class="koboSpan">The axis passing through the hole perpendicular on the projection plane is called an optical axis. </span><span id="kobo.188.4" class="koboSpan">Because the light travels in straight lines and the hole is very small, each point on the projection plane is mapped uniquely to a point from the outside scene.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-4"><span id="kobo.189.1" class="koboSpan"><img class="img2" src="../images/Figure2-4.png"/></span>
</a><figcaption><a id="fig2-4"></a><p class="CAP"><a id="fig2-4"><span class="FIGN"></span></a><a href="#rfig2-4"><span id="kobo.190.1" class="koboSpan">Figure 2.4</span></a> <span class="FIG"><span id="kobo.191.1" class="koboSpan">The pinhole camera. </span><span id="kobo.191.2" class="koboSpan">(a) The camera obscura, a darkened room with only a hole in a wall, is an example of the pinhole camera principle; (b) a diagram represents the pinhole camera principle.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.192.1" class="koboSpan">The diagram of the pinhole camera principle is shown in </span><a href="chapter_2.xhtml#fig2-4"><span id="kobo.193.1" class="koboSpan">Figure 2.4b</span></a><span id="kobo.194.1" class="koboSpan">. </span><span id="kobo.194.2" class="koboSpan">Here the coordinate frame is placed with coordinate </span><i><span id="kobo.195.1" class="koboSpan">Z</span></i><span id="kobo.196.1" class="koboSpan"> along the optical axis, coordinate </span><i><span id="kobo.197.1" class="koboSpan">Y</span></i><span id="kobo.198.1" class="koboSpan"> perpendicular on the diagram plane and therefore not displayed, and the center in the pinhole. </span><span id="kobo.198.2" class="koboSpan">The distance </span><i><span id="kobo.199.1" class="koboSpan">d</span></i><span id="kobo.200.1" class="koboSpan"> between the pinhole and the projection plane is called a focal distance, </span><i><span id="kobo.201.1" class="koboSpan">X</span></i><sub><span id="kobo.202.1" class="koboSpan">0</span></sub><span id="kobo.203.1" class="koboSpan">, </span><i><span id="kobo.204.1" class="koboSpan">Y</span></i><sub><span id="kobo.205.1" class="koboSpan">0</span></sub><span id="kobo.206.1" class="koboSpan">, </span><i><span id="kobo.207.1" class="koboSpan">Z</span></i><sub><span id="kobo.208.1" class="koboSpan">0</span></sub><span id="kobo.209.1" class="koboSpan"> denote the coordinates of a point in the scene, and − </span><i><span id="kobo.210.1" class="koboSpan">x</span></i><span id="kobo.211.1" class="koboSpan">, − </span><i><span id="kobo.212.1" class="koboSpan">y</span></i><span id="kobo.213.1" class="koboSpan">, </span><i><span id="kobo.214.1" class="koboSpan">d</span></i><span id="kobo.215.1" class="koboSpan"> denote the coordinates of </span><span id="pg_17"><span id="kobo.216.1" class="koboSpan">the corresponding point in the projection plane. </span><span id="kobo.216.2" class="koboSpan">Then it follows that </span><i><span id="kobo.217.1" class="koboSpan">x</span></i><span id="kobo.218.1" class="koboSpan"> = −</span><i><span id="kobo.219.1" class="koboSpan">d</span></i><span id="kobo.220.1" class="koboSpan"> · </span><i><span id="kobo.221.1" class="koboSpan">X</span></i><sub><span id="kobo.222.1" class="koboSpan">0</span></sub><i><span id="kobo.223.1" class="koboSpan">/Z</span></i><sub><span id="kobo.224.1" class="koboSpan">0</span></sub><span id="kobo.225.1" class="koboSpan"> and </span><i><span id="kobo.226.1" class="koboSpan">y</span></i><span id="kobo.227.1" class="koboSpan"> = −</span><i><span id="kobo.228.1" class="koboSpan">d</span></i><span id="kobo.229.1" class="koboSpan"> · </span><i><span id="kobo.230.1" class="koboSpan">Y</span></i><sub><span id="kobo.231.1" class="koboSpan">0</span></sub><i><span id="kobo.232.1" class="koboSpan">/Z</span></i><sub><span id="kobo.233.1" class="koboSpan">0</span></sub><span id="kobo.234.1" class="koboSpan">. </span><span id="kobo.234.2" class="koboSpan">In a more compact form, the model of the </span><i><span id="kobo.235.1" class="koboSpan">ideal pinhole camera</span></i><span id="kobo.236.1" class="koboSpan"> is</span></span></p>
<p class="DIS-IMG"><span id="kobo.237.1" class="koboSpan"><img class="img1" src="../images/pg17-1.png"/></span></p>
<p class="noindent"><span id="kobo.238.1" class="koboSpan">Here, </span><i><span id="kobo.239.1" class="koboSpan">∼</span></i><span id="kobo.240.1" class="koboSpan"> means that the two quantities are proportional. </span><span id="kobo.240.2" class="koboSpan">If we want to produce a digital image, then the coordinates of a pixel on the projection plane </span><i><span id="kobo.241.1" class="koboSpan">x</span></i><sub><i><span id="kobo.242.1" class="koboSpan">p</span></i></sub><span id="kobo.243.1" class="koboSpan">, </span><i><span id="kobo.244.1" class="koboSpan">y</span></i><sub><i><span id="kobo.245.1" class="koboSpan">p</span></i></sub><span id="kobo.246.1" class="koboSpan"> satisfy </span><i><span id="kobo.247.1" class="koboSpan">x</span></i><sub><i><span id="kobo.248.1" class="koboSpan">p</span></i></sub><span id="kobo.249.1" class="koboSpan"> = </span><i><span id="kobo.250.1" class="koboSpan">s</span></i><sub><i><span id="kobo.251.1" class="koboSpan">x</span></i></sub><i><span id="kobo.252.1" class="koboSpan">x</span></i><span id="kobo.253.1" class="koboSpan">, </span><i><span id="kobo.254.1" class="koboSpan">y</span></i><sub><i><span id="kobo.255.1" class="koboSpan">p</span></i></sub><span id="kobo.256.1" class="koboSpan"> = </span><i><span id="kobo.257.1" class="koboSpan">s</span></i><sub><i><span id="kobo.258.1" class="koboSpan">y</span></i></sub><i><span id="kobo.259.1" class="koboSpan">y</span></i><span id="kobo.260.1" class="koboSpan"> where </span><i><span id="kobo.261.1" class="koboSpan">s</span></i><sub><i><span id="kobo.262.1" class="koboSpan">x</span></i></sub><span id="kobo.263.1" class="koboSpan">, </span><i><span id="kobo.264.1" class="koboSpan">s</span></i><sub><i><span id="kobo.265.1" class="koboSpan">y</span></i></sub><span id="kobo.266.1" class="koboSpan"> represent the scaling constants. </span><span id="kobo.266.2" class="koboSpan">In a real scenario, the coordinate system in the projection plane is not centered on the optical axis. </span><span id="kobo.266.3" class="koboSpan">Therefore we introduce constants </span><i><span id="kobo.267.1" class="koboSpan">u</span></i><sub><span id="kobo.268.1" class="koboSpan">0</span></sub><span id="kobo.269.1" class="koboSpan">, </span><i><span id="kobo.270.1" class="koboSpan">v</span></i><sub><span id="kobo.271.1" class="koboSpan">0</span></sub><span id="kobo.272.1" class="koboSpan"> to account for this</span></p>
<p class="DIS-IMG"><span id="kobo.273.1" class="koboSpan"><img class="img1" src="../images/pg17-2.png"/></span></p>
<p class="noindent"><span id="kobo.274.1" class="koboSpan">Moreover, to generate a more realistic model, we need to take into account the skew effect caused by the fact that the optical axis may not be perfectly perpendicular on the projection plane. </span><span id="kobo.274.2" class="koboSpan">This effect is modeled by the skew factor </span><i><span><span id="kobo.275.1" class="koboSpan">α</span></span></i><span id="kobo.276.1" class="koboSpan">, leading to the final </span><i><span id="kobo.277.1" class="koboSpan">internal camera </span><span id="pg_18"><span id="kobo.278.1" class="koboSpan">model</span></span></i><span id="kobo.279.1" class="koboSpan">:</span></p>
<p class="DIS-IMG"><span id="kobo.280.1" class="koboSpan"><img class="img1" src="../images/pg18-1.png"/></span></p>
<p><span id="kobo.281.1" class="koboSpan">A pinhole camera creates an image by projecting light onto a plane. </span><span id="kobo.281.2" class="koboSpan">However, an image can be created using the opposite principle, by casting shadow. </span><span id="kobo.281.3" class="koboSpan">This is the functioning principle of the pinspeck camera (Cohen, 1982). </span><span id="kobo.281.4" class="koboSpan">Instead of a tiny hole, this camera is based on a wide aperture with a small speck in the middle. </span><span id="kobo.281.5" class="koboSpan">When objects are illuminating the camera, the speck is casting a shadow on the projection plane, effectively creating a negative image. </span><span id="kobo.281.6" class="koboSpan">A diagram of the pinspeck camera is shown in </span><a id="rfig2-5" href="chapter_2.xhtml#fig2-5"><span id="kobo.282.1" class="koboSpan">Figure 2.5</span></a><span id="kobo.283.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-5"><span id="kobo.284.1" class="koboSpan"><img class="img1" src="../images/Figure2-5.png"/></span>
</a><figcaption><a id="fig2-5"></a><p class="CAP"><a id="fig2-5"><span class="FIGN"></span></a><a href="#rfig2-5"><span id="kobo.285.1" class="koboSpan">Figure 2.5</span></a> <span class="FIG"><span id="kobo.286.1" class="koboSpan">The pinspeck camera: this imaging device is based on the opposite functioning principle of the pinhole camera, casting shadows that form a negative image.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.287.1" class="koboSpan">Pinhole and pinspeck cameras are good mechanisms to study light properties. </span><span id="kobo.287.2" class="koboSpan">However, from a practical perspective they are subject to several problems, such as long exposure times, limited sharpness, and limited field of view. </span><span id="kobo.287.3" class="koboSpan">The exposure time is long because the pinhole permits only a small number of light rays to hit the sensor plane per time unit, which means that it takes longer for an image to be created. </span><span id="kobo.287.4" class="koboSpan">The image sharpness for a pinhole camera is inversely proportional to the hole size. </span><span id="kobo.287.5" class="koboSpan">However, holes that are too small cause diffraction, which is bending of light around the corners of the hole.</span></p>
<p><span id="kobo.288.1" class="koboSpan">It is therefore important to find the right pinhole aperture size </span><i><span><span id="kobo.289.1" class="koboSpan">δ</span></span></i><span id="kobo.290.1" class="koboSpan"> to capture good photographs. </span><span id="kobo.290.2" class="koboSpan">As we discussed previously, for larger pinholes each point on the projection plane is mapped to a point of the scene along a series of lines, as depicted in </span><a id="rfig2-6" href="chapter_2.xhtml#fig2-6"><span id="kobo.291.1" class="koboSpan">Figure 2.6</span></a><span id="kobo.292.1" class="koboSpan">. </span><span id="kobo.292.2" class="koboSpan">Therefore a distant object is imaged as a disk of radius </span><i><span><span id="kobo.293.1" class="koboSpan">δ</span></span></i><span id="kobo.294.1" class="koboSpan">. </span><span id="kobo.294.2" class="koboSpan">However, when </span><i><span><span id="kobo.295.1" class="koboSpan">δ</span></span></i><span id="kobo.296.1" class="koboSpan"> is comparable with </span><i><span><span id="kobo.297.1" class="koboSpan">λ</span></span></i><span id="kobo.298.1" class="koboSpan">, the incoming light wavelength, the diffraction phenomenon causes the light rays passing close to the aperture boundaries to bend, leading to a circular disk with rings around it as shown in </span><a href="chapter_2.xhtml#fig2-6"><span id="kobo.299.1" class="koboSpan">Figure 2.6</span></a><span id="kobo.300.1" class="koboSpan">. </span><span id="kobo.300.2" class="koboSpan">The diameter of the disk is given by </span><i><span id="kobo.301.1" class="koboSpan">D</span></i><span id="kobo.302.1" class="koboSpan"> = 2.44 · </span><i><span><span id="kobo.303.1" class="koboSpan">λ</span></span></i><span id="kobo.304.1" class="koboSpan"> · </span><i><span id="kobo.305.1" class="koboSpan">d/</span><span><span id="kobo.306.1" class="koboSpan">δ</span></span></i><span id="kobo.307.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-6"><span id="kobo.308.1" class="koboSpan"><img class="img1" src="../images/Figure2-6.png"/></span>
</a><figcaption><a id="fig2-6"></a><p class="CAP"><a id="fig2-6"><span class="FIGN"></span></a><a href="#rfig2-6"><span id="kobo.309.1" class="koboSpan">Figure 2.6</span></a> <span class="FIG"><span id="kobo.310.1" class="koboSpan">The pinhole camera diffraction: when the pinhole size is comparable to the wavelength of the incoming light, a distant object is imaged as a circular disk with rings around it.</span></span></p></figcaption>
</figure>
</div>
<p><span id="pg_19"><span id="kobo.311.1" class="koboSpan">The optimal aperture size is attained when the disk of the diffraction pattern has diameter </span><i><span><span id="kobo.312.1" class="koboSpan">δ</span></span></i><span id="kobo.313.1" class="koboSpan">. </span><span id="kobo.313.2" class="koboSpan">A smaller aperture would cause distortion due to diffraction, and a larger one would lead to blurry images due to loss in sharpness. </span><span id="kobo.313.3" class="koboSpan">Let us compute the optimal aperture size for the wavelength located in the center of the visible spectrum </span><i><span><span id="kobo.314.1" class="koboSpan">λ</span></span></i><span id="kobo.315.1" class="koboSpan"> = 500 nm. </span><span id="kobo.315.2" class="koboSpan">Given that the focal distance </span><i><span id="kobo.316.1" class="koboSpan">d</span></i><span id="kobo.317.1" class="koboSpan"> is measured in millimeters, after the appropriate conversions, the optimal aperture size is</span></span></p>
<p class="DIS-IMG"><span id="kobo.318.1" class="koboSpan"><img class="img1" src="../images/pg19-1.png"/></span></p>
<p class="noindent"><span id="kobo.319.1" class="koboSpan">However, this estimation assumes that all imaged objects are far from the aperture. </span><span id="kobo.319.2" class="koboSpan">Close objects would create disks larger than the aperture and thus distort the image. </span><span id="kobo.319.3" class="koboSpan">Moreover, the smaller the hole is, the more it limits the field of view—at the limit, a hole of infinitesimal width allows only rays perpendicular to the projection plane to enter the camera.</span></p>
</section>
<section>
<h4 id="sec6" class="head b-head"><a id="sec2-1-5"><span id="kobo.320.1" class="koboSpan">2.1.5 Ray Bending and Lenses</span></a></h4><a id="sec2-1-5">
<p class="noindent"><span id="kobo.321.1" class="koboSpan">Given all the previously mentioned drawbacks of pinhole cameras, a better device is needed for successful photography. </span><span id="kobo.321.2" class="koboSpan">The lens, the component of choice in modern cameras, is based on the refraction principle. </span><span id="kobo.321.3" class="koboSpan">When a light ray passes through the smooth boundary of two different materials, it is bent by an angle depending on the indices of refraction of the two materials. </span><span id="kobo.321.4" class="koboSpan">The index of refraction is a constant characteristic of each material, and the </span><span id="pg_20"><span id="kobo.322.1" class="koboSpan">refraction is governed by Snell’s law</span></span></p>
<p class="DIS-IMG"><span id="kobo.323.1" class="koboSpan"><img class="img1" src="../images/pg20-1.png"/></span></p>
</a><p class="noindent"><a id="sec2-1-5"><span id="kobo.324.1" class="koboSpan">where </span><i><span id="kobo.325.1" class="koboSpan">n</span></i><sub><span id="kobo.326.1" class="koboSpan">1</span></sub><span id="kobo.327.1" class="koboSpan">, </span><i><span id="kobo.328.1" class="koboSpan">n</span></i><sub><span id="kobo.329.1" class="koboSpan">2</span></sub><span id="kobo.330.1" class="koboSpan"> are the indices of refraction, and </span><i><span><span id="kobo.331.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.332.1" class="koboSpan">1</span></sub><span id="kobo.333.1" class="koboSpan">, </span><i><span><span id="kobo.334.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.335.1" class="koboSpan">2</span></sub><span id="kobo.336.1" class="koboSpan"> are the angle of incidence and angle of refraction, respectively. </span><span id="kobo.336.2" class="koboSpan">The bending process, called refraction, is depicted in </span></a><a id="rfig2-7" href="chapter_2.xhtml#fig2-7"><span id="kobo.337.1" class="koboSpan">Figure 2.7</span></a><span id="kobo.338.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-7"><span id="kobo.339.1" class="koboSpan"><img class="img3" src="../images/Figure2-7.png"/></span>
</a><figcaption><a id="fig2-7"></a><p class="CAP"><a id="fig2-7"><span class="FIGN"></span></a><a href="#rfig2-7"><span id="kobo.340.1" class="koboSpan">Figure 2.7</span></a> <span class="FIG"><span id="kobo.341.1" class="koboSpan">The refraction principle: a ray of light is bent at the boundary of two materials with an angle given by Snell’s law.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.342.1" class="koboSpan">A lens has a relatively generic definition. </span><span id="kobo.342.2" class="koboSpan">Any object that bends incoming rays into outgoing rays can be considered a lens. </span><span id="kobo.342.3" class="koboSpan">The number of lenses that can be generated on the basis of how they refract light according to Snell’s law is very large. </span><span id="kobo.342.4" class="koboSpan">However, in optics it is common to use an idealized concept, called the </span><i><span id="kobo.343.1" class="koboSpan">thin lens</span></i><span id="kobo.344.1" class="koboSpan">. </span><span id="kobo.344.2" class="koboSpan">A thin lens, also called a paraxial, is a plane that bends light governed only by three parameters: the focal length, the aperture diameter, and the lens speed.</span></p>
<p><span id="kobo.345.1" class="koboSpan">The </span><i><span id="kobo.346.1" class="koboSpan">focal length</span></i> <samp><span id="kobo.347.1" class="koboSpan">f</span></samp><span id="kobo.348.1" class="koboSpan"> is defined as the distance in millimeters between a thin lens and the point of convergence of a number of parallel rays passing through the lens. </span><span id="kobo.348.2" class="koboSpan">The inverse of the focal length 1/</span><samp><span id="kobo.349.1" class="koboSpan">f</span></samp><span id="kobo.350.1" class="koboSpan"> is known as the focusing power and is measured in diopters. </span><span id="kobo.350.2" class="koboSpan">This parameter is an important characteristic of common eyeglasses.</span></p>
<p><span id="pg_21"><span id="kobo.351.1" class="koboSpan">The </span><i><span id="kobo.352.1" class="koboSpan">aperture diameter D</span></i><span id="kobo.353.1" class="koboSpan"> is the diameter of the base of a conical shaped ray bundle passing through the lens. </span><span id="kobo.353.2" class="koboSpan">In other words, it is the diameter of the largest portion of the lens that is bending light.</span></span></p>
<p><span id="kobo.354.1" class="koboSpan">The </span><i><span id="kobo.355.1" class="koboSpan">lens speed</span></i><span id="kobo.356.1" class="koboSpan">, or the </span><samp><span id="kobo.357.1" class="koboSpan">f</span></samp><span id="kobo.358.1" class="koboSpan">-number </span><i><span id="kobo.359.1" class="koboSpan">N</span></i><span id="kobo.360.1" class="koboSpan">, is the ratio between the focal length and the aperture diameter. </span><span id="kobo.360.2" class="koboSpan">It describes the ability of the lens to transmit light (i.e., the required exposure time for capturing an image).</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-8"><span id="kobo.361.1" class="koboSpan"><img class="img1" src="../images/Figure2-8.png"/></span>
</a><figcaption><a id="fig2-8"></a><p class="CAP"><a id="fig2-8"><span class="FIGN"></span></a><a href="#rfig2-8"><span id="kobo.362.1" class="koboSpan">Figure 2.8</span></a> <span class="FIG"><span id="kobo.363.1" class="koboSpan">The diagram of a thin lens: a point in the scene (black dot) is in focus if the light it reflects toward the lens converges on the sensor plane. </span><span id="kobo.363.2" class="koboSpan">When the camera refocuses on a different point, the sensor plane moves relative to the lens.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.364.1" class="koboSpan">The light bending principle with a thin lens is depicted in </span><a id="rfig2-8" href="chapter_2.xhtml#fig2-8"><span id="kobo.365.1" class="koboSpan">Figure 2.8</span></a><span id="kobo.366.1" class="koboSpan">. </span><span id="kobo.366.2" class="koboSpan">A scene point is in focus relative to a thin lens and a sensor plane if it obeys the thin lens equation:</span></p>
<p class="DIS-IMG"><span id="kobo.367.1" class="koboSpan"><img class="img1" src="../images/pg21-1.png"/></span></p>
<p class="noindent"><span id="kobo.368.1" class="koboSpan">where </span><i><span id="kobo.369.1" class="koboSpan">S</span></i><sub><span id="kobo.370.1" class="koboSpan">1</span></sub><span id="kobo.371.1" class="koboSpan"> is the distance between the lens and the object in focus, </span><i><span id="kobo.372.1" class="koboSpan">S</span></i><sub><span id="kobo.373.1" class="koboSpan">2</span></sub><span id="kobo.374.1" class="koboSpan"> is the distance between the lens and the sensor plane, and </span><samp><span id="kobo.375.1" class="koboSpan">f</span></samp><span id="kobo.376.1" class="koboSpan"> is the focal length.</span></p>
<p><span id="kobo.377.1" class="koboSpan">In the early days of photography, capturing an image required manually focusing the camera using a ground-glass viewer, inserting the light-sensitive plate inside the camera, and then manually controlling the exposure time by removing the cap from the lens for a predefined amount of time, as shown in </span><a id="rfig2-9" href="chapter_2.xhtml#fig2-9"><span id="kobo.378.1" class="koboSpan">Figure 2.9</span></a><span id="kobo.379.1" class="koboSpan">. </span><span id="kobo.379.2" class="koboSpan">Initially, this time was measured in hours, but it decreased to milliseconds thanks to the development of light sensors. </span><span id="kobo.379.3" class="koboSpan">Automatic shutters allowed for a fine control of the exposure time.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-9"><span id="kobo.380.1" class="koboSpan"><img class="img1" src="../images/Figure2-9.png"/></span>
</a><figcaption><a id="fig2-9"></a><p class="CAP"><a id="fig2-9"><span class="FIGN"></span></a><a href="#rfig2-9"><span id="kobo.381.1" class="koboSpan">Figure 2.9</span></a> <span class="FIG"><span id="kobo.382.1" class="koboSpan">Lens-based camera obscura: the first cameras required a manual adjustment of the exposure time. </span><span id="kobo.382.2" class="koboSpan">Reprinted from (Raskar and Tumblin, 2011).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.383.1" class="koboSpan">When we capture an image with a modern digital camera the device itself automatically tunes a large number of parameters to allow a crisp and detailed view of the scene. </span><span id="kobo.383.2" class="koboSpan">Those </span><span id="pg_22"><span id="kobo.384.1" class="koboSpan">parameters can relate to the camera as a whole, to the camera lens, to the shutter, or to the light sensor.</span></span></p>
<p><span id="kobo.385.1" class="koboSpan">The camera itself has a certain </span><i><span id="kobo.386.1" class="koboSpan">position</span></i><span id="kobo.387.1" class="koboSpan"> and </span><i><span id="kobo.388.1" class="koboSpan">orientation</span></i><span id="kobo.389.1" class="koboSpan"> in space, allowing it to capture a portion of the scene. </span><span id="kobo.389.2" class="koboSpan">For a dynamic scene, the </span><i><span id="kobo.390.1" class="koboSpan">time of the capture</span></i><span id="kobo.391.1" class="koboSpan"> is another important parameter. </span><span id="kobo.391.2" class="koboSpan">The scene </span><i><span id="kobo.392.1" class="koboSpan">lighting</span></i><span id="kobo.393.1" class="koboSpan">, either coming from the camera flash or from an external source, can affect the colors or visibility of the scene objects.</span></p>
<p><span id="kobo.394.1" class="koboSpan">The light reflected by the scene first meets the opening of the camera shutter. </span><span id="kobo.394.2" class="koboSpan">This opening is called </span><i><span id="kobo.395.1" class="koboSpan">aperture</span></i><span id="kobo.396.1" class="koboSpan">, controlling how many light rays enter the camera at any given moment. </span><span id="kobo.396.2" class="koboSpan">This parameter is tightly connected with the </span><i><span id="kobo.397.1" class="koboSpan">exposure time</span></i><span id="kobo.398.1" class="koboSpan">, which measures how long the shutter is kept open. </span><span id="kobo.398.2" class="koboSpan">Therefore, a small aperture and short exposure time lead to darker images. </span><span id="kobo.398.3" class="koboSpan">However, apart from image brightness, the two parameters have different effects on the image, as subsequently explained.</span></p>
<p><span id="kobo.399.1" class="koboSpan">After passing through the aperture, the light rays are bent by the camera lens. </span><span id="kobo.399.2" class="koboSpan">This is determined by the lens </span><i><span id="kobo.400.1" class="koboSpan">focal length</span></i><span id="kobo.401.1" class="koboSpan">, which describes how fast parallel light beams converge after passing through the lens. </span><span id="kobo.401.2" class="koboSpan">This parameter affects the width of the field of view. </span><span id="kobo.401.3" class="koboSpan">A lens </span><span id="pg_23"><span id="kobo.402.1" class="koboSpan">with short focal length corresponds to a wide </span><i><span id="kobo.403.1" class="koboSpan">field of view</span></i><span id="kobo.404.1" class="koboSpan">, which allows it to capture a larger portion of the scene. </span><span id="kobo.404.2" class="koboSpan">At the same time, the short focal length lens in conjunction with a small aperture allows a much longer depth of field. </span><span id="kobo.404.3" class="koboSpan">Lenses with wide fields of view shrink scene features and exaggerate foreshortening (depth-dependent size). </span><span id="kobo.404.4" class="koboSpan">On the other hand, lenses with narrow fields of view, also known as telephoto lenses, enlarge scene features and reduce foreshortening.</span></span></p>
<p><span id="kobo.405.1" class="koboSpan">Using lenses instead of pinholes vastly increases the final image brightness, as discussed previously. </span><span id="kobo.405.2" class="koboSpan">Therefore, fewer rays emitted by a distant object would reach the camera. </span><span id="kobo.405.3" class="koboSpan">This might suggest that this object might appear less bright in the image, but that is not true for the following reason.</span></p>
<p><span id="kobo.406.1" class="koboSpan">If we double the distance between an object and the camera, the light beam detected by the camera is decreased by 1/2 both horizontally and vertically, leading to a solid angle decreased by 1/4, and therefore a radiant flux </span><span><span id="kobo.407.1" class="koboSpan">Φ</span></span><span id="kobo.408.1" class="koboSpan"> smaller by 1/4. </span><span id="kobo.408.2" class="koboSpan">However, the brightness of each point of the object projected on the sensor is given by the irradiance </span><i><span id="kobo.409.1" class="koboSpan">R</span></i><span id="kobo.410.1" class="koboSpan"> = </span><i><span id="kobo.411.1" class="koboSpan">d</span></i><span><span id="kobo.412.1" class="koboSpan">Φ</span></span><i><span id="kobo.413.1" class="koboSpan">/dA</span></i><span id="kobo.414.1" class="koboSpan">, where </span><i><span id="kobo.415.1" class="koboSpan">A</span></i><span id="kobo.416.1" class="koboSpan"> denotes the area on the sensor that is illuminated by the point. </span><span id="kobo.416.2" class="koboSpan">Given that the solid angle of the light beam is 1/4 smaller, it follows that the area </span><i><span id="kobo.417.1" class="koboSpan">A</span></i><span id="kobo.418.1" class="koboSpan"> projected on the sensor is also decreased by 1/4, and thus the average irradiance of the object in question </span><span id="kobo.419.1" class="koboSpan"><img class="inline" src="../images/pg23-in-1.png"/></span><span id="kobo.420.1" class="koboSpan"> stays the same.</span></p>
<p><span id="kobo.421.1" class="koboSpan">By comparison with the human eye, we can define a </span><i><span id="kobo.422.1" class="koboSpan">normal lens</span></i><span id="kobo.423.1" class="koboSpan"> that replicates approximately the eye’s field of view. </span><span id="kobo.423.2" class="koboSpan">Given that the field of view is affected by the focal length, a normal lens has a focal length approximately equal to the diagonal dimension of the film or digital sensor that captures the photograph. </span><span id="kobo.423.3" class="koboSpan">However, in order to create a realistic perception, we need to take into account that we usually view images from a distance, which is why in practice the normal lens generates slightly larger fields of view than the biological eye. </span><span id="kobo.423.4" class="koboSpan">Wide angle lenses are used to capture larger areas of the scene that cannot be accommodated by normal lenses. </span><span id="kobo.423.5" class="koboSpan">This leads to distorted photographs, but the effect is generally addressed with larger prints.</span></p>
<p><span id="kobo.424.1" class="koboSpan">When one attempts to photograph a tall object in a scene such as a building, the photographer notices the tilt effect, in which the object in the image seems to lean backward. </span><span id="kobo.424.2" class="koboSpan">This is due to the upward tilt in the camera required to include the whole building in the image frame. </span><span id="kobo.424.3" class="koboSpan">Professional photographers and architects are interested in capturing a tall object that appears straight in the final photograph. </span><span id="kobo.424.4" class="koboSpan">One option is the post-processing of the image. </span><span id="kobo.424.5" class="koboSpan">However the problem can be solved by capturing the image with a tilt-shift lens that compensates for this effect.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-10"><span id="kobo.425.1" class="koboSpan"><img class="img1" src="../images/Figure2-10.png"/></span>
</a><figcaption><a id="fig2-10"></a><p class="CAP"><a id="fig2-10"><span class="FIGN"></span></a><a href="#rfig2-10"><span id="kobo.426.1" class="koboSpan">Figure 2.10</span></a> <span class="FIG"><span id="kobo.427.1" class="koboSpan">Ray bending at one surface of a thin lens: the ray reflected by the object (</span><i><span id="kobo.428.1" class="koboSpan">red line</span></i><span id="kobo.429.1" class="koboSpan">) intersects the lens surface at distance </span><i><span id="kobo.430.1" class="koboSpan">h</span></i><span id="kobo.431.1" class="koboSpan"> from the optical axis. </span><span id="kobo.431.2" class="koboSpan">Snell’s law governs the new direction of the ray relative to the line normal to the surface (</span><i><span id="kobo.432.1" class="koboSpan">dotted line</span></i><span id="kobo.433.1" class="koboSpan">).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.434.1" class="koboSpan">In order to understand the limits of the thin lens formula, let us consider one surface of a convex lens, as shown in </span><a id="rfig2-10" href="chapter_2.xhtml#fig2-10"><span id="kobo.435.1" class="koboSpan">Figure 2.10</span></a><span id="kobo.436.1" class="koboSpan">. </span><span id="kobo.436.2" class="koboSpan">When the ray intersects the lens surface, it is refracted according to Snell’s law. </span><span id="kobo.436.3" class="koboSpan">However, because the lens surface is not flat, the incidence angle </span><i><span><span id="kobo.437.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.438.1" class="koboSpan">1</span></sub><span id="kobo.439.1" class="koboSpan"> is computed relative to the line normal to the surface. </span><span id="kobo.439.2" class="koboSpan">Therefore Snell’s law is</span></p>
<p class="DIS-IMG"><span id="kobo.440.1" class="koboSpan"><img class="img1" src="../images/pg23-1.png"/></span></p>
<p class="noindent"><span id="pg_24"><span id="kobo.441.1" class="koboSpan">Furthermore, we can derive the following:</span></span></p>
<p class="DIS-IMG"><span id="kobo.442.1" class="koboSpan"><img class="img1" src="../images/pg24-1.png"/></span></p>
<p class="noindent"><span id="kobo.443.1" class="koboSpan">These trigonometric expressions lead to rather complex calculations. </span><span id="kobo.443.2" class="koboSpan">Therefore it is common to use the </span><i><span id="kobo.444.1" class="koboSpan">paraxial approximation</span></i><span id="kobo.445.1" class="koboSpan">, which assumes that the angle between the light ray and the optical axis is very small. </span><span id="kobo.445.2" class="koboSpan">From trigonometry, we know that when an angle </span><i><span><span id="kobo.446.1" class="koboSpan">β</span></span></i><span id="kobo.447.1" class="koboSpan"> is very small, we can say that </span><i><span><span id="kobo.448.1" class="koboSpan">β</span></span></i><span id="kobo.449.1" class="koboSpan"> ≃ tan (</span><i><span><span id="kobo.450.1" class="koboSpan">β</span></span></i><span id="kobo.451.1" class="koboSpan">1) ≃ sin(</span><i><span><span id="kobo.452.1" class="koboSpan">β</span></span></i><span id="kobo.453.1" class="koboSpan">). </span><span id="kobo.453.2" class="koboSpan">Using this approximation, the equations become</span></p>
<p class="DIS-IMG"><span id="kobo.454.1" class="koboSpan"><img class="img1" src="../images/pg24-2.png"/></span></p>
<p class="noindent"><span id="kobo.455.1" class="koboSpan">If we substitute the last two lines in the first equation, we have</span></p>
<p class="DIS-IMG"><span id="kobo.456.1" class="koboSpan"><img class="img1" src="../images/pg24-3.png"/></span></p>
<p class="noindent"><span id="pg_25"><span id="kobo.457.1" class="koboSpan">We now combine the equations describing light bending on both sides of the lens to yield the lens equation</span></span></p>
<p class="DIS-IMG"><span id="kobo.458.1" class="koboSpan"><img class="img1" src="../images/pg25-1.png"/></span></p>
<p class="noindent"><span id="kobo.459.1" class="koboSpan">where </span><i><span id="kobo.460.1" class="koboSpan">R</span></i><sub><span id="kobo.461.1" class="koboSpan">1</span></sub><span id="kobo.462.1" class="koboSpan">, </span><i><span id="kobo.463.1" class="koboSpan">R</span></i><sub><span id="kobo.464.1" class="koboSpan">2</span></sub><span id="kobo.465.1" class="koboSpan"> are the radii of the two surfaces of the lens. </span><span id="kobo.465.2" class="koboSpan">Now let us assume that the object is located far away from the lens. </span><span id="kobo.465.3" class="koboSpan">In this scenario we have</span></p>
<p class="DIS-IMG"><span id="kobo.466.1" class="koboSpan"><img class="img1" src="../images/pg25-2.png"/></span></p>
<p class="noindent"><span id="kobo.467.1" class="koboSpan">where </span><samp><span id="kobo.468.1" class="koboSpan">f</span></samp><span id="kobo.469.1" class="koboSpan"> is the focal length of the lens, which is in line with the paraxial approximation. </span><span id="kobo.469.2" class="koboSpan">The lens equation then takes the following form, which is also known as the lens maker’s equation:</span></p>
<p class="DIS-IMG"><span id="kobo.470.1" class="koboSpan"><img class="img1" src="../images/pg25-3.png"/></span></p>
<p class="noindent"><span id="kobo.471.1" class="koboSpan">The two versions of the equation also lead to the previously introduced thin lens equation</span></p>
<p class="DIS-IMG"><span id="kobo.472.1" class="koboSpan"><img class="img1" src="../images/pg25-4.png"/></span></p>
<p class="noindent"><span id="kobo.473.1" class="koboSpan">As we have previously shown, this equation describes how to change the distance between the sensor and the lens in order to keep an object in focus. </span><span id="kobo.473.2" class="koboSpan">However, with regard to its derivation, it is important to remember that it relies on the </span><i><span id="kobo.474.1" class="koboSpan">paraxial approximation</span></i><span id="kobo.475.1" class="koboSpan">. </span><span id="kobo.475.2" class="koboSpan">This means that the equation will no longer be precise for objects that are close to the lens and not on the optical axis. </span><span id="kobo.475.3" class="koboSpan">Similarly, the object may not be close, but if the lens is large, then the paraxial approximation will not hold for rays intersecting the lens at its outer boundaries.</span></p>
<p><span id="kobo.476.1" class="koboSpan">In the preceding equations 1/</span><samp><span id="kobo.477.1" class="koboSpan">f</span></samp><span id="kobo.478.1" class="koboSpan"> is the </span><i><span id="kobo.479.1" class="koboSpan">focusing power</span></i><span id="kobo.480.1" class="koboSpan"> of the lens. </span><span id="kobo.480.2" class="koboSpan">The explanation of its name is intuitive. </span><span id="kobo.480.3" class="koboSpan">When 1/</span><samp><span id="kobo.481.1" class="koboSpan">f</span></samp><span id="kobo.482.1" class="koboSpan"> increases, then </span><i><span id="kobo.483.1" class="koboSpan">q</span></i><span id="kobo.484.1" class="koboSpan"> decreases, so that the rays converge faster, and thus we say that the lens has a higher focusing power.</span></p>
<p><span id="kobo.485.1" class="koboSpan">Let us see what happens when we place two thin lenses in a sequence. </span><span id="kobo.485.2" class="koboSpan">The equations are</span></p>
<p class="DIS-IMG"><span id="kobo.486.1" class="koboSpan"><img class="img1" src="../images/pg25-5.png"/></span></p>
<p><span id="kobo.487.1" class="koboSpan">In this case the rule is that the object plane of the second thin lens is located at −</span><i><span id="kobo.488.1" class="koboSpan">q</span></i><sub><span id="kobo.489.1" class="koboSpan">2</span></sub><span id="kobo.490.1" class="koboSpan">, where </span><i><span id="kobo.491.1" class="koboSpan">q</span></i><sub><span id="kobo.492.1" class="koboSpan">2</span></sub><span id="kobo.493.1" class="koboSpan"> is the focus plane of the first thin lens. </span><span id="kobo.493.2" class="koboSpan">Therefore </span><i><span id="kobo.494.1" class="koboSpan">p</span></i><sub><span id="kobo.495.1" class="koboSpan">2</span></sub><span id="kobo.496.1" class="koboSpan"> = −</span><i><span id="kobo.497.1" class="koboSpan">q</span></i><sub><span id="kobo.498.1" class="koboSpan">2</span></sub><span id="kobo.499.1" class="koboSpan"> and thus by adding these equations we obtain</span></p>
<p class="DIS-IMG"><span id="kobo.500.1" class="koboSpan"><img class="img1" src="../images/pg25-6.png"/></span></p>
<p class="noindent"><span id="kobo.501.1" class="koboSpan">where </span><samp><span id="kobo.502.1" class="koboSpan">f</span></samp><sub><i><span id="kobo.503.1" class="koboSpan">c</span></i></sub><span id="kobo.504.1" class="koboSpan"> is the focal length of the compound lens. </span><span id="kobo.504.2" class="koboSpan">We notice that the focusing power of the compound lens is the sum of the focusing powers of all the individual lenses. </span><span id="kobo.504.3" class="koboSpan">Conversely, we can work out the focal length of the compound lens </span><samp><span id="kobo.505.1" class="koboSpan">f</span></samp><sub><i><span id="kobo.506.1" class="koboSpan">c</span></i></sub><span id="kobo.507.1" class="koboSpan"> as </span><samp><span id="kobo.508.1" class="koboSpan">f</span></samp><sub><i><span id="kobo.509.1" class="koboSpan">c</span></i></sub><span id="kobo.510.1" class="koboSpan"> = </span><samp><span id="kobo.511.1" class="koboSpan">f</span></samp><sub><span id="kobo.512.1" class="koboSpan">1</span></sub><samp><span id="kobo.513.1" class="koboSpan">f</span></samp><sub><span id="kobo.514.1" class="koboSpan">2</span></sub><span id="kobo.515.1" class="koboSpan">/</span><samp><span id="kobo.516.1" class="koboSpan">f</span></samp><sub><span id="kobo.517.1" class="koboSpan">1</span></sub><span id="kobo.518.1" class="koboSpan"> + </span><samp><span id="kobo.519.1" class="koboSpan">f</span></samp><sub><span id="kobo.520.1" class="koboSpan">2</span></sub><span id="kobo.521.1" class="koboSpan">. </span><span id="kobo.521.2" class="koboSpan">Intuitively, the second lens makes the rays that pass through the first lens converge even faster, therefore leading to an accumulated focusing power.</span></p>
<p><span id="pg_26"><span id="kobo.522.1" class="koboSpan">If the two lenses are separated by length </span><i><span id="kobo.523.1" class="koboSpan">d</span></i><span id="kobo.524.1" class="koboSpan"> then the equation becomes (Ronchi and Rosen, 1991)</span></span></p>
<p class="DIS-IMG"><span id="kobo.525.1" class="koboSpan"><img class="img1" src="../images/pg26-1.png"/></span></p>
<p><span id="kobo.526.1" class="koboSpan">A real lens does not need to be spherical. </span><span id="kobo.526.2" class="koboSpan">The main categories of lens shapes, defining how they bend the light, are shown in </span><a id="rfig2-11" href="chapter_2.xhtml#fig2-11"><span id="kobo.527.1" class="koboSpan">Figure 2.11</span></a><span id="kobo.528.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-11"><span id="kobo.529.1" class="koboSpan"><img class="img1" src="../images/Figure2-11.png"/></span>
</a><figcaption><a id="fig2-11"></a><p class="CAP"><a id="fig2-11"><span class="FIGN"></span></a><a href="#rfig2-11"><span id="kobo.530.1" class="koboSpan">Figure 2.11</span></a> <span class="FIG"><span id="kobo.531.1" class="koboSpan">The main categories of lenses, which are based on the shapes of the lenses’ two surfaces.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.532.1" class="koboSpan">One may wonder why it is necessary to use more than one lens. </span><span id="kobo.532.2" class="koboSpan">The reason is that in using a single element spherical lens, the image of the object is not created on a plane but instead on a sphere. </span><span id="kobo.532.3" class="koboSpan">This optical aberration is known as the </span><i><span id="kobo.533.1" class="koboSpan">Petzval field curvature</span></i><span id="kobo.534.1" class="koboSpan"> or simply the </span><i><span id="kobo.535.1" class="koboSpan">field curvature</span></i><span id="kobo.536.1" class="koboSpan">. </span><span id="kobo.536.2" class="koboSpan">The result is that we cannot focus an entire object on a plane sensor, which causes the image to look blurred around the edges. </span><span id="kobo.536.3" class="koboSpan">Other than using complex lens designs, a hardware solution is to use a curved imaging sensor that compensates for this effect. </span><span id="kobo.536.4" class="koboSpan">An example of such an imaging sensor, NASA’s Kepler focal plane array, is shown in </span><a id="rfig2-12" href="chapter_2.xhtml#fig2-12"><span id="kobo.537.1" class="koboSpan">Figure 2.12</span></a><span id="kobo.538.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-12"><span id="kobo.539.1" class="koboSpan"><img class="img1" src="../images/Figure2-12.png"/></span>
</a><figcaption><a id="fig2-12"></a><p class="CAP"><a id="fig2-12"><span class="FIGN"></span></a><a href="#rfig2-12"><span id="kobo.540.1" class="koboSpan">Figure 2.12</span></a> <span class="FIG"><span id="kobo.541.1" class="koboSpan">The image curvature effect and the Kepler focal plane array: in using a single element spherical lens it is not possible to focus a whole object on the sensor plane, and thus the edges look out of focus. </span><span id="kobo.541.2" class="koboSpan">Unlike conventional digital sensors, the imaging sensor array used in the Kepler space observatory is curved so that the Petzval field curvature can be compensated. </span><i><span id="kobo.542.1" class="koboSpan">Source</span></i><span id="kobo.543.1" class="koboSpan">: NASA (NASA and Ball Aerospace, 2008).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.544.1" class="koboSpan">Another way to address this effect is to use the meniscus lens, which creates a much flatter image. </span><span id="kobo.544.2" class="koboSpan">However, this lens introduces chromatic aberration, that is, it focalizes different waveforms on different planes.</span></p>
<p><span id="kobo.545.1" class="koboSpan">The paraxial approximation is a significant constraint on the lens size and object position relative to the lens. </span><span id="kobo.545.2" class="koboSpan">It assumes that </span><i><span><span id="kobo.546.1" class="koboSpan">β</span></span></i><span id="kobo.547.1" class="koboSpan"> ≃ sin (</span><i><span><span id="kobo.548.1" class="koboSpan">β</span></span></i><span id="kobo.549.1" class="koboSpan">) for small values of </span><i><span><span id="kobo.550.1" class="koboSpan">β</span></span></i><span id="kobo.551.1" class="koboSpan">. </span><span id="kobo.551.2" class="koboSpan">This estimation </span><span id="pg_27"><span id="kobo.552.1" class="koboSpan">comes from the Taylor series expansion of the sine, which states that</span></span></p>
<p class="DIS-IMG"><span id="kobo.553.1" class="koboSpan"><img class="img1" src="../images/pg27-1.png"/></span></p>
<p><span id="kobo.554.1" class="koboSpan">The estimation is more precise if we include more terms. </span><span id="kobo.554.2" class="koboSpan">For example, Ludwig von Seidel used the third-order approximation sin (</span><i><span><span id="kobo.555.1" class="koboSpan">β</span></span></i><span id="kobo.556.1" class="koboSpan">) ≃ </span><i><span><span id="kobo.557.1" class="koboSpan">β</span></span></i><span id="kobo.558.1" class="koboSpan"> + </span><i><span><span id="kobo.559.1" class="koboSpan">β</span></span></i><sup><span id="kobo.560.1" class="koboSpan">3</span></sup><span id="kobo.561.1" class="koboSpan">/3! </span><span id="kobo.561.2" class="koboSpan">to evaluate the imperfections of lenses, and concluded that there are five aberrations that make real lenses bend light differently from a perfect lens. </span><span id="kobo.561.3" class="koboSpan">Therefore one may wonder why we do not use very high-order optics to make better lenses? </span><span id="kobo.561.4" class="koboSpan">The reason is that making less regular lenses is very expensive, and it is much more affordable to use stacks of compound lenses of simple shapes.</span></p>
<p><span id="kobo.562.1" class="koboSpan">A well known example of lens imperfection is the chromatic aberration, also called </span><i><span id="kobo.563.1" class="koboSpan">dispersion</span></i><span id="kobo.564.1" class="koboSpan">. </span><span id="kobo.564.2" class="koboSpan">The phenomenon, depicted in </span><a id="rfig2-13" href="chapter_2.xhtml#fig2-13"><span id="kobo.565.1" class="koboSpan">Figure 2.13</span></a><span id="kobo.566.1" class="koboSpan">, causes light rays of different wavelengths to focus at variable distances from the lens. </span><span id="kobo.566.2" class="koboSpan">In essence, it means that the lens has a wavelength-dependent focal length. </span><span id="kobo.566.3" class="koboSpan">One option to correct this is based on the observation that plano-concave lenses also bend different wavelengths differently but in the opposite </span><span id="pg_28"><span id="kobo.567.1" class="koboSpan">direction. </span><span id="kobo.567.2" class="koboSpan">Therefore the dispersion can be corrected by pairing the biconvex lens with a plano-concave lens as shown in </span><a href="chapter_2.xhtml#fig2-13"><span id="kobo.568.1" class="koboSpan">Figure 2.13</span></a><span id="kobo.569.1" class="koboSpan">. </span><span id="kobo.569.2" class="koboSpan">Section 8.3.1 explains how the properties of dispersion are used in image capture setups to obtain spectral information.</span></span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-13"><span id="kobo.570.1" class="koboSpan"><img class="img3" src="../images/Figure2-13.png"/></span>
</a><figcaption><a id="fig2-13"></a><p class="CAP"><a id="fig2-13"><span class="FIGN"></span></a><a href="#rfig2-13"><span id="kobo.571.1" class="koboSpan">Figure 2.13</span></a> <span class="FIG"><span id="kobo.572.1" class="koboSpan">Correcting chromatic aberration: the biconvex lens has a frequency-dependent focal length, which can be corrected by pairing it with a plano-concave lens.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.573.1" class="koboSpan">As one might expect, using a stack of two lenses can correct the chromatic aberration for two frequencies, resulting in an </span><i><span id="kobo.574.1" class="koboSpan">achromat</span></i><span id="kobo.575.1" class="koboSpan"> lens. </span><span id="kobo.575.2" class="koboSpan">The same principle can be applied by stacking up three (apochromat) and four (superchromat) lenses, which corrects three and four frequencies, respectively. </span><span id="kobo.575.3" class="koboSpan">However, this also leads to an increase in cost.</span></p>
<p><span id="kobo.576.1" class="koboSpan">Another lens aberration occurs because a proportion of the light is reflected by each lens, bouncing back and causing flares and other undesired effects. </span><span id="kobo.576.2" class="koboSpan">The ratio of reflected light intensity between two materials is given by Fresnel’s equation</span></p>
<p class="DIS-IMG"><span id="kobo.577.1" class="koboSpan"><img class="img1" src="../images/pg28-1.png"/></span></p>
<p class="noindent"><span id="pg_29"><span id="kobo.578.1" class="koboSpan">where </span><i><span id="kobo.579.1" class="koboSpan">n</span></i><sub><span id="kobo.580.1" class="koboSpan">1</span></sub><span id="kobo.581.1" class="koboSpan"> and </span><i><span id="kobo.582.1" class="koboSpan">n</span></i><sub><span id="kobo.583.1" class="koboSpan">2</span></sub><span id="kobo.584.1" class="koboSpan"> are the refractive indices of the two materials, and </span><i><span id="kobo.585.1" class="koboSpan">r</span></i><span id="kobo.586.1" class="koboSpan"> is the amplitude ratio of the reflected light. </span><span id="kobo.586.2" class="koboSpan">For the case of the air/glass reflectivity, we have that </span><i><span id="kobo.587.1" class="koboSpan">n</span></i><sub><span id="kobo.588.1" class="koboSpan">1</span></sub><span id="kobo.589.1" class="koboSpan"> = 1, </span><i><span id="kobo.590.1" class="koboSpan">n</span></i><sub><span id="kobo.591.1" class="koboSpan">2</span></sub><span id="kobo.592.1" class="koboSpan"> = 1.5, and this leads to </span><i><span id="kobo.593.1" class="koboSpan">r</span></i><span id="kobo.594.1" class="koboSpan"> = 0.2. </span><span id="kobo.594.2" class="koboSpan">This corresponds to a ratio of reflected light intensity of </span><i><span id="kobo.595.1" class="koboSpan">r</span></i><sub><span id="kobo.596.1" class="koboSpan">2</span></sub><span id="kobo.597.1" class="koboSpan"> = 0.04 or 4%. </span><span id="kobo.597.2" class="koboSpan">It is important to point out that this ratio of light intensity is reflected at all boundaries between materials, and therefore it is specifically undesirable for large stacks of compound lenses.</span></span></p>
<p><span id="kobo.598.1" class="koboSpan">The solution in this case is applying a layer of antireflective coating on each lens. </span><span id="kobo.598.2" class="koboSpan">This will not stop the reflection, but it will cancel out the reflected light to prevent the image from being distorted. </span><span id="kobo.598.3" class="koboSpan">The coating introduces a new boundary of reflection, and this results in two separate light reflections, one caused by the coating and the second by the glass, as depicted in </span><a id="rfig2-14" href="chapter_2.xhtml#fig2-14"><span id="kobo.599.1" class="koboSpan">Figure 2.14</span></a><span id="kobo.600.1" class="koboSpan">. </span><span id="kobo.600.2" class="koboSpan">The two reflected rays are subject to the following conditions:</span></p>
<ul class="numbered">
<li class="NL"><span id="kobo.601.1" class="koboSpan">1. </span><span id="kobo.601.2" class="koboSpan">The rays should have identical intensities.</span></li>
<li class="NL"><span id="kobo.602.1" class="koboSpan">2. </span><span id="kobo.602.2" class="koboSpan">The phases of the rays should be opposite.</span></li>
</ul>
<div class="figure">
<figure class="IMG"><a id="fig2-14"><span id="kobo.603.1" class="koboSpan"><img class="img1" src="../images/Figure2-14.png"/></span>
</a><figcaption><a id="fig2-14"></a><p class="CAP"><a id="fig2-14"><span class="FIGN"></span></a><a href="#rfig2-14"><span id="kobo.604.1" class="koboSpan">Figure 2.14</span></a> <span class="FIG"><span id="kobo.605.1" class="koboSpan">Functioning principle of antireflective coating: the incoming light ray is reflected twice by the coating layer and the glass. </span><span id="kobo.605.2" class="koboSpan">The two reflections are in opposite phases and thus cancel each other. </span><span id="kobo.605.3" class="koboSpan">The sinusoidal curves do not represent the ray paths but rather their intensities.</span></span></p></figcaption>
</figure>
</div>
<p><span id="pg_30"><span id="kobo.606.1" class="koboSpan">These two conditions ensure that the reflections cancel each other, as shown in </span><a href="chapter_2.xhtml#fig2-14"><span id="kobo.607.1" class="koboSpan">Figure 2.14</span></a><span id="kobo.608.1" class="koboSpan">. </span><span id="kobo.608.2" class="koboSpan">Let </span><i><span id="kobo.609.1" class="koboSpan">n</span></i><sub><span id="kobo.610.1" class="koboSpan">1</span></sub><span id="kobo.611.1" class="koboSpan">, </span><i><span id="kobo.612.1" class="koboSpan">n</span></i><sub><span id="kobo.613.1" class="koboSpan">2</span></sub><span id="kobo.614.1" class="koboSpan">, </span><i><span id="kobo.615.1" class="koboSpan">n</span></i><sub><span id="kobo.616.1" class="koboSpan">3</span></sub><span id="kobo.617.1" class="koboSpan"> be the refractive indices of the three regions. </span><span id="kobo.617.2" class="koboSpan">Assuming that </span><i><span id="kobo.618.1" class="koboSpan">n</span></i><sub><span id="kobo.619.1" class="koboSpan">1</span></sub> <i><span id="kobo.620.1" class="koboSpan">&lt; n</span></i><sub><span id="kobo.621.1" class="koboSpan">2</span></sub> <i><span id="kobo.622.1" class="koboSpan">&lt; n</span></i><sub><span id="kobo.623.1" class="koboSpan">3</span></sub><span id="kobo.624.1" class="koboSpan">, condition 1 can be written</span></span></p>
<p class="DIS-IMG"><span id="kobo.625.1" class="koboSpan"><img class="img1" src="../images/pg30-1.png"/></span></p>
<p class="noindent"><span id="kobo.626.1" class="koboSpan">It is easy to see that this condition is satisfied for </span><span id="kobo.627.1" class="koboSpan"><img class="inline" src="../images/pg30-in-1.png"/></span><span id="kobo.628.1" class="koboSpan">, which gives us the refractive index of the coating layer.</span></p>
<p><span id="kobo.629.1" class="koboSpan">Condition 2 is satisfied by choosing the thickness of the coating layer </span><i><span id="kobo.630.1" class="koboSpan">d</span></i><span id="kobo.631.1" class="koboSpan"> = </span><i><span><span id="kobo.632.1" class="koboSpan">λ</span></span><span id="kobo.633.1" class="koboSpan">/</span></i><span id="kobo.634.1" class="koboSpan">4, where </span><i><span><span id="kobo.635.1" class="koboSpan">λ</span></span></i><span id="kobo.636.1" class="koboSpan"> is the wavelength of the incoming light. </span><span id="kobo.636.2" class="koboSpan">This ensures that by the time the second reflection travels through the coating layer twice, which amounts to half its wavelength, its phase is opposite to that of the first reflection and thus it cancels the first reflection.</span></p>
<p><span id="kobo.637.1" class="koboSpan">As before, this correction works for one wavelength only. </span><span id="kobo.637.2" class="koboSpan">Typically lenses have two or three coating layers to cover a larger portion of the spectrum.</span></p>
</section>
<section>
<h4 id="sec7" class="head b-head"><a id="sec2-1-6"><span id="kobo.638.1" class="koboSpan">2.1.6 Lenses and Focus</span></a></h4><a id="sec2-1-6">
<p class="noindent"><span id="kobo.639.1" class="koboSpan">In the previous subsection we examined how Snell’s law determines when a point in the scene is in focus. </span><span id="kobo.639.2" class="koboSpan">However, Snell’s law cannot be used on a regular basis to focus a camera in a real-life scenario, because the length </span><i><span id="kobo.640.1" class="koboSpan">S</span></i><sub><span id="kobo.641.1" class="koboSpan">1</span></sub><span id="kobo.642.1" class="koboSpan"> from the lens to the object is usually unknown.</span></p>
<p><span id="kobo.643.1" class="koboSpan">In the early days of photography, focusing was accomplished by moving the lens manually to maximize the image contrast.</span></p>
</a><p><a id="sec2-1-6"><span id="kobo.644.1" class="koboSpan">One of the main methods to focus a camera is phase-based autofocus. </span><span id="kobo.644.2" class="koboSpan">This method was used in 1977 for the first autofocus camera, the Konica C35 AF. </span><span id="kobo.644.3" class="koboSpan">The autofocus system measures the light intensity on the sensor originating from the two halves of the lens. </span><span id="kobo.644.4" class="koboSpan">Each half generates an intensity curve, as shown in </span></a><a id="rfig2-15" href="chapter_2.xhtml#fig2-15"><span id="kobo.645.1" class="koboSpan">Figure 2.15</span></a><span id="kobo.646.1" class="koboSpan">. </span><span id="kobo.646.2" class="koboSpan">The lens moves relative to the sensor plane until the two curves are in phase, which ensures that the camera is in focus.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-15"><span id="kobo.647.1" class="koboSpan"><img class="img1" src="../images/Figure2-15.png"/></span>
</a><figcaption><a id="fig2-15"></a><p class="CAP"><a id="fig2-15"><span class="FIGN"></span></a><a href="#rfig2-15"><span id="kobo.648.1" class="koboSpan">Figure 2.15</span></a> <span class="FIG"><span id="kobo.649.1" class="koboSpan">Lens bending rays reflected by an object: cases in which the intersection between the rays from the upper and lower halves of the lens falls (a) onto the sensor plane, (c) in front of the sensor plane, and (e) behind the sensor plane; and the corresponding intensities along the sensor plane (b), (d), and (f). </span><span id="kobo.649.2" class="koboSpan">The green and red curves in (d) and (f) show how the two lens halves generate out-of-phase intensity functions when the object is out of focus. </span><span id="kobo.649.3" class="koboSpan">The intensity curves can then be measured independently and their phase difference used to compute the direction and distance of the lens motion. </span><span id="kobo.649.4" class="koboSpan">Reprinted from (Ramanath et al., 2005).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.650.1" class="koboSpan">Current autofocus cameras embed the lens-translating motor in the lens itself. </span><span id="kobo.650.2" class="koboSpan">Let us look in more detail at the functioning principle of phase-based autofocus. </span><span id="kobo.650.3" class="koboSpan">As shown in </span><a href="chapter_2.xhtml#fig2-15"><span id="kobo.651.1" class="koboSpan">Figure 2.15</span></a><span id="kobo.652.1" class="koboSpan">, the system requires measurements of the intensity curves from different parts of the lens. </span><span id="kobo.652.2" class="koboSpan">However, this is not possible using only one lens and a sensor. </span><span id="kobo.652.3" class="koboSpan">The system uses a beam splitter to measure light phase coming from the opposite sides of the lens using one-dimensional (1D) sensor arrays. </span><span id="kobo.652.4" class="koboSpan">The intensity curves can then be measured independently and their phase difference used to compute the direction and distance of the lens motion.</span></p>
<p><span id="kobo.653.1" class="koboSpan">Alternatively, a camera can use </span><i><span id="kobo.654.1" class="koboSpan">contrast-based autofocus</span></i><span id="kobo.655.1" class="koboSpan">. </span><span id="kobo.655.2" class="koboSpan">This mechanism involves a sensor that calculates the contrast as the difference in light intensity of nearby pixels. </span><span id="kobo.655.3" class="koboSpan">Unlike phase-based autofocus, the direction of movement cannot be derived immediately and requires a search routine. </span><span id="kobo.655.4" class="koboSpan">Therefore contrast-based autofocus is slower, as used in some smaller setups such as cell-phone cameras.</span></p>
<p><span id="pg_31"><span id="kobo.656.1" class="koboSpan">A third category of focusing methods is </span><i><span id="kobo.657.1" class="koboSpan">active autofocus</span></i><span id="kobo.658.1" class="koboSpan">. </span><span id="kobo.658.2" class="koboSpan">This method involves measuring the distance to the object independently using ultrasound waves or infrared light. </span><span id="kobo.658.3" class="koboSpan">This </span><span id="pg_32"><span id="kobo.659.1" class="koboSpan">principle does not require a minimal contrast in the scene to work, and it generally leads to lower performance.</span></span></span></p>
<p><span id="kobo.660.1" class="koboSpan">In addition to choosing the best focusing mechanism, the photographer needs to choose the focus plane. </span><span id="kobo.660.2" class="koboSpan">A scene has several planes on which the camera can focus. </span><span id="kobo.660.3" class="koboSpan">Some cameras automatically focus on the objects that are closest, are brightest, or have the highest contrast. </span><span id="kobo.660.4" class="koboSpan">Modern cameras can perform live face detection to pick the focus plane, or allow the user to manually select the plane, typically by tapping on the desired scene point on a touchscreen.</span></p>
</section>
<section>
<h4 id="sec8" class="head b-head"><a id="sec2-1-7"><span id="kobo.661.1" class="koboSpan">2.1.7 Masks and Aperture Manipulation</span></a></h4><a id="sec2-1-7">
<p class="noindent"><span id="kobo.662.1" class="koboSpan">In the previous subsection we saw that adding an obstacle in the light pathway, a mask with a rectangular window in its center, allowed separating the light coming from different parts of the lens and revealed information that otherwise would have been unknown, that is, the level of focus on an object in the scene. </span><span id="kobo.662.2" class="koboSpan">Generally speaking, masks represent planar elements that occlude or attenuate light rays in a spatially varying fashion.</span></p>
<p><span id="kobo.663.1" class="koboSpan">Interestingly, it is possible to fully replace the lens with a mask. </span><span id="kobo.663.2" class="koboSpan">This allows extracting information from the image that is not available with lens imaging. </span><span id="kobo.663.3" class="koboSpan">Compared with a pinhole camera, which uses a mask with a single hole, this approach has a much higher light throughput, leading to brighter images. </span><span id="kobo.663.4" class="koboSpan">A drawback however is that the image requires postprocessing. </span><span id="kobo.663.5" class="koboSpan">This idea is elaborated for a variety of applications in chapter 4.</span></p>
</a><p><a id="sec2-1-7"><span id="kobo.664.1" class="koboSpan">One example of such an application in photography is the modified uniformly redundant array (MURA) architecture, which gathers around 22, 000 times more light than a pinhole camera by using a mask that is almost 50% empty (Gottesman and Fenimore, 1989). </span><span id="kobo.664.2" class="koboSpan">The mask is defined by a binary matrix </span><i><span id="kobo.665.1" class="koboSpan">A</span></i><sub><i><span id="kobo.666.1" class="koboSpan">i, j</span></i></sub><span id="kobo.667.1" class="koboSpan"> such that a value 0 represents an occluder and a 1 denotes a gap allowing the light to pass through. </span><span id="kobo.667.2" class="koboSpan">The matrix </span><i><span id="kobo.668.1" class="koboSpan">A</span></i><sub><i><span id="kobo.669.1" class="koboSpan">i, j</span></i></sub><span id="kobo.670.1" class="koboSpan"> is displayed in </span></a><a id="rfig2-16" href="chapter_2.xhtml#fig2-16"><span id="kobo.671.1" class="koboSpan">Figure 2.16a</span></a><span id="kobo.672.1" class="koboSpan">, in which white corresponds to 1 and black corresponds to 0.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-16"><span id="kobo.673.1" class="koboSpan"><img class="img1" src="../images/Figure2-16.png"/></span>
</a><figcaption><a id="fig2-16"></a><p class="CAP"><a id="fig2-16"><span class="FIGN"></span></a><a href="#rfig2-16"><span id="kobo.674.1" class="koboSpan">Figure 2.16</span></a> <span class="FIG"><span id="kobo.675.1" class="koboSpan">Example of lensless MURA photography: (a) mask used to capture the image, (b) detected image, (c) image reconstructed from the measurements, and (</span><i><span id="kobo.676.1" class="koboSpan">bottom row</span></i><span id="kobo.677.1" class="koboSpan">) MURA patterns with different matrix sizes.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.678.1" class="koboSpan">The raw captured image, depicted in </span><a href="chapter_2.xhtml#fig2-16"><span id="kobo.679.1" class="koboSpan">Figure 2.16b</span></a><span id="kobo.680.1" class="koboSpan">, does not reveal a lot about the scene. </span><span id="kobo.680.2" class="koboSpan">However, a decoding algorithm allows recovering a high quality image, as shown in </span><a href="chapter_2.xhtml#fig2-16"><span id="kobo.681.1" class="koboSpan">Figure 2.16c</span></a><span id="kobo.682.1" class="koboSpan">. </span><span id="kobo.682.2" class="koboSpan">Further examples of MURA patterns with different matrix sizes are shown in </span><a href="chapter_2.xhtml#fig2-16"><span id="kobo.683.1" class="koboSpan">Figure 2.16</span></a><span id="kobo.684.1" class="koboSpan"> (</span><i><span id="kobo.685.1" class="koboSpan">bottom row</span></i><span id="kobo.686.1" class="koboSpan">).</span></p>
<p><span id="kobo.687.1" class="koboSpan">The concept was later extended by replacing the lens with a series of light-attenuating layers that can be controlled in both time and space (Zomet and Nayar, 2006). </span><span id="kobo.687.2" class="koboSpan">This setup allows extracting more information from the measurements, such as changing the viewing angle after the image has been captured.</span></p>
<p><span id="kobo.688.1" class="koboSpan">Masks have also been used together with lenses to generate images. </span><span id="kobo.688.2" class="koboSpan">The mask can be positioned in one of three places relative to the imaging system, as shown in </span><a id="rfig2-17" href="chapter_2.xhtml#fig2-17"><span id="kobo.689.1" class="koboSpan">Figure 2.17</span></a><span id="kobo.690.1" class="koboSpan">:</span></p>
<ul class="numbered">
<li class="NL"><span id="kobo.691.1" class="koboSpan">1. </span><span id="kobo.691.2" class="koboSpan">on the camera aperture,</span></li>
<li class="NL"><span id="kobo.692.1" class="koboSpan">2. </span><span id="kobo.692.2" class="koboSpan">on the sensor, or</span></li>
<li class="NL"><span id="kobo.693.1" class="koboSpan">3. </span><span id="kobo.693.2" class="koboSpan">on the scene.</span></li>
</ul>
<div class="figure">
<figure class="IMG"><a id="fig2-17"><span id="kobo.694.1" class="koboSpan"><img class="img2" src="../images/Figure2-17.png"/></span>
</a><figcaption><a id="fig2-17"></a><p class="CAP"><a id="fig2-17"><span class="FIGN"></span></a><a href="#rfig2-17"><span id="kobo.695.1" class="koboSpan">Figure 2.17</span></a> <span class="FIG"><span id="kobo.696.1" class="koboSpan">Positions for placing a mask in a camera.</span></span></p></figcaption>
</figure>
</div>
<p><span id="pg_33"><span id="kobo.697.1" class="koboSpan">Masks placed at the aperture level reveal many interesting properties of the scene. </span><span id="kobo.697.2" class="koboSpan">For example, (Farid and Simoncelli, 1998) computed the differentiation of the image intensity that is passed through a mask as the camera viewpoint changes. </span><span id="kobo.697.3" class="koboSpan">They demonstrated that two masks can be applied such that the derivatives calculated for each mask can be used to estimate the range of the scene. </span><span id="kobo.697.4" class="koboSpan">Later it was also shown that both conventional photographs and the corresponding depth maps can be recovered by placing a mask at the aperture of a consumer grade camera (Levin et al., 2007), therefore applying a technique known as </span><i><span id="kobo.698.1" class="koboSpan">depth from defocus</span></i><span id="kobo.699.1" class="koboSpan">. </span><span id="kobo.699.2" class="koboSpan">Single view depth estimation can also be achieved using an end-to-end design approach, jointly designing the mask and the reconstruction algorithm (Wu et al., 2019).</span></span></p>
<p><span id="kobo.700.1" class="koboSpan">Rather than applying a mask, the camera aperture can be programmed to act like a mask. </span><span id="kobo.700.2" class="koboSpan">Taking multiple photos with various aperture sizes and shapes can be used to generate an image with much higher spatial resolution (Liang et al., 2008, 2007). </span><span id="kobo.700.3" class="koboSpan">(Sinha et al., 2017) used a lensless imaging system and model the inverse transform as a deep neural network to obtain the image phase. </span><span id="kobo.700.4" class="koboSpan">Further examples and several other state-of-the-art implementations are discussed in the context of spatially coded imaging in section 4.1.5.</span></p>
<p><span id="pg_34"><span id="kobo.701.1" class="koboSpan">The second option for positioning a mask is on the sensor. </span><span id="kobo.701.2" class="koboSpan">The most common example, existing on many consumer grade cameras, is the Bayer filter. </span><span id="kobo.701.3" class="koboSpan">A color image is made up of three images, each capturing the intensity of the color red, green, or blue. </span><span id="kobo.701.4" class="koboSpan">Using the Bayer filter all three images can be captured at once with one single sensor array, by using pixels sensitive to each of the RGB colors arranged in a pattern called the Bayer pattern.</span></span></p>
<p><span id="kobo.702.1" class="koboSpan">It is possible to project light onto only selected regions of a sensor, the equivalent of placing a mask on a sensor, by using a digital micromirror device (DMD). </span><span id="kobo.702.2" class="koboSpan">A DMD is essentially an array of micromirrors that have two possible orientations, reflecting the light either toward the sensor array or away from it. </span><span id="kobo.702.3" class="koboSpan">A DMD is used for computing high dynamic range images and also for performing object recognition (Nayar et al., 2006). </span><span id="kobo.702.4" class="koboSpan">Using a DMD allows reducing the sensor array to a single pixel (Takhar et al., 2006). </span><span id="kobo.702.5" class="koboSpan">This is done by generating various patterns with the DMD and directing the accumulated light toward the pixel sensor. </span><span id="kobo.702.6" class="koboSpan">This system was used together with compressive sensing theory to generate images of reduced size (Takhar et al., 2006).</span></p>
<p><span id="kobo.703.1" class="koboSpan">Placing a mask on the sensor or lens is easy to imagine, but placing a mask on the scene would be more difficult. </span><span id="kobo.703.2" class="koboSpan">In this case it is common to simply illuminate certain parts of a scene while leaving others in darkness, which has a similar effect. </span><span id="kobo.703.3" class="koboSpan">This has been done to extract the two sources of illumination in a scene: the direct illumination by the source and the global illumination from other points in the scene (Nayar et al., 2006). </span><span id="kobo.703.4" class="koboSpan">This separation of the sources has a practical significance because each one reveals different information about the scene. </span><span id="kobo.703.5" class="koboSpan">The direct component enhances the material properties of a given point, and the global component reveals the optical properties of the scene, indicating how a certain point is illuminated by other points in the scene.</span></p>
</section>
</section>
<section>
<h3 id="sec9" class="head a-head"><span id="pg_35"><a id="sec2-2"><span id="kobo.704.1" class="koboSpan">2.2 Image Sensors</span></a></span></h3><a id="sec2-2">
</a><section><a id="sec2-2">
</a><h4 id="sec10" class="head b-head"><a id="sec2-2"></a><a id="sec2-2-1"><span id="kobo.705.1" class="koboSpan">2.2.1 Cameras, Rays, and Radiance</span></a></h4><a id="sec2-2-1">
<p class="noindent"><span id="kobo.706.1" class="koboSpan">A point in the scene is imaged by measuring the emitted/reflected light that converges on the sensor plane. </span><span id="kobo.706.2" class="koboSpan">However a real sensor detects the light irradiance, which is zero if the light is absorbed by the sensor only at a point. </span><span id="kobo.706.3" class="koboSpan">A single-pixel detector measures the irradiance in the vicinity of the point receiving light from the scene.</span></p>
<p><span id="kobo.707.1" class="koboSpan">Therefore the detected brightness depends on the area on the sensor that a light beam covers. </span><span id="kobo.707.2" class="koboSpan">In the previous section we looked at how light falling at an angle leads to the </span><i><span id="kobo.708.1" class="koboSpan">cosine falloff</span></i><span id="kobo.709.1" class="koboSpan"> effect, in which the irradiance decreases with the cosine of the light incident angle. </span><span id="kobo.709.2" class="koboSpan">One may then wonder why when one looks at a screen at an angle the image brightness does not appear to change. </span><span id="kobo.709.3" class="koboSpan">The explanation is that because light has an incident angle, the new perspective captures more light rays and therefore the increased radiant intensity compensates for the larger area covered by the light beam on the sensor.</span></p>
<p><span id="kobo.710.1" class="koboSpan">After they are processed by the camera lenses, the light rays hit the sensor, which converts them into electrical signals. </span><span id="kobo.710.2" class="koboSpan">The sensor affects the appearance of the final image with a range of parameters. </span><span id="kobo.710.3" class="koboSpan">First, the </span><i><span id="kobo.711.1" class="koboSpan">light sensitivity</span></i><span id="kobo.712.1" class="koboSpan">, or ISO, can be used to brighten dark images. </span><span id="kobo.712.2" class="koboSpan">However, increasing the ISO also generates a noisier image. </span><span id="kobo.712.3" class="koboSpan">Moreover, the </span><i><span id="kobo.713.1" class="koboSpan">dynamic range</span></i><span id="kobo.714.1" class="koboSpan">, defined as the range of luminance the sensor is able to detect, has a strong impact on the clarity of the resulting image, especially if it contains both bright and dark areas. </span><span id="kobo.714.2" class="koboSpan">The </span><i><span id="kobo.715.1" class="koboSpan">tonal range</span></i><span id="kobo.716.1" class="koboSpan">, on the other hand, is given by the actual number of tones captured by the camera and is affected by other sensor parameters such as ISO. </span><span id="kobo.716.2" class="koboSpan">The sensitivity of the sensor to color is called </span><i><span id="kobo.717.1" class="koboSpan">wavelength sensitivity</span></i><span id="kobo.718.1" class="koboSpan"> and can be adjusted on most cameras using the color balance or saturation settings. </span><span id="kobo.718.2" class="koboSpan">The clarity of an image is, of course, strongly influenced by the sensor </span><i><span id="kobo.719.1" class="koboSpan">spatial resolution</span></i><span id="kobo.720.1" class="koboSpan">, which is the number of pixels on the sensor.</span></p>
</a><p><a id="sec2-2-1"><span id="kobo.721.1" class="koboSpan">To view an object clearly, it must be at </span><i><span id="kobo.722.1" class="koboSpan">focusing distance</span></i><span id="kobo.723.1" class="koboSpan"> from the lens. </span><span id="kobo.723.2" class="koboSpan">The reason is that the light rays reflected by the points at focus distance converge on the sensor. </span><span id="kobo.723.3" class="koboSpan">When the camera focuses on a different object, it changes the distance between the sensor and the lens, which in turns changes the focusing distance. </span><span id="kobo.723.4" class="koboSpan">A focus measure can be used to determine the right lens position to keep an image in focus. </span></a><a id="rfig2-18" href="chapter_2.xhtml#fig2-18"><span id="kobo.724.1" class="koboSpan">Figure 2.18</span></a><span id="kobo.725.1" class="koboSpan"> shows the focus measure as a function of a lens’s position and three captured images with different lens positions.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-18"><span id="kobo.726.1" class="koboSpan"><img class="img1" src="../images/Figure2-18.png"/></span>
</a><figcaption><a id="fig2-18"></a><p class="CAP"><a id="fig2-18"><span class="FIGN"></span></a><a href="#rfig2-18"><span id="kobo.727.1" class="koboSpan">Figure 2.18</span></a> <span class="FIG"><span id="kobo.728.1" class="koboSpan">Imaging for different lens positions: (a)–(c) show an image captured with a gradually improved focus and (d) shows focus measure as a function of the lens position, which is maximized when the image is in focus. </span><span id="kobo.728.2" class="koboSpan">Reprinted from (Ramanath et al., 2005).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.729.1" class="koboSpan">The depth of field is the distance between the closest and farthest points in the scene on which the camera can focus. </span><span id="kobo.729.2" class="koboSpan">The longest depth of field is achieved by a camera with short focal length and small aperture. </span><span id="kobo.729.3" class="koboSpan">Even if points in the distant part of the scene may be out of focus, a small aperture decreases how much the light rays diverge when they arrive at the sensor. </span><span id="kobo.729.4" class="koboSpan">The human eye is subject to the same effect. </span><span id="kobo.729.5" class="koboSpan">That is why squinting can help focus better on objects that are far away.</span></p>
<p><span id="pg_36"><span id="kobo.730.1" class="koboSpan">It can be observed that in a camera, achieving a long depth of field will typically lead to a darker image due to the small aperture. </span><span id="kobo.730.2" class="koboSpan">This can be fixed by increasing the exposure time. </span><span id="kobo.730.3" class="koboSpan">If the scene is dynamic, this would cause motion blur. </span><span id="kobo.730.4" class="koboSpan">Increasing the ISO instead addresses the brightness issue but adds noise to the measurements.</span></span></p>
<p><span id="kobo.731.1" class="koboSpan">The sensor is affected by several types of noise, and each type is more prevalent in certain imaging environments. </span><span id="kobo.731.2" class="koboSpan">The </span><i><span id="kobo.732.1" class="koboSpan">read noise</span></i><span id="kobo.733.1" class="koboSpan"> consists of the sensor pixel noise and the noise generated by the analog-to-digital converter. </span><span id="kobo.733.2" class="koboSpan">This noise determines the contrast of the captured image and affects the CMOS and the CCD sensors differently, because in the CCD the ADC is not part of the actual sensor. </span><span id="kobo.733.3" class="koboSpan">The </span><i><span id="kobo.734.1" class="koboSpan">shot noise</span></i><span id="kobo.735.1" class="koboSpan"> results from the discrete nature of electrons captured by the potential well, and is more prevalent in brighter environments. </span><span id="kobo.735.2" class="koboSpan">A larger potential well is needed to address this type of noise.</span></p>
<p><span id="kobo.736.1" class="koboSpan">Mathematically, the read noise is considered the Gaussian part of the noise, and is caused by stationary disturbances, and the shot noise is the Poissonian part, and is caused by the sensing of photons. </span><span id="kobo.736.2" class="koboSpan">Their names arise from the distributions that model their values. </span><span id="kobo.736.3" class="koboSpan">As the screens have higher and higher resolutions, the pixel sizes decrease and the sensitivity to photon noise increases for each pixel. </span><span id="kobo.736.4" class="koboSpan">Thus the shot noise is currently the main contributor to noise in imaging sensors.</span></p>
<p><span id="kobo.737.1" class="koboSpan">The overall noise is thus signal dependent and very different from the usual additive white Gaussian noise that is common in image processing. </span><span id="kobo.737.2" class="koboSpan">The limited dynamic range of pixels </span><span id="pg_37"><span id="kobo.738.1" class="koboSpan">leads in many cases to overexposure, or capturing light close to the maximum capacity per pixel. </span><span id="kobo.738.2" class="koboSpan">This effect further enhances signal-dependent noise.</span></span></p>
<p><span id="kobo.739.1" class="koboSpan">The measurements </span><i><span id="kobo.740.1" class="koboSpan">z</span></i><span id="kobo.741.1" class="koboSpan">(</span><i><span id="kobo.742.1" class="koboSpan">x, y</span></i><span id="kobo.743.1" class="koboSpan">) from a sensor at each pixel are given</span></p>
<p class="DIS-IMG"><span id="kobo.744.1" class="koboSpan"><img class="img1" src="../images/pg37-1.png"/></span></p>
<p class="noindent"><span id="kobo.745.1" class="koboSpan">where </span><i><span id="kobo.746.1" class="koboSpan">x</span></i><span id="kobo.747.1" class="koboSpan">, </span><i><span id="kobo.748.1" class="koboSpan">y</span></i><span id="kobo.749.1" class="koboSpan"> is the pixel position in two dimensions (2D), </span><i><span id="kobo.750.1" class="koboSpan">I</span></i><span id="kobo.751.1" class="koboSpan">(</span><i><span id="kobo.752.1" class="koboSpan">x, y</span></i><span id="kobo.753.1" class="koboSpan">) is the light signal, </span><i><span><span id="kobo.754.1" class="koboSpan">ζ</span></span></i><span id="kobo.755.1" class="koboSpan">(</span><i><span id="kobo.756.1" class="koboSpan">x, y</span></i><span id="kobo.757.1" class="koboSpan">) is the Gaussian noise of 0 mean and standard deviation 1, and </span><i><span><span id="kobo.758.1" class="koboSpan">σ</span></span></i><span id="kobo.759.1" class="koboSpan">(</span><i><span id="kobo.760.1" class="koboSpan">I</span></i><span id="kobo.761.1" class="koboSpan">(</span><i><span id="kobo.762.1" class="koboSpan">x, y</span></i><span id="kobo.763.1" class="koboSpan">)) is the signal-dependent standard deviation. </span><span id="kobo.763.2" class="koboSpan">The aim of the preceding equation is to estimate </span><i><span id="kobo.764.1" class="koboSpan">I</span></i><span id="kobo.765.1" class="koboSpan">(</span><i><span id="kobo.766.1" class="koboSpan">x, y</span></i><span id="kobo.767.1" class="koboSpan">) and </span><i><span><span id="kobo.768.1" class="koboSpan">σ</span></span></i><span id="kobo.769.1" class="koboSpan">(</span><i><span id="kobo.770.1" class="koboSpan">I</span></i><span id="kobo.771.1" class="koboSpan">(</span><i><span id="kobo.772.1" class="koboSpan">x, y</span></i><span id="kobo.773.1" class="koboSpan">)) from measurements </span><i><span id="kobo.774.1" class="koboSpan">z</span></i><span id="kobo.775.1" class="koboSpan">(</span><i><span id="kobo.776.1" class="koboSpan">x, y</span></i><span id="kobo.777.1" class="koboSpan">).</span></p>
<p><span id="kobo.778.1" class="koboSpan">To separate the influences of Gaussian and Poissonian noise, we write the measurements deviation (Foi et al., 2008)</span></p>
<p class="DIS-IMG"><span id="kobo.779.1" class="koboSpan"><img class="img1" src="../images/pg37-2.png"/></span></p>
<p><span id="kobo.780.1" class="koboSpan">The noise distributions can be written as follows</span></p>
<p class="DIS-IMG"><span id="kobo.781.1" class="koboSpan"><img class="img1" src="../images/pg37-3.png"/></span></p>
<p class="noindent"><span id="kobo.782.1" class="koboSpan">where </span><i><span id="kobo.783.1" class="koboSpan">P</span></i><span id="kobo.784.1" class="koboSpan">(</span><i><span id="kobo.785.1" class="koboSpan">r</span></i><span id="kobo.786.1" class="koboSpan">) is the Poisson distribution, and 풩(</span><i><span id="kobo.787.1" class="koboSpan">m, v</span></i><span id="kobo.788.1" class="koboSpan">) is the Gaussian distribution with mean </span><i><span id="kobo.789.1" class="koboSpan">m</span></i><span id="kobo.790.1" class="koboSpan"> and variance </span><i><span id="kobo.791.1" class="koboSpan">v</span></i><span id="kobo.792.1" class="koboSpan">. </span><span id="kobo.792.2" class="koboSpan">The standard deviation has the expression (Foi et al., 2008)</span></p>
<p class="DIS-IMG"><span id="kobo.793.1" class="koboSpan"><img class="img1" src="../images/pg37-4.png"/></span></p>
<p><span id="kobo.794.1" class="koboSpan">Denoising is a common process in signal processing. </span><span id="kobo.794.2" class="koboSpan">However, removing a signal-dependent noise is a more challenging task. </span><span id="kobo.794.3" class="koboSpan">In (Foi et al., 2008), the authors employ an algorithm in several steps to recover the image </span><i><span id="kobo.795.1" class="koboSpan">I</span></i><span id="kobo.796.1" class="koboSpan">(</span><i><span id="kobo.797.1" class="koboSpan">x, y</span></i><span id="kobo.798.1" class="koboSpan">) and also estimate the varying standard deviation </span><i><span><span id="kobo.799.1" class="koboSpan">σ</span></span></i><span id="kobo.800.1" class="koboSpan">(</span><i><span id="kobo.801.1" class="koboSpan">I</span></i><span id="kobo.802.1" class="koboSpan">(</span><i><span id="kobo.803.1" class="koboSpan">x, y</span></i><span id="kobo.804.1" class="koboSpan">)). </span><span id="kobo.804.2" class="koboSpan">First, the image needs to be divided in smooth regions. </span><span id="kobo.804.3" class="koboSpan">To this end, they employ edge detection via segmentation. </span><span id="kobo.804.4" class="koboSpan">Second, they compute a </span><i><span id="kobo.805.1" class="koboSpan">local estimation</span></i><span id="kobo.806.1" class="koboSpan"> of the standard deviation in the smooth regions. </span><span id="kobo.806.2" class="koboSpan">This estimation is based on the assumption that the changing standard deviation is relatively constant in a local small region. </span><span id="kobo.806.3" class="koboSpan">Last, a </span><i><span id="kobo.807.1" class="koboSpan">global model</span></i><span id="kobo.808.1" class="koboSpan"> of the noise is fit using local measurements.</span></p>
</section>
<section>
<h4 id="sec11" class="head b-head"><a id="sec2-2-2"><span id="kobo.809.1" class="koboSpan">2.2.2 Digital Image Formation</span></a></h4><a id="sec2-2-2">
<p class="noindent"><span id="kobo.810.1" class="koboSpan">The world as we see it using our eyes is a continuous three-dimensional function of the spatial coordinates. </span><span id="kobo.810.2" class="koboSpan">A photograph is a two-dimensional map of the </span><i><span id="kobo.811.1" class="koboSpan">number of photons</span></i><span id="kobo.812.1" class="koboSpan"> that map from the three-dimensional scene. </span><span id="kobo.812.2" class="koboSpan">In film-based photography, this map is a continuous function. </span><span id="kobo.812.3" class="koboSpan">However, in referring to a digital image, the corresponding two-dimensional function is a discrete representation because the number of pixels used for imaging are discrete and finitely many. </span><span id="kobo.812.4" class="koboSpan">Hence, we can think of an image as a mathematical representation of a physical entity that describes a function over spatial coordinates. </span><span id="kobo.812.5" class="koboSpan">The individual pixels are the basic elements of the discrete representation of the continuous </span><span id="pg_38"><span id="kobo.813.1" class="koboSpan">scene, and in analogy to the one-dimensional case, these are the samples of a function with reference to the Shannon-Nyquist sampling formula. </span><span id="kobo.813.2" class="koboSpan">Consequently, we must bear in mind that the image is merely a representation of the scene and not the continuous scene itself. </span><span id="kobo.813.3" class="koboSpan">To understand the basis of the image formation process, we must understand the physical laws that govern such a process.</span></span></p>
<p><span id="kobo.814.1" class="koboSpan">From a mathematical standpoint, the image can be seen as a mapping from the spatial domain to the range of the imaging sensor. </span><span id="kobo.814.2" class="koboSpan">Let </span><b><span class="ePub-B"><span id="kobo.815.1" class="koboSpan">r</span></span></b><span id="kobo.816.1" class="koboSpan"> refer to the spatial coordinates in the Cartesian plane; then the image </span><b><span class="ePub-B"><span id="kobo.817.1" class="koboSpan">I</span></span></b><span id="kobo.818.1" class="koboSpan">: </span><i><span id="kobo.819.1" class="koboSpan">S → P</span></i><span id="kobo.820.1" class="koboSpan"> is a mapping from the scene to the pixel domain such that each </span><b><span class="ePub-B"><span id="kobo.821.1" class="koboSpan">r</span></span></b><span id="kobo.822.1" class="koboSpan"> ∈ </span><i><span id="kobo.823.1" class="koboSpan">S</span></i><span id="kobo.824.1" class="koboSpan"> is mapped to </span><b><span class="ePub-B"><span id="kobo.825.1" class="koboSpan">I</span></span></b><span id="kobo.826.1" class="koboSpan">(</span><b><span class="ePub-B"><span id="kobo.827.1" class="koboSpan">r</span></span></b><span id="kobo.828.1" class="koboSpan">) ∈ </span><i><span id="kobo.829.1" class="koboSpan">P</span></i><span id="kobo.830.1" class="koboSpan">.</span></p>
<p><span id="kobo.831.1" class="koboSpan">In the case of color imaging, for every point </span><b><span class="ePub-B"><span id="kobo.832.1" class="koboSpan">r</span></span></b><span id="kobo.833.1" class="koboSpan"> in the two-dimensional space, we obtain three values per pixel, namely, the red, green, and blue values (in intensity). </span><span id="kobo.833.2" class="koboSpan">Hence, the resulting image can be represented as the following function that maps a vector to a vector,</span></p>
<p class="DIS-IMG"><span id="kobo.834.1" class="koboSpan"><img class="img1" src="../images/pg38-1.png"/></span></p>
<p class="noindent"><span id="kobo.835.1" class="koboSpan">In contrast, when working with monochromatic images, we have a simpler mapping of the form </span><b><span class="ePub-B"><span id="kobo.836.1" class="koboSpan">r</span></span></b><span id="kobo.837.1" class="koboSpan"> ∈ ℝ</span><sup><span id="kobo.838.1" class="koboSpan">2</span></sup> <i><span id="kobo.839.1" class="koboSpan">→</span></i> <b><span class="ePub-B"><span id="kobo.840.1" class="koboSpan">I</span></span></b><span id="kobo.841.1" class="koboSpan">(</span><b><span class="ePub-B"><span id="kobo.842.1" class="koboSpan">r</span></span></b><span id="kobo.843.1" class="koboSpan">) ∈ ℝ.</span></p>
<p><span id="kobo.844.1" class="koboSpan">For the data defined by </span><b><span class="ePub-B"><span id="kobo.845.1" class="koboSpan">I</span></span></b><span id="kobo.846.1" class="koboSpan">(</span><b><span class="ePub-B"><span id="kobo.847.1" class="koboSpan">r</span></span></b><span id="kobo.848.1" class="koboSpan">) to be stored on a computer, it needs to be processed in two stages: sampling and quantization. </span><span id="kobo.848.2" class="koboSpan">The luminance values, given by the values of </span><b><span class="ePub-B"><span id="kobo.849.1" class="koboSpan">I</span></span></b><span id="kobo.850.1" class="koboSpan">(</span><b><span class="ePub-B"><span id="kobo.851.1" class="koboSpan">r</span></span></b><span id="kobo.852.1" class="koboSpan">), are always positive and belong to a restricted interval.</span></p>
<p><span id="kobo.853.1" class="koboSpan">In order to be stored or processed by digital devices, the luminance value is mapped to a set of finite values, typically {0, 1, ⋯, 255}, which is also known as quantization. </span><span id="kobo.853.2" class="koboSpan">In the case of color images, each color is mapped to one of the 256 possible values. </span><span id="kobo.853.3" class="koboSpan">In the case of monochromatic images, some sensors employ a higher resolution, with values encoded in 12 bits, that is, in the range {0, 1, ⋯, 4095}. </span><span id="kobo.853.4" class="koboSpan">The choice of resolution is dependent on two factors: the images captured and the processing to be performed. </span><span id="kobo.853.5" class="koboSpan">For example, computed tomography (CT) images use more than 10 bits, while a low-grade webcam around 6 bits per color. </span><span id="kobo.853.6" class="koboSpan">More complex processing, such as gradient calculations, also requires a higher resolution for good results.</span></p>
<p><span id="kobo.854.1" class="koboSpan">Quantization guarantees that an image luminance can take only one of a finite number of possible values. </span><span id="kobo.854.2" class="koboSpan">However, the image has an infinite number of points. </span><span id="kobo.854.3" class="koboSpan">Therefore we need to sample the values of the images along each axis and define a new sampled image </span><b><span class="ePub-B"><span id="kobo.855.1" class="koboSpan">I</span></span></b><sub><i><span id="kobo.856.1" class="koboSpan">s</span></i></sub><span id="kobo.857.1" class="koboSpan">(</span><i><span id="kobo.858.1" class="koboSpan">i, j</span></i><span id="kobo.859.1" class="koboSpan">) = </span><b><span class="ePub-B"><span id="kobo.860.1" class="koboSpan">I</span></span></b><span id="kobo.861.1" class="koboSpan">(</span><i><span id="kobo.862.1" class="koboSpan">i</span></i><span><span id="kobo.863.1" class="koboSpan">Δ</span></span><i><span id="kobo.864.1" class="koboSpan">x, j</span></i><span><span id="kobo.865.1" class="koboSpan">Δ</span></span><i><span id="kobo.866.1" class="koboSpan">y</span></i><span id="kobo.867.1" class="koboSpan">), where </span><i><span id="kobo.868.1" class="koboSpan">r</span></i><sub><i><span id="kobo.869.1" class="koboSpan">i, j</span></i></sub><span id="kobo.870.1" class="koboSpan"> = (</span><i><span id="kobo.871.1" class="koboSpan">i</span></i><span><span id="kobo.872.1" class="koboSpan">Δ</span></span><i><span id="kobo.873.1" class="koboSpan">x, j</span></i><span><span id="kobo.874.1" class="koboSpan">Δ</span></span><i><span id="kobo.875.1" class="koboSpan">y</span></i><span id="kobo.876.1" class="koboSpan">) denotes the sampled spatial coordinates, and </span><span><span id="kobo.877.1" class="koboSpan">Δ</span></span><i><span id="kobo.878.1" class="koboSpan">x</span></i><span id="kobo.879.1" class="koboSpan"> and </span><span><span id="kobo.880.1" class="koboSpan">Δ</span></span><i><span id="kobo.881.1" class="koboSpan">y</span></i><span id="kobo.882.1" class="koboSpan"> denote the sampling distances along </span><i><span id="kobo.883.1" class="koboSpan">x</span></i><span id="kobo.884.1" class="koboSpan"> and </span><i><span id="kobo.885.1" class="koboSpan">y</span></i><span id="kobo.886.1" class="koboSpan">, respectively. </span><span id="kobo.886.2" class="koboSpan">Here, </span><b><span class="ePub-B"><span id="kobo.887.1" class="koboSpan">I</span></span></b><sub><i><span id="kobo.888.1" class="koboSpan">s</span></i></sub><span id="kobo.889.1" class="koboSpan">(</span><i><span id="kobo.890.1" class="koboSpan">i, j</span></i><span id="kobo.891.1" class="koboSpan">) represents a pixel. </span><span id="kobo.891.2" class="koboSpan">It is important to point out that the pixel is a point sample taken from the image, and is not a small square of measurable dimensions as commonly misconstrued. </span><span id="kobo.891.3" class="koboSpan">When the image is 3D instead of 2D, the pixel is called a voxel, which is a point sample in a 3D space.</span></p>
</a></section><a id="sec2-2-2">
</a><section><a id="sec2-2-2">
</a><h4 id="sec12" class="head b-head"><a id="sec2-2-2"><span id="pg_39"></span></a><a id="sec2-2-3"><span id="kobo.892.1" class="koboSpan">2.2.3 Image Interpolation</span></a></h4><a id="sec2-2-3">
<p class="noindent"><span id="kobo.893.1" class="koboSpan">After the continuous image has been sampled and quantized, it is important to be able to compute the values of the original continuous image at any desired coordinates. </span><span id="kobo.893.2" class="koboSpan">The process of computing the value of </span><b><span class="ePub-B"><span id="kobo.894.1" class="koboSpan">I</span></span></b><span id="kobo.895.1" class="koboSpan">(</span><i><span id="kobo.896.1" class="koboSpan">x, y</span></i><span id="kobo.897.1" class="koboSpan">) at locations different from the sampling points is called interpolation. </span><span id="kobo.897.2" class="koboSpan">One may ask whether the interpolation can work for any selection of the sampling distances </span><span><span id="kobo.898.1" class="koboSpan">Δ</span></span><i><span id="kobo.899.1" class="koboSpan">x</span></i><span id="kobo.900.1" class="koboSpan"> and </span><span><span id="kobo.901.1" class="koboSpan">Δ</span></span><i><span id="kobo.902.1" class="koboSpan">y</span></i><span id="kobo.903.1" class="koboSpan">? </span><span id="kobo.903.2" class="koboSpan">The answer is that the maximum sampling distances are a function of the image bandwidth, and the values represent an extension of the Shannon sampling theory, originally introduced for time samples. </span><span id="kobo.903.3" class="koboSpan">For now, assume that the sampling distances </span><span><span id="kobo.904.1" class="koboSpan">Δ</span></span><i><span id="kobo.905.1" class="koboSpan">x</span></i><span id="kobo.906.1" class="koboSpan"> and </span><span><span id="kobo.907.1" class="koboSpan">Δ</span></span><i><span id="kobo.908.1" class="koboSpan">y</span></i><span id="kobo.909.1" class="koboSpan"> are small enough that the samples are a good representation of the image. </span><span id="kobo.909.2" class="koboSpan">The luminance function </span><b><span class="ePub-B"><span id="kobo.910.1" class="koboSpan">I</span></span></b><span id="kobo.911.1" class="koboSpan">(</span><i><span id="kobo.912.1" class="koboSpan">x, y</span></i><span id="kobo.913.1" class="koboSpan">) is sampled along a two-dimensional space. </span><span id="kobo.913.2" class="koboSpan">Let us first look at some examples for interpolating one-dimensional functions </span><i><span id="kobo.914.1" class="koboSpan">g</span></i><span id="kobo.915.1" class="koboSpan">(</span><i><span id="kobo.916.1" class="koboSpan">x</span></i><span id="kobo.917.1" class="koboSpan">) using the sampled function </span><i><span id="kobo.918.1" class="koboSpan">g</span></i><sub><i><span id="kobo.919.1" class="koboSpan">S</span></i></sub><span id="kobo.920.1" class="koboSpan">(</span><i><span id="kobo.921.1" class="koboSpan">k</span></i><span id="kobo.922.1" class="koboSpan">) = </span><i><span id="kobo.923.1" class="koboSpan">g</span></i><span id="kobo.924.1" class="koboSpan">(</span><i><span id="kobo.925.1" class="koboSpan">k</span></i><span id="kobo.926.1" class="koboSpan">). </span><span id="kobo.926.2" class="koboSpan">Notice that for simplicity we use a sampling interval of 1.</span></p>
</a><p class="noindent-t"><a id="sec2-2-3"><span class="paragraphHead"><b><span id="kobo.927.1" class="koboSpan">Nearest neighbor interpolation.</span></b></span><span id="kobo.928.1" class="koboSpan"> As the name suggests, this method selects the sample value from the nearest sampling location without calculating a new value. </span><span id="kobo.928.2" class="koboSpan">This is a very computationally inexpensive interpolation method, but it does not always generate useful </span><span id="pg_40"><span id="kobo.929.1" class="koboSpan">results. </span><span id="kobo.929.2" class="koboSpan">The values of the new samples are computed </span><span id="kobo.930.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.931.1" class="koboSpan">(</span><i><span id="kobo.932.1" class="koboSpan">x</span></i><span id="kobo.933.1" class="koboSpan">) = </span><i><span id="kobo.934.1" class="koboSpan">g</span></i><sub><i><span id="kobo.935.1" class="koboSpan">S</span></i></sub><span id="kobo.936.1" class="koboSpan">([</span><i><span id="kobo.937.1" class="koboSpan">x</span></i><span id="kobo.938.1" class="koboSpan"> + 1/2]), where [·] denotes the round function, and [</span><i><span id="kobo.939.1" class="koboSpan">x</span></i><span id="kobo.940.1" class="koboSpan"> + 1/2] denotes the integer closest to </span><i><span id="kobo.941.1" class="koboSpan">x</span></i><span id="kobo.942.1" class="koboSpan">. </span></span></a><a id="rfig2-19" href="chapter_2.xhtml#fig2-19"><span id="kobo.943.1" class="koboSpan">Figure 2.19</span></a><span id="kobo.944.1" class="koboSpan"> shows function </span><span id="kobo.945.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.946.1" class="koboSpan">(</span><i><span id="kobo.947.1" class="koboSpan">x</span></i><span id="kobo.948.1" class="koboSpan">) for </span><i><span id="kobo.949.1" class="koboSpan">g</span></i><span id="kobo.950.1" class="koboSpan">(</span><i><span id="kobo.951.1" class="koboSpan">x</span></i><span id="kobo.952.1" class="koboSpan">) = sin(</span><i><span id="kobo.953.1" class="koboSpan">x</span></i><span id="kobo.954.1" class="koboSpan">). </span><span id="kobo.954.2" class="koboSpan">Note that the function closely resembles </span><i><span id="kobo.955.1" class="koboSpan">g</span></i><span id="kobo.956.1" class="koboSpan">(</span><i><span id="kobo.957.1" class="koboSpan">x</span></i><span id="kobo.958.1" class="koboSpan">) at the sample points and is very different from </span><i><span id="kobo.959.1" class="koboSpan">g</span></i><span id="kobo.960.1" class="koboSpan">(</span><i><span id="kobo.961.1" class="koboSpan">x</span></i><span id="kobo.962.1" class="koboSpan">) between the sampling points.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-19"><span id="kobo.963.1" class="koboSpan"><img class="img1" src="../images/Figure2-19.png"/></span>
</a><figcaption><a id="fig2-19"></a><p class="CAP"><a id="fig2-19"><span class="FIGN"></span></a><a href="#rfig2-19"><span id="kobo.964.1" class="koboSpan">Figure 2.19</span></a> <span class="FIG"><span id="kobo.965.1" class="koboSpan">Nearest neighbor interpolation: the original continuous function </span><i><span id="kobo.966.1" class="koboSpan">g</span></i><span id="kobo.967.1" class="koboSpan">(</span><i><span id="kobo.968.1" class="koboSpan">x</span></i><span id="kobo.969.1" class="koboSpan">) = sin(</span><i><span id="kobo.970.1" class="koboSpan">x</span></i><span id="kobo.971.1" class="koboSpan">) is shown by the dashed line, and the interpolation </span><span id="kobo.972.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.973.1" class="koboSpan">(</span><i><span id="kobo.974.1" class="koboSpan">x</span></i><span id="kobo.975.1" class="koboSpan">) is shown by the solid line.</span></span></p></figcaption>
</figure>
</div>
<div class="figure">
<figure class="IMG"><a id="fig2-20"><span id="kobo.976.1" class="koboSpan"><img class="img1" src="../images/Figure2-20.png"/></span>
<figcaption><p class="CAP"><span class="FIGN"><span id="kobo.977.1" class="koboSpan">Figure 2.20</span></span> <span class="FIG"><span id="kobo.978.1" class="koboSpan">Linear interpolation: the original continuous function </span><i><span id="kobo.979.1" class="koboSpan">g</span></i><span id="kobo.980.1" class="koboSpan">(</span><i><span id="kobo.981.1" class="koboSpan">x</span></i><span id="kobo.982.1" class="koboSpan">) = sin(</span><i><span id="kobo.983.1" class="koboSpan">x</span></i><span id="kobo.984.1" class="koboSpan">) is shown by the dashed line, and the interpolation </span><span id="kobo.985.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.986.1" class="koboSpan">(</span><i><span id="kobo.987.1" class="koboSpan">x</span></i><span id="kobo.988.1" class="koboSpan">) is shown by the solid line.</span></span></p></figcaption>
</a></figure><a id="fig2-20">
</a></div><a id="fig2-20">
<p class="noindent-t"><span class="paragraphHead"><b><span id="kobo.989.1" class="koboSpan">Linear interpolation.</span></b></span><span id="kobo.990.1" class="koboSpan"> This is a slightly more complex interpolation with improved results. </span><span id="kobo.990.2" class="koboSpan">Whereas nearest neighbor interpolation required one sample, here we use two samples to calculate the interpolation at a new location. </span><span id="kobo.990.3" class="koboSpan">This interpolated function is</span></p>
<p class="DIS-IMG"><span id="kobo.991.1" class="koboSpan"><img class="img1" src="../images/pg40-1.png"/></span></p>
<p class="noindent"><span id="kobo.992.1" class="koboSpan">Here the interpolated function </span><span id="kobo.993.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.994.1" class="koboSpan">(</span><i><span id="kobo.995.1" class="koboSpan">x</span></i><span id="kobo.996.1" class="koboSpan">) is continuous in the mathematical sense, that is, it contains no jumps. </span><span id="kobo.996.2" class="koboSpan">However, it is not differentiable in the sampling points.</span></p>
<p class="noindent-t"><span class="paragraphHead"><b><span id="kobo.997.1" class="koboSpan">Higher order interpolation.</span></b></span><span id="kobo.998.1" class="koboSpan"> The nearest neighbor and linear interpolations can be increased in complexity. </span><span id="kobo.998.2" class="koboSpan">Each of the two methods is essentially fitting a polynomial of degree 0 (nearest neighbor) and 1 (linear) to a number of samples, and the new sample value is computed as the fitted polynomial evaluated in the new sample location of interest. </span><span id="kobo.998.3" class="koboSpan">For </span><span id="pg_41"><span id="kobo.999.1" class="koboSpan">example, cubic interpolation refers to fitting polynomials of degree 3 of the form</span></span></p>
<p class="DIS-IMG"><span id="kobo.1000.1" class="koboSpan"><img class="img1" src="../images/pg41-1.png"/></span></p>
<p class="noindent"><span id="kobo.1001.1" class="koboSpan">The values of the coefficients </span><i><span id="kobo.1002.1" class="koboSpan">c</span></i><sub><span id="kobo.1003.1" class="koboSpan">1</span></sub><span id="kobo.1004.1" class="koboSpan">, </span><i><span id="kobo.1005.1" class="koboSpan">c</span></i><sub><span id="kobo.1006.1" class="koboSpan">2</span></sub><span id="kobo.1007.1" class="koboSpan">, </span><i><span id="kobo.1008.1" class="koboSpan">c</span></i><sub><span id="kobo.1009.1" class="koboSpan">3</span></sub><span id="kobo.1010.1" class="koboSpan">, </span><i><span id="kobo.1011.1" class="koboSpan">c</span></i><sub><span id="kobo.1012.1" class="koboSpan">4</span></sub><span id="kobo.1013.1" class="koboSpan"> can be computed analytically from the known samples</span></p>
<p class="DIS-IMG"><span id="kobo.1014.1" class="koboSpan"><img class="img1" src="../images/pg41-2.png"/></span></p>
</a><p class="noindent"><a id="fig2-20"><span id="kobo.1015.1" class="koboSpan">Clearly, this is a more complex interpolation, but it leads to very good results, as shown by </span></a><a id="rfig2-21" href="chapter_2.xhtml#fig2-21"><span id="kobo.1016.1" class="koboSpan">Figure 2.21</span></a><span id="kobo.1017.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-21"><span id="kobo.1018.1" class="koboSpan"><img class="img1" src="../images/Figure2-21.png"/></span>
</a><figcaption><a id="fig2-21"></a><p class="CAP"><a id="fig2-21"><span class="FIGN"></span></a><a href="#rfig2-21"><span id="kobo.1019.1" class="koboSpan">Figure 2.21</span></a> <span class="FIG"><span id="kobo.1020.1" class="koboSpan">Cubic interpolation: the original continuous function </span><i><span id="kobo.1021.1" class="koboSpan">g</span></i><span id="kobo.1022.1" class="koboSpan">(</span><i><span id="kobo.1023.1" class="koboSpan">x</span></i><span id="kobo.1024.1" class="koboSpan">) = sin(</span><i><span id="kobo.1025.1" class="koboSpan">x</span></i><span id="kobo.1026.1" class="koboSpan">) is shown by the dashed line, and the interpolation </span><span id="kobo.1027.1" class="koboSpan"><img class="inline" src="../images/g-cap.png"/></span><span id="kobo.1028.1" class="koboSpan">(</span><i><span id="kobo.1029.1" class="koboSpan">x</span></i><span id="kobo.1030.1" class="koboSpan">) is shown by the solid line.</span></span></p></figcaption>
</figure>
</div>
<p class="noindent"><span id="pg_42"><span class="paragraphHead"><b><span id="kobo.1031.1" class="koboSpan">Image 2D interpolation.</span></b></span><span id="kobo.1032.1" class="koboSpan"> Once the 1D interpolation is understood, generalizing to 2D is straightforward. </span><span id="kobo.1032.2" class="koboSpan">Image 2D interpolation is made up of two components: interpolating in the </span><i><span id="kobo.1033.1" class="koboSpan">x</span></i><span id="kobo.1034.1" class="koboSpan"> direction and in the </span><i><span id="kobo.1035.1" class="koboSpan">y</span></i><span id="kobo.1036.1" class="koboSpan"> direction. </span><span id="kobo.1036.2" class="koboSpan">The linear interpolation of an image at points </span><i><span id="kobo.1037.1" class="koboSpan">x</span></i><span id="kobo.1038.1" class="koboSpan">, </span><i><span id="kobo.1039.1" class="koboSpan">y</span></i><span id="kobo.1040.1" class="koboSpan">, denoted as </span><span id="kobo.1041.1" class="koboSpan"><img class="inline" src="../images/I-cap-roman.png"/></span><span id="kobo.1042.1" class="koboSpan">(</span><i><span id="kobo.1043.1" class="koboSpan">x, y</span></i><span id="kobo.1044.1" class="koboSpan">), is computed</span></span></p>
<p class="DIS-IMG"><span id="kobo.1045.1" class="koboSpan"><img class="img1" src="../images/pg42-1.png"/></span></p>
<p class="noindent"><span id="kobo.1046.1" class="koboSpan">where </span><i><span id="kobo.1047.1" class="koboSpan">x</span></i><span id="kobo.1048.1" class="koboSpan"> ∈ [</span><i><span id="kobo.1049.1" class="koboSpan">i</span></i><span id="kobo.1050.1" class="koboSpan">, </span><i><span id="kobo.1051.1" class="koboSpan">i</span></i><span id="kobo.1052.1" class="koboSpan"> + 1] and </span><i><span id="kobo.1053.1" class="koboSpan">y</span></i><span id="kobo.1054.1" class="koboSpan"> ∈ [</span><i><span id="kobo.1055.1" class="koboSpan">j</span></i><span id="kobo.1056.1" class="koboSpan">, </span><i><span id="kobo.1057.1" class="koboSpan">j</span></i><span id="kobo.1058.1" class="koboSpan"> + 1] are the coordinates of the interpolation point. </span><span id="kobo.1058.2" class="koboSpan">An example of 2D linear interpolation is shown in </span><a id="rfig2-22" href="chapter_2.xhtml#fig2-22"><span id="kobo.1059.1" class="koboSpan">Figure 2.22</span></a><span id="kobo.1060.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-22"><span id="kobo.1061.1" class="koboSpan"><img class="img1" src="../images/Figure2-22.png"/></span>
</a><figcaption><a id="fig2-22"></a><p class="CAP"><a id="fig2-22"><span class="FIGN"></span></a><a href="#rfig2-22"><span id="kobo.1062.1" class="koboSpan">Figure 2.22</span></a> <span class="FIG"><span id="kobo.1063.1" class="koboSpan">Linear 2D interpolation: the original sample </span><b><span class="ePub-B"><span id="kobo.1064.1" class="koboSpan">I</span></span></b><sub><i><span id="kobo.1065.1" class="koboSpan">S</span></i></sub><span id="kobo.1066.1" class="koboSpan">(</span><i><span id="kobo.1067.1" class="koboSpan">i, j</span></i><span id="kobo.1068.1" class="koboSpan">) is shown by black dots, and the interpolation </span><span id="kobo.1069.1" class="koboSpan"><img class="inline" src="../images/I-cap-roman.png"/></span><span id="kobo.1070.1" class="koboSpan">(</span><i><span id="kobo.1071.1" class="koboSpan">x, y</span></i><span id="kobo.1072.1" class="koboSpan">) is shown by gray mesh. </span><span id="kobo.1072.2" class="koboSpan">The samples were taken from the function </span><span id="kobo.1073.1" class="koboSpan"><img class="inline" src="../images/pg42-in-1.png"/></span><span id="kobo.1074.1" class="koboSpan">.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.1075.1" class="koboSpan">Let us see what happens to a real image during the process of sampling. </span><span id="kobo.1075.2" class="koboSpan">To enhance the effect on high frequencies, we start with a checkerboard pattern with a tilted view, as shown in </span><a id="rfig2-23" href="chapter_2.xhtml#fig2-23"><span id="kobo.1076.1" class="koboSpan">Figure 2.23</span></a><span id="kobo.1077.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-23"><span id="kobo.1078.1" class="koboSpan"><img class="img1" src="../images/Figure2-23.png"/></span>
</a><figcaption><a id="fig2-23"></a><p class="CAP"><a id="fig2-23"><span class="FIGN"></span></a><a href="#rfig2-23"><span id="kobo.1079.1" class="koboSpan">Figure 2.23</span></a> <span class="FIG"><span id="kobo.1080.1" class="koboSpan">Image downsampling without filtering: the original image (</span><i><span id="kobo.1081.1" class="koboSpan">left</span></i><span id="kobo.1082.1" class="koboSpan">) is downsampled by a factor of two (</span><i><span id="kobo.1083.1" class="koboSpan">right</span></i><span id="kobo.1084.1" class="koboSpan">). </span><span id="kobo.1084.2" class="koboSpan">The aliasing effect is particularly visible for the high spatial frequencies at the top of the image.</span></span></p></figcaption>
</figure>
</div>
<p><span id="pg_43"><span id="kobo.1085.1" class="koboSpan">Most of the image’s high spatial frequencies are located at the top, where the squares are smaller and closer together. </span><span id="kobo.1085.2" class="koboSpan">The figure shows that simply downsampling the image, by discarding one out of two pixels on both spatial dimensions, leads to distortions in the top part with high frequencies. </span><span id="kobo.1085.3" class="koboSpan">These distortions, called aliasing, are explained theoretically by </span><span id="pg_44"><span id="kobo.1086.1" class="koboSpan">Shannon’s Nyquist rate formula, which states that the sampling frequency </span><i><span id="kobo.1087.1" class="koboSpan">f</span></i><sub><i><span id="kobo.1088.1" class="koboSpan">s</span></i></sub><span id="kobo.1089.1" class="koboSpan"> should satisfy</span></span></span></p>
<p class="DIS-IMG"><span id="kobo.1090.1" class="koboSpan"><img class="img1" src="../images/pg44-1.png"/></span></p>
<p class="noindent"><span id="kobo.1091.1" class="koboSpan">where </span><i><span id="kobo.1092.1" class="koboSpan">f</span></i><sub><span id="kobo.1093.1" class="koboSpan">MAX</span></sub><span id="kobo.1094.1" class="koboSpan"> is the maximum frequency in the signal, and 2</span><i><span id="kobo.1095.1" class="koboSpan">f</span></i><sub><span id="kobo.1096.1" class="koboSpan">MAX</span></sub><span id="kobo.1097.1" class="koboSpan"> is known as the Nyquist rate (see section 3.2.1). </span><span id="kobo.1097.2" class="koboSpan">Therefore, to obtain good results after sampling an image, we can filter the image. </span><a id="rfig2-24" href="chapter_2.xhtml#fig2-24"><span id="kobo.1098.1" class="koboSpan">Figure 2.24</span></a><span id="kobo.1099.1" class="koboSpan"> shows the Fourier transform of the original image ℱ [</span><b><span id="kobo.1100.1" class="koboSpan">I</span></b><span id="kobo.1101.1" class="koboSpan">(</span><i><span><span id="kobo.1102.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1103.1" class="koboSpan">1</span></sub><span id="kobo.1104.1" class="koboSpan">, </span><i><span><span id="kobo.1105.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1106.1" class="koboSpan">2</span></sub><span id="kobo.1107.1" class="koboSpan">)] and of the image filtered with a boxcar function ℱ [</span><b><span id="kobo.1108.1" class="koboSpan">I</span></b><sub><i><span id="kobo.1109.1" class="koboSpan">f</span></i></sub><span id="kobo.1110.1" class="koboSpan">(</span><i><span><span id="kobo.1111.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1112.1" class="koboSpan">1</span></sub><span id="kobo.1113.1" class="koboSpan">, </span><i><span><span id="kobo.1114.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1115.1" class="koboSpan">2</span></sub><span id="kobo.1116.1" class="koboSpan">)]. </span><span id="kobo.1116.2" class="koboSpan">The original image has frequency components near the edge of the frequency domain. </span><span id="kobo.1116.3" class="koboSpan">A downsampled image has a reduced frequency domain, and therefore we need to remove the high frequencies up to half the maximum frequency in the signal via filtering to satisfy Shannon’s Nyquist rate condition. </span><span id="kobo.1116.4" class="koboSpan">The right-hand side of </span><a href="chapter_2.xhtml#fig2-24"><span id="kobo.1117.1" class="koboSpan">Figure 2.24</span></a><span id="kobo.1118.1" class="koboSpan"> shows that the filtered spectrum is still surrounded by four small lobes. </span><span id="kobo.1118.2" class="koboSpan">The reason is that the boxcar function is not an ideal low-pass filter, and its Fourier transform is a cardinal sine function.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-24"><span id="kobo.1119.1" class="koboSpan"><img class="img1" src="../images/Figure2-24.png"/></span>
</a><figcaption><a id="fig2-24"></a><p class="CAP"><a id="fig2-24"><span class="FIGN"></span></a><a href="#rfig2-24"><span id="kobo.1120.1" class="koboSpan">Figure 2.24</span></a> <span class="FIG"><span id="kobo.1121.1" class="koboSpan">Image filtering in the frequency domain: the Fourier transform of the original image (</span><i><span id="kobo.1122.1" class="koboSpan">left</span></i><span id="kobo.1123.1" class="koboSpan">), and of the image filtered with a boxcar function (</span><i><span id="kobo.1124.1" class="koboSpan">right</span></i><span id="kobo.1125.1" class="koboSpan">).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.1126.1" class="koboSpan">If we perform downsampling on this new filtered image, we notice that the aliasing effect is barely visible, as depicted in </span><a id="rfig2-25" href="chapter_2.xhtml#fig2-25"><span id="kobo.1127.1" class="koboSpan">Figure 2.25</span></a><span id="kobo.1128.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-25"><span id="kobo.1129.1" class="koboSpan"><img class="img1" src="../images/Figure2-25.png"/></span>
</a><figcaption><a id="fig2-25"></a><p class="CAP"><a id="fig2-25"><span class="FIGN"></span></a><a href="#rfig2-25"><span id="kobo.1130.1" class="koboSpan">Figure 2.25</span></a> <span class="FIG"><span id="kobo.1131.1" class="koboSpan">Image downsampling with filtering: the original image is filtered with a boxcar function (</span><i><span id="kobo.1132.1" class="koboSpan">left</span></i><span id="kobo.1133.1" class="koboSpan">) and subsequently downsampled by a factor of two (</span><i><span id="kobo.1134.1" class="koboSpan">right</span></i><span id="kobo.1135.1" class="koboSpan">).</span></span></p></figcaption>
</figure>
</div>
</section>
<section>
<h4 id="sec13" class="head b-head"><a id="sec2-2-4"><span id="kobo.1136.1" class="koboSpan">2.2.4 Digital Imaging Pipeline</span></a></h4><a id="sec2-2-4">
</a><p class="noindent"><a id="sec2-2-4"><span id="kobo.1137.1" class="koboSpan">The transformation stages from the light rays reflected by the scene to the final image files on our computers is called the digital imaging pipeline. </span><span id="kobo.1137.2" class="koboSpan">It consists of a few major stages, depicted in </span></a><a id="rfig2-26" href="chapter_2.xhtml#fig2-26"><span id="kobo.1138.1" class="koboSpan">Figure 2.26</span></a><span id="kobo.1139.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-26"><span id="kobo.1140.1" class="koboSpan"><img class="img1" src="../images/Figure2-26.png"/></span>
</a><figcaption><a id="fig2-26"></a><p class="CAP"><a id="fig2-26"><span class="FIGN"></span></a><a href="#rfig2-26"><span id="kobo.1141.1" class="koboSpan">Figure 2.26</span></a> <span class="FIG"><span id="kobo.1142.1" class="koboSpan">The main steps in the digital imaging pipeline. </span><span id="kobo.1142.2" class="koboSpan">Reprinted from (Ramanath et al., 2005).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.1143.1" class="koboSpan">First, the light reflected by the scene is manipulated using the optical parameters such as aperture and exposure time, which bend the light and direct it toward the sensor. </span><span id="kobo.1143.2" class="koboSpan">Sensors have evolved considerably over centuries, and today there are two main categories: charged coupled device (CCD) and complementary metal oxide semiconductor (CMOS). </span><span id="kobo.1143.3" class="koboSpan">The CCD is based on a MOS capacitor and is used mainly in high-end cameras due to its high price </span><span id="pg_45"><span id="kobo.1144.1" class="koboSpan">and power consumption. </span><span id="kobo.1144.2" class="koboSpan">The CMOS is based on MOSFET transistors and is a lot more consumer friendly, with lower power consumption and a more affordable price. </span><span id="kobo.1144.3" class="koboSpan">It is more prone to noise, but this can be reduced with digital denoising. </span><span id="kobo.1144.4" class="koboSpan">Therefore we focus on CMOS sensors in this presentation.</span></span></p>
<p><span id="kobo.1145.1" class="koboSpan">The CMOS sensor is equipped with a microlens for each pixel, which has the effect of increasing the amount of light captured by that pixel. </span><span id="kobo.1145.2" class="koboSpan">The light then passes through a color filter, which extracts the wavelengths relevant for each of the red, green, and blue colors. </span><span id="kobo.1145.3" class="koboSpan">Then the filtered light hits the photodiode, which in response generates electrons that are then stored in the potential well. </span><span id="kobo.1145.4" class="koboSpan">A diagram of a sensor is shown in </span><a id="rfig2-27" href="chapter_2.xhtml#fig2-27"><span id="kobo.1146.1" class="koboSpan">Figure 2.27</span></a><span id="kobo.1147.1" class="koboSpan">, which for simplicity shows only three pixels. </span><span id="kobo.1147.2" class="koboSpan">The sensitivity of each color filter to each wavelength is depicted in </span><a id="rfig2-28" href="chapter_2.xhtml#fig2-28"><span id="kobo.1148.1" class="koboSpan">Figure 2.28</span></a><span id="kobo.1149.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-27"><span id="kobo.1150.1" class="koboSpan"><img class="img1" src="../images/Figure2-27.png"/></span>
</a><figcaption><a id="fig2-27"></a><p class="CAP"><a id="fig2-27"><span class="FIGN"></span></a><a href="#rfig2-27"><span id="kobo.1151.1" class="koboSpan">Figure 2.27</span></a> <span class="FIG"><span id="kobo.1152.1" class="koboSpan">The basic components of a CMOS camera sensor.</span></span></p></figcaption>
</figure>
</div>
<div class="figure">
<figure class="IMG"><a id="fig2-28"><span id="kobo.1153.1" class="koboSpan"><img class="img1" src="../images/Figure2-28.png"/></span>
</a><figcaption><a id="fig2-28"></a><p class="CAP"><a id="fig2-28"><span class="FIGN"></span></a><a href="#rfig2-28"><span id="kobo.1154.1" class="koboSpan">Figure 2.28</span></a> <span class="FIG"><span id="kobo.1155.1" class="koboSpan">Spectral sensitivities in digital color cameras. </span><span id="kobo.1155.2" class="koboSpan">Reprinted from (Ramanath et al., 2005).</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.1156.1" class="koboSpan">In a full commercial sensor, the color filters are not uniformly distributed. </span><span id="kobo.1156.2" class="koboSpan">They follow a specific pattern, called the color filter array, which determines the final look of the image. </span><span id="kobo.1156.3" class="koboSpan">One of the common color filter arrays is the Bayer filter, which contains 50% green filters, 25% blue, and 25% red. </span><span id="kobo.1156.4" class="koboSpan">This proportion is inspired by the human retina, which during the daytime uses cone cells that are most sensitive to green light. </span><span id="kobo.1156.5" class="koboSpan">The image generated by the sensor is called a </span><i><span id="kobo.1157.1" class="koboSpan">mosaiced</span></i><span id="kobo.1158.1" class="koboSpan"> image due to its mix of pixels of different colors.</span></p>
<p><span id="pg_46"><span id="kobo.1159.1" class="koboSpan">The potential well of each pixel generates an analog voltage signal that enters the preprocessing pipeline, which consists of several stages. </span><span id="kobo.1159.2" class="koboSpan">First, the voltage is processed with the analog front end, depicted in </span><a id="rfig2-29" href="chapter_2.xhtml#fig2-29"><span id="kobo.1160.1" class="koboSpan">Figure 2.29</span></a><span id="kobo.1161.1" class="koboSpan">. </span><span id="kobo.1161.2" class="koboSpan">This converts the analog mosaiced image from the sensor outputs into the raw digital mosaiced image. </span><span id="kobo.1161.3" class="koboSpan">First, the analog voltage is passed through an amplifier, whose gain is modulated by the ISO settings of the camera. </span><span id="kobo.1161.4" class="koboSpan">The gain is larger for pixels farther from the image center, due to the vignetting effect, which darkens the extremities of an image. </span><span id="kobo.1161.5" class="koboSpan">Second, the analog-to-digital (ADC) converter generates a digital signal, usually with sizes of 10–16 bits. </span><span id="kobo.1161.6" class="koboSpan">Third, the sensor suffers from nonlinearities in the extremities of its range (very bright or very dark pixels), which is corrected using a lookup table. </span><span id="kobo.1161.7" class="koboSpan">A lookup table, which simply maps an output value to any possible input value, is a very fast technique to process digital signals. </span><span id="kobo.1161.8" class="koboSpan">(Gruev and Etienne-Cummings, 2002) describe implementing a pseudo-general image processor chip that enables steerable spatial and temporal filters at the focal plane.</span></span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-29"><span id="kobo.1162.1" class="koboSpan"><img class="img1" src="../images/Figure2-29.png"/></span>
</a><figcaption><a id="fig2-29"></a><p class="CAP"><a id="fig2-29"><span class="FIGN"></span></a><a href="#rfig2-29"><span id="kobo.1163.1" class="koboSpan">Figure 2.29</span></a> <span class="FIG"><span id="kobo.1164.1" class="koboSpan">The analog front end in the digital processing pipeline.</span></span></p></figcaption>
</figure>
</div>
<p><span id="kobo.1165.1" class="koboSpan">The output image of the analog front end is called the raw image. </span><span id="kobo.1165.2" class="koboSpan">Many consumer-grade cameras allow access to this format, because many applications, such as physics based computer vision, work much better on raw images than processed ones. </span><span id="kobo.1165.3" class="koboSpan">However, they do not look very appealing due to a high level of noise and unsuitable color balancing.</span></p>
<p><span id="kobo.1166.1" class="koboSpan">The next processing stage adjusts the white balance of the raw digital images. </span><span id="kobo.1166.2" class="koboSpan">This stage is necessary because what someone sees as white is significant in the viewer’s perception of the scene. </span><span id="kobo.1166.3" class="koboSpan">Therefore, the white balance is adjusted by imposing an assumption on the image coloring. </span><span id="kobo.1166.4" class="koboSpan">One way to do that is to assume that the average color of an image is gray; this is called gray world assumption. </span><span id="kobo.1166.5" class="koboSpan">A different method, called white world assumption, </span><span id="pg_47"><span id="kobo.1167.1" class="koboSpan">assumes the brightest object in the scene to be white. </span><span id="kobo.1167.2" class="koboSpan">However, modern cameras use histogram-based algorithms, assuming specific proportions of various colors.</span></span></p>
<p><span id="kobo.1168.1" class="koboSpan">Next, recall that at this stage the image is still a mosaic of colors, so that each pixel stores color-specific information. </span><span id="kobo.1168.2" class="koboSpan">We need to turn this mosaicked image into three images, corresponding to the colors red, green, and blue. </span><span id="kobo.1168.3" class="koboSpan">However, the red and green information is lost at the location of a blue pixel. </span><span id="kobo.1168.4" class="koboSpan">So how can this information be recovered? </span><span id="kobo.1168.5" class="koboSpan">This can be accomplished by interpolation, and even simple algorithms averaging the nearest neighbors can achieve good results. </span><span id="kobo.1168.6" class="koboSpan">The three images at this stage are still strongly affected by noise. </span><span id="pg_48"><span id="kobo.1169.1" class="koboSpan">Therefore, a denoising stage is often applied, such as averaging or computing the median of the neighboring pixels.</span></span></p>
<p><span id="kobo.1170.1" class="koboSpan">The human-perceived colors are mapped to light wavelengths using color spaces. </span><span id="kobo.1170.2" class="koboSpan">Color spaces allow a reproducible representation of color. </span><span id="kobo.1170.3" class="koboSpan">These mappings are denoted in the diagram as color transformations. </span><span id="kobo.1170.4" class="koboSpan">Examples of color spaces are CIEXYZ and ISO-RGB (Ramanath et al., 2005).</span></p>
<p><span id="kobo.1171.1" class="koboSpan">After all these steps, the image still does not look natural. </span><span id="kobo.1171.2" class="koboSpan">That is because light detection in the human retina is nonlinear as a function of the luminance, and is more sensitive to dark tones, whereas for a camera, this relationship is linear. </span><span id="kobo.1171.3" class="koboSpan">To address this, a subsequent postprocessing step is employed. </span><span id="kobo.1171.4" class="koboSpan">Because the nonlinear function in the case of the human eye resembles the mathematical function gamma, the process that compensates for this effect is called </span><i><span id="kobo.1172.1" class="koboSpan">gamma correction</span></i><span id="kobo.1173.1" class="koboSpan">. </span><span id="kobo.1173.2" class="koboSpan">After this step, the image appearance to the human eye is significantly improved, but the image requires a lot of space. </span><span id="kobo.1173.3" class="koboSpan">This motivates the final step, called compression, which decreases the image size by a third. </span><span id="kobo.1173.4" class="koboSpan">The final result is an appealing image in a compressed format, such as jpeg or png.</span></p>
</section>
</section>
<section>
<h3 id="sec14" class="head a-head"><a id="sec2-3"><span id="kobo.1174.1" class="koboSpan">2.3 Illumination</span></a></h3><a id="sec2-3">
<p class="noindent"><span id="kobo.1175.1" class="koboSpan">The use of lighting has not evolved a great deal since the beginnings of photography. </span><span id="kobo.1175.2" class="koboSpan">It can be argued that lighting is the main thing that distinguishes an amateur photographer from a professional. </span><span id="kobo.1175.3" class="koboSpan">A professional photographer measures the light intensity and then manually selects the optimal camera parameters such as ISO sensitivity, exposure time, and aperture. </span><span id="kobo.1175.4" class="koboSpan">In automated cameras these parameters are selected automatically, but these choices do not always lead to the most pleasing picture.</span></p>
<p><span id="kobo.1176.1" class="koboSpan">Similarly to ISO or exposure time, several other parameters can be adjusted for the camera lighting:</span></p>
<ul class="bullet">
<li class="BL"><span id="kobo.1177.1" class="koboSpan">duration and intensity;</span></li>
<li class="BL"><span id="kobo.1178.1" class="koboSpan">presence or absence of auxiliary lighting;</span></li>
<li class="BL"><span id="kobo.1179.1" class="koboSpan">color, wavelength, and polarization;</span></li>
<li class="BL"><span id="kobo.1180.1" class="koboSpan">position and orientation; and</span></li>
<li class="BL"><span id="kobo.1181.1" class="koboSpan">modulation in space and time.</span></li>
</ul>
<p class="noindent"><span id="kobo.1182.1" class="koboSpan">Each of the parameters above will be discussed separately below.</span></p>
</a><section><a id="sec2-3">
</a><h4 id="sec15" class="head b-head"><a id="sec2-3"></a><a id="sec2-3-1"><span id="kobo.1183.1" class="koboSpan">2.3.1 Duration and Intensity</span></a></h4><a id="sec2-3-1">
<p class="noindent"><span id="kobo.1184.1" class="koboSpan">Capturing fast-moving objects is possible using high shutter speeds. </span><span id="kobo.1184.2" class="koboSpan">However, this approach is quite limited because it involves moving mechanical parts. </span><span id="kobo.1184.3" class="koboSpan">Therefore, we cannot capture certain physical phenomena in this way.</span></p>
<p><span id="pg_49"><span id="kobo.1185.1" class="koboSpan">An alternative is to use fast bursting flashes of light with electronic devices called strobes. </span><span id="kobo.1185.2" class="koboSpan">The image of a </span><i><span id="kobo.1186.1" class="koboSpan">bullet fired through an apple</span></i><span id="kobo.1187.1" class="koboSpan">, captured by MIT professor Harold Edgerton is an iconic example of a technique called </span><i><span id="kobo.1188.1" class="koboSpan">strobe photography</span></i><span id="kobo.1189.1" class="koboSpan">, developed in the 1930s, which uses light and sound to trigger the flash burst with precise timing. </span><span id="kobo.1189.2" class="koboSpan">A natural continuation of strobe photography was high frame rate film. </span><span id="kobo.1189.3" class="koboSpan">By combining the short light bursts of strobes with the high sensitivity of CCD and CMOS sensors developed in the 1980s, manufacturers developed cameras with ultrashort exposures. </span><span id="kobo.1189.4" class="koboSpan">This technology has evolved to the point where today’s affordable cameras reach up to 300 frames per second at 0.2 MB resolution, and high-end cameras reach 2, 570 frames per second at full HD (2 MB) resolution.</span></span></p>
<p><span id="kobo.1190.1" class="koboSpan">Another way to exploit the capabilities of strobe photography is to generate several bursts in one camera exposure; this is called </span><i><span id="kobo.1191.1" class="koboSpan">sequential multiflash stroboscopy</span></i><span id="kobo.1192.1" class="koboSpan">. </span><span id="kobo.1192.2" class="koboSpan">Typically this is done with a dark background, and the bursting frequency and duration is set so that the frames of the object in motion do not overlap.</span></p>
</a></section><a id="sec2-3-1">
</a><section><a id="sec2-3-1">
</a><h4 id="sec16" class="head b-head"><a id="sec2-3-1"></a><a id="sec2-3-2"><span id="kobo.1193.1" class="koboSpan">2.3.2 Auxiliary Lighting</span></a></h4><a id="sec2-3-2">
<p class="noindent"><span id="kobo.1194.1" class="koboSpan">The flash illumination is built into most of today’s cameras. </span><span id="kobo.1194.2" class="koboSpan">By adjusting the illumination during image capture, it is possible to extract various features. </span><span id="kobo.1194.3" class="koboSpan">(Dicarlo et al., 2001) have described recovering the object reflectivity using two snapshots with ambient lighting and flash, respectively. </span><span id="kobo.1194.4" class="koboSpan">Taking photos with flash with various intensities makes it possible to simulate images captured with a continuous level of flash (Hoppe and Toyama, 2003).</span></p>
<p><span id="kobo.1195.1" class="koboSpan">The use of flash is clearly necessary when the ambient lighting is low. </span><span id="kobo.1195.2" class="koboSpan">However, if there is adequate ambient light, is it recommended to use flash? </span><span id="kobo.1195.3" class="koboSpan">Flash photographs are known to lead to images with clear high-frequency details and also with more noise robustness. </span><span id="kobo.1195.4" class="koboSpan">However, ambient light is a part of the scene that we may want to capture. </span><span id="kobo.1195.5" class="koboSpan">Moreover, flash photographs look rather unnatural due to the artificial lighting. </span><span id="kobo.1195.6" class="koboSpan">It is possible to combine a flash photograph with one captured with ambient light and thus enjoy the advantages of both methodologies (Petschnigg et al., 2004; Raskar et al., 2004). </span><span id="kobo.1195.7" class="koboSpan">The two methods generate a new image incorporating the details from the flash photo and the shadows from the ambient light photo. </span><span id="kobo.1195.8" class="koboSpan">The separation is performed using an image processing technique called </span><i><span id="kobo.1196.1" class="koboSpan">joint bilateral filter</span></i><span id="kobo.1197.1" class="koboSpan">.</span></p>
<p><span id="kobo.1198.1" class="koboSpan">Similarly, a bilateral filter can be used to simply denoise the image captured without flash (Tomasi and Manduchi, 1998). </span><span id="kobo.1198.2" class="koboSpan">Normally, when an image is filtered, the details are removed along with the noise. </span><span id="kobo.1198.3" class="koboSpan">In using a bilateral filter, an intensity similarity measure cancels the filter effect in areas where there are image details, quantified as high frequencies in the flash image. </span><span id="kobo.1198.4" class="koboSpan">This technique is prone to errors and artifacts when the flash image contains shadows that are interpreted as details by the bilateral filter, or when it is overexposed, so that the details are dimmed or removed altogether. </span><span id="kobo.1198.5" class="koboSpan">This would cause the bilateral filter to remove details from the ambient image, or to leave unfiltered areas with no detail.</span></p>
<p><span id="pg_50"><span id="kobo.1199.1" class="koboSpan">As mentioned previously, the bilateral filter method fails when the flash saturates portions of the image. </span><span id="kobo.1199.2" class="koboSpan">Similarly, because sensors have predefined dynamic ranges, the flash might lead to colors too bright to be captured. </span><span id="kobo.1199.3" class="koboSpan">In another scenario, objects could be located at different distances from the flash, and therefore a low intensity would not illuminate the distant objects, and a high intensity would saturate or </span><i><span id="kobo.1200.1" class="koboSpan">blow out</span></i><span id="kobo.1201.1" class="koboSpan"> the nearby points in the scene. </span><span id="kobo.1201.2" class="koboSpan">As in the case of the ambient-light image, the solution is to combine the beneficial characteristics of several images into one high-quality image. </span><span id="kobo.1201.3" class="koboSpan">In a case of this kind, (Raskar et al., 2008) combined images captured with various flash intensities to generate a single high dynamic range (HDR) image.</span></span></p>
<p><span id="kobo.1202.1" class="koboSpan">Another way to address the artifacts in flash images is to compute the </span><i><span id="kobo.1203.1" class="koboSpan">gradient vector</span></i><span id="kobo.1204.1" class="koboSpan"> for the flash and ambient light image (Raskar et al., 2008). </span><span id="kobo.1204.2" class="koboSpan">The gradient vector in a pixel is the direction in which the intensity change is most abrupt. </span><span id="kobo.1204.3" class="koboSpan">Therefore it is intuitive that the gradient at an edge is perpendicular on the edge for all pixels close to it. </span><span id="kobo.1204.4" class="koboSpan">On the basis of this observation, an artifact is located at pixels where there is significant difference in gradient vector orientation between the ambient light and the flash image. </span><span id="kobo.1204.5" class="koboSpan">This technique is called </span><i><span id="kobo.1205.1" class="koboSpan">gradient coherence</span></i><span id="kobo.1206.1" class="koboSpan"> (Raskar et al., 2008).</span></p>
<p><span id="kobo.1207.1" class="koboSpan">An interesting research question is whether an image can be reconstructed from its generated gradient vectors. </span><span id="kobo.1207.2" class="koboSpan">The gradient is typically implemented as a difference</span></p>
<p class="DIS-IMG"><span id="kobo.1208.1" class="koboSpan"><img class="img1" src="../images/pg50-1.png"/></span></p>
</a><p><a id="sec2-3-2"><span id="kobo.1209.1" class="koboSpan">Therefore the problem of reconstructing </span><b><span class="ePub-B"><span id="kobo.1210.1" class="koboSpan">I</span></span></b><span id="kobo.1211.1" class="koboSpan"> seems trivial, that is, recovering through the cumulative summation of </span><span><span id="kobo.1212.1" class="koboSpan">Δ</span></span><b><span class="ePub-B"><span id="kobo.1213.1" class="koboSpan">I</span></span></b><span id="kobo.1214.1" class="koboSpan">(</span><i><span id="kobo.1215.1" class="koboSpan">x</span></i><span id="kobo.1216.1" class="koboSpan">, </span><i><span id="kobo.1217.1" class="koboSpan">y</span></i><span id="kobo.1218.1" class="koboSpan">). </span><span id="kobo.1218.2" class="koboSpan">However, complications arise when the gradient is not consistent and therefore the result is dependent on the path along which summation is done. </span><span id="kobo.1218.3" class="koboSpan">There have been several methods addressing this issue (Agrawal et al., 2006). </span><span id="kobo.1218.4" class="koboSpan">(Agrawal et al., 2005), used gradient vector projection to combine ambient and flash images into a high quality image with ambient-light features (see </span></a><a id="rfig2-30" href="chapter_2.xhtml#fig2-30"><span id="kobo.1219.1" class="koboSpan">Figure 2.30</span></a><span id="kobo.1220.1" class="koboSpan">). </span><span id="kobo.1220.2" class="koboSpan">Interestingly, the residuals from the flash image gradients can be integrated to recover an image of the photographer that is not visible in the original flash image.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-30"><span id="kobo.1221.1" class="koboSpan"><img class="img1" src="../images/Figure2-30.png"/></span>
</a><figcaption><a id="fig2-30"></a><p class="CAP"><a id="fig2-30"><span class="FIGN"></span></a><a href="#rfig2-30"><span id="kobo.1222.1" class="koboSpan">Figure 2.30</span></a> <span class="FIG"><span id="kobo.1223.1" class="koboSpan">Removing artifacts from a flash image: the image gradients are used to locate the image artifact and remove it. </span><span id="kobo.1223.2" class="koboSpan">Subsequently, the isolated artifact can be integrated to generate an image of the photographer. </span><span id="kobo.1223.3" class="koboSpan">Reprinted from (Agrawal et al., 2005).</span></span></p></figcaption>
</figure>
</div>
</section>
<section>
<h4 id="sec17" class="head b-head"><a id="sec2-3-3"><span id="kobo.1224.1" class="koboSpan">2.3.3 Modifying Color, Wavelength, and Polarization</span></a></h4><a id="sec2-3-3">
<p class="noindent"><span id="kobo.1225.1" class="koboSpan">So far we have looked at white illumination with varying intensity to achieve desired image characteristics. </span><span id="kobo.1225.2" class="koboSpan">Choosing a flash containing specific colors allows performing programmable color manipulations on images. </span><span id="kobo.1225.3" class="koboSpan">For example, two colors can look the same (see section 8.2.3) or different, depending on the type of lighting during capture, which could be countered by modulating the illumination wavelength (see section 8.3.4).</span></p>
<p><span id="kobo.1226.1" class="koboSpan">This approach is also used in fluorescence photography, which exploits the fact that fluorescent surfaces emit low frequency light in response to high frequency illumination. </span><span id="kobo.1226.2" class="koboSpan">In this case, the light source emits ultraviolet light and the camera filters out nonvisible light, thus capturing only the reflection of fluorescent surfaces.</span></p>
</a><p><a id="sec2-3-3"><span id="pg_51"><span id="kobo.1227.1" class="koboSpan">Colored lighting can be simulated using photographs captured with conventional lighting. </span><span id="kobo.1227.2" class="koboSpan">(Haeberli, 1992) used two lamps with white light positioned on either side of the subject, and captured three photographs: one with ambient lighting and two with lighting from either direction. </span><span id="kobo.1227.3" class="koboSpan">By subtracting the ambient light image from the other two, it was possible </span><span id="pg_52"><span id="kobo.1228.1" class="koboSpan">to quantify the contribution of each lamp. </span><span id="kobo.1228.2" class="koboSpan">Then, through software manipulation, the author simulated an image in which each light source has a different wavelength, as shown in </span></span></span></a><a id="rfig2-31" href="chapter_2.xhtml#fig2-31"><span id="kobo.1229.1" class="koboSpan">Figure 2.31</span></a><span id="kobo.1230.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-31"><span id="kobo.1231.1" class="koboSpan"><img class="img1" src="../images/Figure2-31.png"/></span>
</a><figcaption><a id="fig2-31"></a><p class="CAP"><a id="fig2-31"><span class="FIGN"></span></a><a href="#rfig2-31"><span id="kobo.1232.1" class="koboSpan">Figure 2.31</span></a> <span class="FIG"><span id="kobo.1233.1" class="koboSpan">Generating synthetic colored lighting using conventional illumination: an image is captured using ambient light and subsequently with lighting from the left direction (</span><i><span id="kobo.1234.1" class="koboSpan">left</span></i><span id="kobo.1235.1" class="koboSpan">) and right (</span><i><span id="kobo.1236.1" class="koboSpan">center</span></i><span id="kobo.1237.1" class="koboSpan">). </span><span id="kobo.1237.2" class="koboSpan">By subtracting the images with artificial lighting from the ambient-light image it is possible to generate synthetic colored lighting (</span><i><span id="kobo.1238.1" class="koboSpan">right</span></i><span id="kobo.1239.1" class="koboSpan">). </span><span id="kobo.1239.2" class="koboSpan">Reprinted from (Haeberli, 1992).</span></span></p></figcaption>
</figure>
</div>
</section>
<section>
<h4 id="sec18" class="head b-head"><a id="sec2-3-4"><span id="kobo.1240.1" class="koboSpan">2.3.4 Modifying Position and Orientation</span></a></h4><a id="sec2-3-4">
<p class="noindent"><span id="kobo.1241.1" class="koboSpan">If we can alter the illumination of a scene, we can reveal different surface details otherwise hidden from view (see section 4.3). </span><span id="kobo.1241.2" class="koboSpan">One example is locating shape discontinuities, which are depth differences between various patches of the scene. </span><span id="kobo.1241.3" class="koboSpan">This connects closely with edge detection, because edges in an image are largely the cause of shape discontinuities.</span></p>
<p><span id="kobo.1242.1" class="koboSpan">In (Raskar et al., 2004) it was shown how to use multiple flashes to find silhouettes using depth discontinuities. </span><span id="kobo.1242.2" class="koboSpan">Depth discontinuities, or edges, are identified via the shadow narrow strip, which is a sliver cast in the opposite direction of the lighting. </span><span id="kobo.1242.3" class="koboSpan">This technique can also be used to generate shadow-free images.</span></p>
<p><span id="kobo.1243.1" class="koboSpan">The weak point of this method is that it does not accommodate small objects, or distant backgrounds. </span><span id="kobo.1243.2" class="koboSpan">These lead to shadows that are detached from the subject. </span><span id="kobo.1243.3" class="koboSpan">The method can be extended, however, to video footage by using a high-speed flash sequence (Raskar et al., 2004; Taguchi, 2014). </span><span id="kobo.1243.4" class="koboSpan">This principle was also used to decode sign language input (Feris et al., 2004).</span></p>
<p><span id="kobo.1244.1" class="koboSpan">Generating synthetic lighting in images postcapture has also been proven useful for generating a painting interface for novices in photographic lighting design (Anrys and Dutré, 2004; Mohan et al., 2005). </span><span id="kobo.1244.2" class="koboSpan">This allowed them to see the results after locally changing the lighting in images, which is much more convenient than retaking the photograph with different lighting each time.</span></p>
<p><span id="kobo.1245.1" class="koboSpan">If the image has only one lighting source, then the pixel brightness is linear with the intensity of that lighting. </span><span id="kobo.1245.2" class="koboSpan">Assuming that the camera has a linear response, then the effect of more powerful lighting can be achieved simply by increasing the resulting pixel brightness (Nimeroff et al., 1995; Haeberli, 1992). </span><span id="kobo.1245.3" class="koboSpan">If there are several light sources present, the final intensity is computed as a weighted sum of the corresponding intensities of each light source.</span></p>
<p><span id="kobo.1246.1" class="koboSpan">For maximum flexibility, one should ideally have access to photographs taken from any possible position. </span><span id="kobo.1246.2" class="koboSpan">However, this is not possible when the lighting equipment is constrained inside a predefined area, such as inside a square. </span><span id="kobo.1246.3" class="koboSpan">In a general framework, a scene is described by two four-dimensional (4D) functions known as light fields:</span></p>
<ul class="bullet">
<li class="BL"><span id="kobo.1247.1" class="koboSpan">the incident light field </span><i><span id="kobo.1248.1" class="koboSpan">L</span></i><sub><i><span id="kobo.1249.1" class="koboSpan">i</span></i></sub><span id="kobo.1250.1" class="koboSpan">(</span><i><span id="kobo.1251.1" class="koboSpan">u, v, </span><span><span id="kobo.1252.1" class="koboSpan">α</span></span><span id="kobo.1253.1" class="koboSpan">, </span><span><span id="kobo.1254.1" class="koboSpan">β</span></span></i><span id="kobo.1255.1" class="koboSpan">), describing the irradiance of light incident on objects in space; and</span></li>
<li class="BL"><span id="kobo.1256.1" class="koboSpan">the radiant light field </span><i><span id="kobo.1257.1" class="koboSpan">L</span></i><sub><i><span id="kobo.1258.1" class="koboSpan">r</span></i></sub><span id="kobo.1259.1" class="koboSpan">(</span><i><span id="kobo.1260.1" class="koboSpan">u, v, </span><span><span id="kobo.1261.1" class="koboSpan">α</span></span><span id="kobo.1262.1" class="koboSpan">, </span><span><span id="kobo.1263.1" class="koboSpan">β</span></span></i><span id="kobo.1264.1" class="koboSpan">) quantifying the irradiance created by an object.</span></li>
</ul>
<p><span id="kobo.1265.1" class="koboSpan">This model was extended to define the eight-dimensional (8D) reflectance field, which measures irradiance at the sensor determined by incident light rays displayed by an arbitrary projector in space (Debevec et al., 2000). </span><span id="kobo.1265.2" class="koboSpan">If we fix the viewpoint, the reflectance field can </span><span id="pg_53"><span id="kobo.1266.1" class="koboSpan">be reduced to be six-dimensional (6D). </span><span id="kobo.1266.2" class="koboSpan">Even so, capturing and storing data of this high dimensionality creates problems in practical scenarios. </span><span id="kobo.1266.3" class="koboSpan">The projector was also mounted on a robotic arm to acquire the reflectance field of a human face (Debevec et al., 2000). </span><span id="kobo.1266.4" class="koboSpan">This can be viewed as a pixel translated over the surface of a sphere, leading to a dimensionality reduction of the incident light field, and therefore a reduced final 4D reflectance field.</span></span></p>
<p><span id="kobo.1267.1" class="koboSpan">By controlling the color and intensity of light from various positions around the subject, it is possible to seamlessly integrate the image of the subject into a new scene (Debevec, 2002). </span><span id="kobo.1267.2" class="koboSpan">The reduction in size of the 4D reflectance field is very desirable in practice. </span><span id="kobo.1267.3" class="koboSpan">To this end, (Malzbender et al., 2001) observed that in changing the lighting incident angle, the color of a pixel changes with a function that can be closely approximated with a biquadratic polynomial. </span><span id="kobo.1267.4" class="koboSpan">This allowed them to store only the coefficients of the polynomial and subsequently use </span><i><span id="kobo.1268.1" class="koboSpan">compressive sensing</span></i><span id="kobo.1269.1" class="koboSpan"> to greatly reduce the size of the reflectance field. </span><span id="kobo.1269.2" class="koboSpan">However, as one might expect, specular reflections, which occur for only certain angles of the incident illumination, cause disturbances in the biquadratic polynomial approximation, and this remains an open problem in the field.</span></p>
<p><span id="kobo.1270.1" class="koboSpan">A 6D reflectance field is a better description of the scene. </span><span id="kobo.1270.2" class="koboSpan">Even though the number of illumination setups is theoretically equal to the number of pixels in each projector multiplied by the number of projectors </span><i><span id="kobo.1271.1" class="koboSpan">n</span></i><span id="kobo.1272.1" class="koboSpan">, it was shown that it can be simplified significantly by illuminating the scene with a single projector moving in </span><i><span id="kobo.1273.1" class="koboSpan">n</span></i><span id="kobo.1274.1" class="koboSpan"> positions (Masselus et al., 2003).</span></p>
</a></section><a id="sec2-3-4">
</a><section><a id="sec2-3-4">
</a><h4 id="sec19" class="head b-head"><a id="sec2-3-4"></a><a id="sec2-3-5"><span id="kobo.1275.1" class="koboSpan">2.3.5 Modifying Space and Time</span></a></h4><a id="sec2-3-5">
<p class="noindent"><span id="kobo.1276.1" class="koboSpan">In order to control the radiance of each ray emitted, one can use projector-like light sources, which allow controlling each individual pixel and not just the overall brightness. </span><span id="kobo.1276.2" class="koboSpan">It has been shown that using such a light source to assist capturing images with a camera allows extracting scene information that would be impossible to access using regular flash (Nayar et al., 2006). </span><span id="kobo.1276.3" class="koboSpan">The projectorlike device, called a CamPro, is rather bulky to fully replace the traditional flash but could be promising if implemented with smart lasers.</span></p>
<p><span id="kobo.1277.1" class="koboSpan">Clearly an important task is recovering the 3D shape of a scene from 2D images. </span><span id="kobo.1277.2" class="koboSpan">It turns out that the problem of recovering the 3D location of an object is closely connected to the </span><i><span id="kobo.1278.1" class="koboSpan">correspondence problem</span></i><span id="kobo.1279.1" class="koboSpan"> of pixels in images with different views. </span><span id="kobo.1279.2" class="koboSpan">This latter problem requires finding a correspondence between sets of points in each image, captured from a different angle, that matches the points in the 3D scene. </span><span id="kobo.1279.3" class="koboSpan">The correspondence problem can be solved by using a projector with temporal multiplexing, which enables projecting a certain pattern at a time that can be identified by cameras recording different perspectives.</span></p>
</a><p><a id="sec2-3-5"><span id="kobo.1280.1" class="koboSpan">Once the correspondence problem is solved, the 3D location is recovered via triangulation between camera and projector. </span><span id="kobo.1280.2" class="koboSpan">This can work with only a camera and a projector, as depicted in </span></a><a id="rfig2-32" href="chapter_2.xhtml#fig2-32"><span id="kobo.1281.1" class="koboSpan">Figure 2.32</span></a><span id="kobo.1282.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="fig2-32"><span id="kobo.1283.1" class="koboSpan"><img class="img2" src="../images/Figure2-32.png"/></span>
</a><figcaption><a id="fig2-32"></a><p class="CAP"><a id="fig2-32"><span class="FIGN"></span></a><a href="#rfig2-32"><span id="kobo.1284.1" class="koboSpan">Figure 2.32</span></a> <span class="FIG"><span id="kobo.1285.1" class="koboSpan">3D object localization and the correspondence problem for a single camera and a projector. </span><span id="kobo.1285.2" class="koboSpan">Several patterns are projected onto the scene object, which are detected by a single camera. </span><span id="kobo.1285.3" class="koboSpan">The object 3D localization is computed via triangulation.</span></span></p></figcaption>
</figure>
</div>
<p><span id="pg_54"><span id="kobo.1286.1" class="koboSpan">This problem is similar to the problem of </span><i><span id="kobo.1287.1" class="koboSpan">stereo triangulation</span></i><span id="kobo.1288.1" class="koboSpan">, in which a set of 3D points in the scene are identified given the disparity map between the images captured from two or more viewing angles. </span><span id="kobo.1288.2" class="koboSpan">In our case, instead of two passive cameras we have an active camera and a projector encoding the space via illumination, in a process called </span><i><span id="kobo.1289.1" class="koboSpan">active stereo triangulation</span></i><span id="kobo.1290.1" class="koboSpan">. </span><span id="kobo.1290.2" class="koboSpan">For readers piqued by stereo imaging, we discuss some examples in which epipolar geometry is used in time-of-flight imaging for sequential acquisition of strips of the image scene in section 10.5.1.</span></span></p>
<p><span id="kobo.1291.1" class="koboSpan">The number of patterns generated by the projector can be reduced by coding the boundaries between the projected shapes (Rusinkiewicz et al., 2002). </span><span id="kobo.1291.2" class="koboSpan">The projected light can also have a binary pattern, in which the pixels can either be turned off or have a fixed level of brightness (Posdamer and Altschuler, 1982).</span></p>
<p><span id="kobo.1292.1" class="koboSpan">The projector can be modulated in </span><i><span id="kobo.1293.1" class="koboSpan">space</span></i><span id="kobo.1294.1" class="koboSpan"> such that at a given time it is illuminating differently the points in the scene, or in </span><i><span id="kobo.1295.1" class="koboSpan">time</span></i><span id="kobo.1296.1" class="koboSpan"> such that the pattern changes in successive frames. </span><span id="kobo.1296.2" class="koboSpan">The two modulations can also be used in conjunction.</span></p>
<p><span id="kobo.1297.1" class="koboSpan">As we briefly mentioned in section 2.1.7, a programmable flash can be used to separate the light scattered by the scene in two components:</span></p>
<ul class="numbered">
<li class="NL"><span id="kobo.1298.1" class="koboSpan">1. </span><span id="kobo.1298.2" class="koboSpan">The </span><i><span id="kobo.1299.1" class="koboSpan">direct illumination</span></i><span id="kobo.1300.1" class="koboSpan">, caused by the light source, which enhances the material properties at a given point; and</span></li>
<li class="NL"><span id="kobo.1301.1" class="koboSpan">2. </span><span id="pg_55"><span id="kobo.1302.1" class="koboSpan">The </span><i><span id="kobo.1303.1" class="koboSpan">global illumination</span></i><span id="kobo.1304.1" class="koboSpan">, determined by other points in the scene, which reveals the optical properties of the objects such as how a certain patch in the scene is illuminated by the scene itself.</span></span></li>
</ul>
<p><span id="kobo.1305.1" class="koboSpan">One way to separate the two components was proposed in (Nayar et al., 2006), in which the projector was spatially encoded with a checkerboard binary pattern. </span><span id="kobo.1305.2" class="koboSpan">The scene was divided into square patches that were lit and unlit intermittently by the projector. </span><span id="kobo.1305.3" class="koboSpan">The technique is based on the observation that if one uses a high-frequency checkerboard pattern, the patches left unlit contain only global illumination components (light reflected from the lit patches). </span><span id="kobo.1305.4" class="koboSpan">On the other hand, the lit patches contain both global and direct illumination components.</span></p>
<p><span id="kobo.1306.1" class="koboSpan">This means that, theoretically, it is enough to capture two frames: one illuminated with the checkerboard pattern and one with its complement illumination pattern. </span><span id="kobo.1306.2" class="koboSpan">This ensures that it covers the whole scene and is enough to recover each illumination component. </span><span id="kobo.1306.3" class="koboSpan">However, due to the common leakage effect in off-the-shelf projectors, it is necessary to capture five times more images to compensate for this imperfection (Nayar et al., 2006). </span><span id="kobo.1306.4" class="koboSpan">The global and direct illumination merely separates one to several bounces of the lighting emitted by a projector. </span><span id="kobo.1306.5" class="koboSpan">It is possible to go one step further, and model the individual bounces of an optical ray (Seitz et al., 2005).</span></p>
<p><span id="kobo.1307.1" class="koboSpan">Apart from modulation in space, the projector can be modulated in </span><i><span id="kobo.1308.1" class="koboSpan">time</span></i><span id="kobo.1309.1" class="koboSpan"> by using high-frequency strobes to acquire snapshots of the scene periodically in a predefined pattern. </span><span id="kobo.1309.2" class="koboSpan">An interesting effect is that the illumination frequency is different from the frequency of a periodic movement in the scene. </span><span id="kobo.1309.3" class="koboSpan">In this case the captured images are characterized by a </span><i><span id="kobo.1310.1" class="koboSpan">perceived frequency</span></i><span id="kobo.1311.1" class="koboSpan">, which is the difference between the two frequencies.</span></p>
<p><span id="kobo.1312.1" class="koboSpan">Consequently, if the two frequencies are the same, the captured footage will show the scene object stagnating. </span><span id="kobo.1312.2" class="koboSpan">This is very useful in applications such as distortion detection in vocal cords. </span><span id="kobo.1312.3" class="koboSpan">By illuminating the cords with predefined frequencies, a physician can tell whether there is a physiological distortion in the cord movement.</span></p>
</section>
</section>
<section>
<h3 id="sec20" class="head a-head"><span id="kobo.1313.1" class="koboSpan">Chapter Appendix: Notation</span></h3>
<div class="longtable">
<figure class="table">
<table><thead>
<tr>
<th class="TB"><p class="TCH"><span id="kobo.1314.1" class="koboSpan">Notation</span></p></th>
<th class="TB"><p class="TCH"><span id="kobo.1315.1" class="koboSpan">Description</span></p></th>
</tr>
</thead><tbody>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1316.1" class="koboSpan">c</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1317.1" class="koboSpan">Speed of light through vacuum</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1318.1" class="koboSpan">ν</span></span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1319.1" class="koboSpan">Frequency</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1320.1" class="koboSpan">λ</span></span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1321.1" class="koboSpan">Wavelength</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1322.1" class="koboSpan">E</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1323.1" class="koboSpan">Energy</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1324.1" class="koboSpan">h</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1325.1" class="koboSpan">Planck’s constant</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span><span id="kobo.1326.1" class="koboSpan">Φ</span></span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1327.1" class="koboSpan">Radiant flux</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1328.1" class="koboSpan">R</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1329.1" class="koboSpan">Irradiance</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span id="pg_56"><i><span id="kobo.1330.1" class="koboSpan">M</span></i></span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1331.1" class="koboSpan">Exitance</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span><span id="kobo.1332.1" class="koboSpan">Ω</span></span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1333.1" class="koboSpan">Solid angle</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1334.1" class="koboSpan">I</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1335.1" class="koboSpan">Radiant intensity</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1336.1" class="koboSpan">L</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1337.1" class="koboSpan">Radiance</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1338.1" class="koboSpan">d</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1339.1" class="koboSpan">Distance between the pinhole and the projection plane</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1340.1" class="koboSpan">α</span></span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1341.1" class="koboSpan">Skew factor</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1342.1" class="koboSpan">δ</span></span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1343.1" class="koboSpan">Aperture size</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1344.1" class="koboSpan">n</span></i><sub><span id="kobo.1345.1" class="koboSpan">1</span></sub><i><span id="kobo.1346.1" class="koboSpan">, n</span></i><sub><span id="kobo.1347.1" class="koboSpan">2</span></sub></p></td>
<td class="TB"><p class="TB"><span id="kobo.1348.1" class="koboSpan">Indices of refraction</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1349.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.1350.1" class="koboSpan">1</span></sub><i><span id="kobo.1351.1" class="koboSpan">, </span><span><span id="kobo.1352.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.1353.1" class="koboSpan">2</span></sub></p></td>
<td class="TB"><p class="TB"><span id="kobo.1354.1" class="koboSpan">Angle of incidence and angle of refraction</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><samp><span id="kobo.1355.1" class="koboSpan">f</span></samp></p></td>
<td class="TB"><p class="TB"><span id="kobo.1356.1" class="koboSpan">Focal length</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1357.1" class="koboSpan">D</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1358.1" class="koboSpan">Aperture diameter</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1359.1" class="koboSpan">N</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1360.1" class="koboSpan">Lens speed, </span><samp><span id="kobo.1361.1" class="koboSpan">f</span></samp><span id="kobo.1362.1" class="koboSpan">-number</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span id="kobo.1363.1" class="koboSpan"><img class="inline" src="../images/pg56-in-1.png"/></span></p>
</td>
<td class="TB"><p class="TB"><span id="kobo.1364.1" class="koboSpan">Average irradiance</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span id="kobo.1365.1" class="koboSpan">1/</span><samp><span id="kobo.1366.1" class="koboSpan">f</span></samp></p></td>
<td class="TB"><p class="TB"><span id="kobo.1367.1" class="koboSpan">Focusing power of the lens</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><samp><span id="kobo.1368.1" class="koboSpan">f</span></samp><sub><i><span id="kobo.1369.1" class="koboSpan">c</span></i></sub></p></td>
<td class="TB"><p class="TB"><span id="kobo.1370.1" class="koboSpan">Focal length of the compound lens</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1371.1" class="koboSpan">r</span></i></p></td>
<td class="TB"><p class="TB"><span id="kobo.1372.1" class="koboSpan">Ratio of reflected light intensity</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1373.1" class="koboSpan">I</span></i><span id="kobo.1374.1" class="koboSpan">(</span><i><span id="kobo.1375.1" class="koboSpan">x, y</span></i><span id="kobo.1376.1" class="koboSpan">)</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1377.1" class="koboSpan">Light signal</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1378.1" class="koboSpan">ζ</span></span></i><span id="kobo.1379.1" class="koboSpan">(</span><i><span id="kobo.1380.1" class="koboSpan">x, y</span></i><span id="kobo.1381.1" class="koboSpan">)</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1382.1" class="koboSpan">Gaussian noise of zero mean and standard deviation 1</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span><span id="kobo.1383.1" class="koboSpan">σ</span></span></i><span id="kobo.1384.1" class="koboSpan"> (</span><i><span id="kobo.1385.1" class="koboSpan">I</span></i><span id="kobo.1386.1" class="koboSpan"> (</span><i><span id="kobo.1387.1" class="koboSpan">x, y</span></i><span id="kobo.1388.1" class="koboSpan">))</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1389.1" class="koboSpan">Signal dependent standard deviation</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1390.1" class="koboSpan">z</span></i><span id="kobo.1391.1" class="koboSpan">(</span><i><span id="kobo.1392.1" class="koboSpan">x, y</span></i><span id="kobo.1393.1" class="koboSpan">)</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1394.1" class="koboSpan">Sensor measurements at pixel (</span><i><span id="kobo.1395.1" class="koboSpan">x, y</span></i><span id="kobo.1396.1" class="koboSpan">)</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1397.1" class="koboSpan">P</span></i><span id="kobo.1398.1" class="koboSpan">(</span><i><span id="kobo.1399.1" class="koboSpan">r</span></i><span id="kobo.1400.1" class="koboSpan">)</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1401.1" class="koboSpan">Poisson distribution</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><b><span class="ePub-B"><span id="kobo.1402.1" class="koboSpan">I</span></span></b></p></td>
<td class="TB"><p class="TB"><span id="kobo.1403.1" class="koboSpan">Image</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><b><span class="ePub-B"><span id="kobo.1404.1" class="koboSpan">I</span></span></b><sub><i><span id="kobo.1405.1" class="koboSpan">s</span></i></sub></p></td>
<td class="TB"><p class="TB"><span id="kobo.1406.1" class="koboSpan">Sampled image</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><i><span id="kobo.1407.1" class="koboSpan">f</span></i><sub><span id="kobo.1408.1" class="koboSpan">MAX</span></sub></p></td>
<td class="TB"><p class="TB"><span id="kobo.1409.1" class="koboSpan">Maximum frequency in the signal</span></p></td>
</tr>
<tr>
<td class="TB"><p class="TB"><span id="kobo.1410.1" class="koboSpan">ℱ</span></p></td>
<td class="TB"><p class="TB"><span id="kobo.1411.1" class="koboSpan">Fourier transform operator</span></p></td>
</tr>
<tr>
<td class="TBL"><p class="TB"><b><span class="ePub-B"><span id="kobo.1412.1" class="koboSpan">I</span></span></b><sub><i><span id="kobo.1413.1" class="koboSpan">f</span></i></sub></p></td>
<td class="TBL"><p class="TB"><span id="kobo.1414.1" class="koboSpan">Filtered image</span></p></td>
</tr>
</tbody></table>
</figure>
</div>
</section>
<section>
<div class="break">
<h3 id="sec21" class="head a-head"><a id="Q1-1-74"><span id="pg_57"><span id="kobo.1415.1" class="koboSpan">Exercises</span></span></a></h3><a id="Q1-1-74">
<p class="NL-E"><span id="kobo.1416.1" class="koboSpan">1. Light ray bending</span></p>
<p class="AL-F"><span id="kobo.1417.1" class="koboSpan">a) Snell’s law</span></p>
</a><div class="figure"><a id="Q1-1-74">
</a><figure class="IMG"><a id="Q1-1-74"></a><a id="figE2-1"><span id="kobo.1418.1" class="koboSpan"><img class="img3" src="../images/FigureE2-1.png"/></span>
<figcaption><p class="CAP"><span class="FIGN"><span id="kobo.1419.1" class="koboSpan">Figure E2.1</span></span> <span class="FIG"><span id="kobo.1420.1" class="koboSpan">Modeling the principle of refraction via Snell’s law.</span></span></p></figcaption>
</a></figure><a id="figE2-1">
</a></div><a id="figE2-1">
<p class="AL-N"><span id="kobo.1421.1" class="koboSpan">Assume that a light ray passes the smooth boundary between air and water with an incident angle </span><i><span><span id="kobo.1422.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.1423.1" class="koboSpan">1</span></sub><span id="kobo.1424.1" class="koboSpan"> = </span><i><span><span id="kobo.1425.1" class="koboSpan">π</span></span><span id="kobo.1426.1" class="koboSpan">/</span></i><span id="kobo.1427.1" class="koboSpan">6. </span><span id="kobo.1427.2" class="koboSpan">Calculate </span><i><span><span id="kobo.1428.1" class="koboSpan">θ</span></span></i><sub><span id="kobo.1429.1" class="koboSpan">2</span></sub><span id="kobo.1430.1" class="koboSpan"> to two decimal places knowing that the refraction index for air is </span><i><span id="kobo.1431.1" class="koboSpan">n</span></i><sub><span id="kobo.1432.1" class="koboSpan">1</span></sub><span id="kobo.1433.1" class="koboSpan"> ≃ 1 and for water </span><i><span id="kobo.1434.1" class="koboSpan">n</span></i><sub><span id="kobo.1435.1" class="koboSpan">2</span></sub><span id="kobo.1436.1" class="koboSpan"> ≃ 1.33.</span></p>
<p class="AL"><span id="kobo.1437.1" class="koboSpan">b) Thin lens</span></p>
</a><p class="AL-N"><a id="figE2-1"><span id="pg_58"><span id="kobo.1438.1" class="koboSpan">Consider the thin lens setup in </span></span></a><a id="rfigE2-2" href="chapter_2.xhtml#figE2-2"><span id="kobo.1439.1" class="koboSpan">Figure E2.2</span></a><span id="kobo.1440.1" class="koboSpan">. </span><span id="kobo.1440.2" class="koboSpan">An object is located on the lens optical axis such that its reflected light rays that pass through the lens aperture cover an angle </span><i><span><span id="kobo.1441.1" class="koboSpan">α</span></span></i><sub><span id="kobo.1442.1" class="koboSpan">1</span></sub><span id="kobo.1443.1" class="koboSpan">. </span><span id="kobo.1443.2" class="koboSpan">The object is in focus and is projected onto the sensor plane at a point where the field of view is given by angle </span><i><span><span id="kobo.1444.1" class="koboSpan">α</span></span></i><sub><span id="kobo.1445.1" class="koboSpan">2</span></sub><span id="kobo.1446.1" class="koboSpan">. </span><span id="kobo.1446.2" class="koboSpan">Assuming that the aperture diameter </span><i><span id="kobo.1447.1" class="koboSpan">d</span></i><span id="kobo.1448.1" class="koboSpan"> is known, derive analytically the expressions of lens focal length </span><i><span id="kobo.1449.1" class="koboSpan">f</span></i><span id="kobo.1450.1" class="koboSpan"> and the distances from the lens to the object </span><i><span id="kobo.1451.1" class="koboSpan">S</span></i><sub><span id="kobo.1452.1" class="koboSpan">1</span></sub><span id="kobo.1453.1" class="koboSpan"> and to the sensor plane </span><i><span id="kobo.1454.1" class="koboSpan">S</span></i><sub><span id="kobo.1455.1" class="koboSpan">2</span></sub><span id="kobo.1456.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="figE2-2"><span id="kobo.1457.1" class="koboSpan"><img class="img2" src="../images/FigureE2-2.png"/></span>
</a><figcaption><a id="figE2-2"></a><p class="CAP"><a id="figE2-2"><span class="FIGN"></span></a><a href="#rfigE2-2"><span id="kobo.1458.1" class="koboSpan">Figure E2.2</span></a> <span class="FIG"><span id="kobo.1459.1" class="koboSpan">The proposed thin lens setup.</span></span></p></figcaption>
</figure>
</div>
<p class="NL-E"><span id="kobo.1460.1" class="koboSpan">2. Capturing images via sampling and quantization</span></p>
<p class="AL-F"><span id="kobo.1461.1" class="koboSpan">a) Image sampling</span><br/><span id="kobo.1462.1" class="koboSpan">When an image is captured by a digital camera, it is sampled spatially by the sensor array, and then each pixel value is coded with a number of bits in a process called quantization.</span></p>
<p class="AL-N"><span id="kobo.1463.1" class="koboSpan">Let us simulate this process by starting with a high-resolution grayscale image, with size around 2000 × 2000 as shown in </span><a id="rfigE2-3" href="chapter_2.xhtml#figE2-3"><span id="kobo.1464.1" class="koboSpan">Figure E2.3a</span></a><span id="kobo.1465.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="figE2-3"><span id="kobo.1466.1" class="koboSpan"><img class="img1" src="../images/FigureE2-3.png"/></span>
</a><figcaption><a id="figE2-3"></a><p class="CAP"><a id="figE2-3"><span class="FIGN"></span></a><a href="#rfigE2-3"><span id="kobo.1467.1" class="koboSpan">Figure E2.3</span></a> <span class="FIG"><span id="kobo.1468.1" class="koboSpan">Simulating the sampling and quantization done by a sensor array.</span></span></p></figcaption>
</figure>
</div>
<p class="AL-N"><span id="kobo.1469.1" class="koboSpan">For demonstration purposes, let us assume we have a sensor array with size 80 × 80 that captures the image.</span></p>
<p class="AL-N"><span id="kobo.1470.1" class="koboSpan">To simulate the spatial sampling process, we need to divide the original image pixels into 80 × 80 blocks and then average the pixels in each block, just as a sensor pixel would average all incident light intensities. </span><span id="kobo.1470.2" class="koboSpan">This should lead to an image similar to the one shown in </span><a href="chapter_2.xhtml#figE2-3"><span id="kobo.1471.1" class="koboSpan">Figure E2.3b</span></a><span id="kobo.1472.1" class="koboSpan">. </span><span id="kobo.1472.2" class="koboSpan">Plot a result using your own image.</span></p>
<p class="AL-N"><span id="kobo.1473.1" class="koboSpan">What is a quick procedure to sample the image in the described way? </span><span id="kobo.1473.2" class="koboSpan">Hint: it may involve convolving with a kernel.</span></p>
<p class="AL"><span id="kobo.1474.1" class="koboSpan">b) Image quantization</span><br/><span id="kobo.1475.1" class="koboSpan">Next, the captured pixel values need to be quantized. </span><span id="kobo.1475.2" class="koboSpan">Of course, in your image they already are quantized (most probably in the range 0–255), but here we implement a course quantization that would enable a good visualization of the process. </span><span id="kobo.1475.3" class="koboSpan">For example, a 4-bit quantization would involve mapping each pixel in an image to a value in {0, 1,</span><span class="ellipsis"><span id="kobo.1476.1" class="koboSpan">…</span></span><span id="kobo.1477.1" class="koboSpan">, 15}. </span><span id="kobo.1477.2" class="koboSpan">Is there a quick way to implement quantization without loops?</span></p>
<p class="AL-N"><span id="kobo.1478.1" class="koboSpan">Generate your own images after 4-bit and 3-bit quantization. </span><span id="kobo.1478.2" class="koboSpan">The results should look similar to those shown in </span><a href="chapter_2.xhtml#figE2-3"><span id="kobo.1479.1" class="koboSpan">Figure E2.3c</span></a><span id="kobo.1480.1" class="koboSpan">,d.</span></p>
<p class="AL"><span id="kobo.1481.1" class="koboSpan">c) Image interpolation</span><br/><span id="kobo.1482.1" class="koboSpan">Subsequent processing tasks could require a higher resolution image. </span><span id="kobo.1482.2" class="koboSpan">How could the new pixel values be computed? </span><span id="kobo.1482.3" class="koboSpan">The most straightforward way is to apply interpolation, as shown in </span><a id="rfigE2-4" href="chapter_2.xhtml#figE2-4"><span id="kobo.1483.1" class="koboSpan">Figure E2.4a</span></a><span id="kobo.1484.1" class="koboSpan">. </span><span id="kobo.1484.2" class="koboSpan">The distortion of the image due to quantization is still present 0–255, but it can be addressed via filtering, as shown in </span><a href="chapter_2.xhtml#figE2-4"><span id="kobo.1485.1" class="koboSpan">Figure E2.4b</span></a><span id="kobo.1486.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="figE2-4"><span id="kobo.1487.1" class="koboSpan"><img class="img1" src="../images/FigureE2-4.png"/></span>
</a><figcaption><a id="figE2-4"></a><p class="CAP"><a id="figE2-4"><span class="FIGN"></span></a><a href="#rfigE2-4"><span id="kobo.1488.1" class="koboSpan">Figure E2.4</span></a> <span class="FIG"><span id="kobo.1489.1" class="koboSpan">Increasing the resolution of a sampled and quantized image via interpolation.</span></span></p></figcaption>
</figure>
</div>
<p class="AL-N"><span id="pg_59"><span id="kobo.1490.1" class="koboSpan">Now apply these steps to your quantized image, increasing the resolution to match the original image. </span><span id="kobo.1490.2" class="koboSpan">Then filter the high-resolution quantized image with a 2 × 2 matrix of ones. </span><span id="kobo.1490.3" class="koboSpan">The results should be similar to those shown in </span><a href="chapter_2.xhtml#figE2-4"><span id="kobo.1491.1" class="koboSpan">Figure E2.4</span></a><span id="kobo.1492.1" class="koboSpan">.</span></span></p>
<p class="AL-N"><span id="pg_60"><span id="kobo.1493.1" class="koboSpan">What differences do you notice between the aspect of the low-resolution and the high-resolution quantized image? </span><span id="kobo.1493.2" class="koboSpan">What drawback do you see in filtering? </span><span id="kobo.1493.3" class="koboSpan">What do you imagine would happen if the filtering were repeated a large number of times?</span></span></p>
<p class="AL-N"><span id="kobo.1494.1" class="koboSpan">Let </span><i><span id="kobo.1495.1" class="koboSpan">I</span></i><span id="kobo.1496.1" class="koboSpan"> (</span><i><span id="kobo.1497.1" class="koboSpan">x, y</span></i><span id="kobo.1498.1" class="koboSpan">) denote a continuous image, and let </span><i><span id="kobo.1499.1" class="koboSpan">I</span></i><sub><i><span id="kobo.1500.1" class="koboSpan">S</span></i></sub><span id="kobo.1501.1" class="koboSpan">(</span><i><span id="kobo.1502.1" class="koboSpan">k, l</span></i><span id="kobo.1503.1" class="koboSpan">) denote the samples taken with period 1. </span><span id="kobo.1503.2" class="koboSpan">Demonstrate that the linear interpolation of </span><i><span id="kobo.1504.1" class="koboSpan">I</span></i><span id="kobo.1505.1" class="koboSpan"> (</span><i><span id="kobo.1506.1" class="koboSpan">x, y</span></i><span id="kobo.1507.1" class="koboSpan">) at samples </span><i><span id="kobo.1508.1" class="koboSpan">I</span></i><sub><i><span id="kobo.1509.1" class="koboSpan">S</span></i></sub><span id="kobo.1510.1" class="koboSpan"> (</span><i><span id="kobo.1511.1" class="koboSpan">k, l</span></i><span id="kobo.1512.1" class="koboSpan">) is</span></p>
<p class="DIS-IMG"><span id="kobo.1513.1" class="koboSpan"><img class="img1" src="../images/pg60-1.png"/></span></p>
<p class="AL"><span id="kobo.1514.1" class="koboSpan">d) Brightness and contrast adjustment</span><br/><span id="kobo.1515.1" class="koboSpan">In this example we examine the concepts of brightness and contrast by manually adjusting them for a chosen image. </span><span id="kobo.1515.2" class="koboSpan">The image pixels with adjusted contrast and brightness are defined</span></p>
<p class="DIS-IMG"><span id="kobo.1516.1" class="koboSpan"><img class="img1" src="../images/pg60-2.png"/></span></p>
<p class="AL-N"><span id="kobo.1517.1" class="koboSpan">where </span><i><span id="kobo.1518.1" class="koboSpan">B, C</span></i><span id="kobo.1519.1" class="koboSpan"> ∈ ℝ denote the brightness and contrast adjustments, respectively. </span><span id="kobo.1519.2" class="koboSpan">Now choose a color image and change its brightness and contrast to enhance its features, as shown in </span><a id="rfigE2-5" href="chapter_2.xhtml#figE2-5"><span id="kobo.1520.1" class="koboSpan">Figure E2.5</span></a><span id="kobo.1521.1" class="koboSpan">.</span></p>
<div class="figure">
<figure class="IMG"><a id="figE2-5"><span id="kobo.1522.1" class="koboSpan"><img class="img1" src="../images/FigureE2-5.png"/></span>
</a><figcaption><a id="figE2-5"></a><p class="CAP"><a id="figE2-5"><span class="FIGN"></span></a><a href="#rfigE2-5"><span id="kobo.1523.1" class="koboSpan">Figure E2.5</span></a> <span class="FIG"><span id="kobo.1524.1" class="koboSpan">The adjustment of brightness and contrast in digital images.</span></span></p></figcaption>
</figure>
</div>
<p class="AL-N"><span id="kobo.1525.1" class="koboSpan">What is the drawback that prevents capturing an image with arbitrary high sharpness with a pinhole camera? </span><span id="kobo.1525.2" class="koboSpan">How is the general quality of the brightness of pinhole camera </span><span id="pg_61"><span id="kobo.1526.1" class="koboSpan">photos, and what trade-off is involved in attempting to adjust it? </span><span id="kobo.1526.2" class="koboSpan">Can we say that the pinspeck camera solved the brightness problem of the captured images?</span></span></p>
<p class="NL-E"><span id="kobo.1527.1" class="koboSpan">3. </span><span id="pg_62"><span id="kobo.1528.1" class="koboSpan">Image deconvolution</span></span></p>
<p class="AL-F"><span id="kobo.1529.1" class="koboSpan">a) Problem setup</span><br/><span id="kobo.1530.1" class="koboSpan">Under LSI conditions, deblurring can be posed as follows:</span></p>
<p class="DIS-IMG"><span id="kobo.1531.1" class="koboSpan"><img class="img1" src="../images/eq2-1.png"/></span></p>
<p class="AL-N"><span id="kobo.1532.1" class="koboSpan">where </span><i><span id="kobo.1533.1" class="koboSpan">J</span></i><span id="kobo.1534.1" class="koboSpan"> (</span><i><span id="kobo.1535.1" class="koboSpan">k, l</span></i><span id="kobo.1536.1" class="koboSpan">) is the observed image; </span><i><span id="kobo.1537.1" class="koboSpan">I</span></i><span id="kobo.1538.1" class="koboSpan"> (</span><i><span id="kobo.1539.1" class="koboSpan">k, l</span></i><span id="kobo.1540.1" class="koboSpan">) is the original image; </span><i><span id="kobo.1541.1" class="koboSpan">h</span></i><span id="kobo.1542.1" class="koboSpan"> (</span><i><span id="kobo.1543.1" class="koboSpan">k, l</span></i><span id="kobo.1544.1" class="koboSpan">) is the kernel; and </span><i><span id="kobo.1545.1" class="koboSpan">e</span></i><span id="kobo.1546.1" class="koboSpan"> (</span><i><span id="kobo.1547.1" class="koboSpan">k, l</span></i><span id="kobo.1548.1" class="koboSpan">) is the noise.</span></p>
<p class="AL-N"><span id="kobo.1549.1" class="koboSpan">To recover the original image, we usually apply deconvolution algorithms to the observations. </span><span id="kobo.1549.2" class="koboSpan">In this exercise, we implement several deconvolution algorithms and compare their performances. </span><span id="kobo.1549.3" class="koboSpan">To evaluate the performance quantitatively, we adopt two image similarity metrics: (1) peak signal-to-noise ratio (PSNR), and (2) structural similarity (SSIM) index.</span></p>
<p class="AL-N"><span id="kobo.1550.1" class="koboSpan">The PSNR between two images, </span><i><span id="kobo.1551.1" class="koboSpan">I</span></i><sub><span id="kobo.1552.1" class="koboSpan">1</span></sub><span id="kobo.1553.1" class="koboSpan"> (</span><i><span id="kobo.1554.1" class="koboSpan">k, l</span></i><span id="kobo.1555.1" class="koboSpan">) and </span><i><span id="kobo.1556.1" class="koboSpan">I</span></i><sub><span id="kobo.1557.1" class="koboSpan">2</span></sub><span id="kobo.1558.1" class="koboSpan"> (</span><i><span id="kobo.1559.1" class="koboSpan">k, l</span></i><span id="kobo.1560.1" class="koboSpan">), can be calculated as follows:</span></p>
<p class="DIS-IMG"><span id="kobo.1561.1" class="koboSpan"><img class="img1" src="../images/eq2-2.png"/></span></p>
<p class="AL-N"><span id="kobo.1562.1" class="koboSpan">where </span><i><span id="kobo.1563.1" class="koboSpan">R</span></i><span id="kobo.1564.1" class="koboSpan"> is the data range of the images, and (</span><i><span id="kobo.1565.1" class="koboSpan">K</span></i><span id="kobo.1566.1" class="koboSpan">, </span><i><span id="kobo.1567.1" class="koboSpan">L</span></i><span id="kobo.1568.1" class="koboSpan">) is the image size.</span></p>
<p class="AL-N"><span id="kobo.1569.1" class="koboSpan">The SSIM between </span><i><span id="kobo.1570.1" class="koboSpan">I</span></i><sub><span id="kobo.1571.1" class="koboSpan">1</span></sub><span id="kobo.1572.1" class="koboSpan"> (</span><i><span id="kobo.1573.1" class="koboSpan">k, l</span></i><span id="kobo.1574.1" class="koboSpan">) and </span><i><span id="kobo.1575.1" class="koboSpan">I</span></i><sub><span id="kobo.1576.1" class="koboSpan">2</span></sub><span id="kobo.1577.1" class="koboSpan"> (</span><i><span id="kobo.1578.1" class="koboSpan">k, l</span></i><span id="kobo.1579.1" class="koboSpan">) is calculated on the basis of three measurements, comprising luminance (ℒ ), contrast (풞 ), and structure (풮 ):</span></p>
<p class="DIS-IMG"><span id="kobo.1580.1" class="koboSpan"><img class="img1" src="../images/eq2-3.png"/></span></p>
<p class="AL-N"><span id="kobo.1581.1" class="koboSpan">where </span><i><span><span id="kobo.1582.1" class="koboSpan">α</span></span></i><span id="kobo.1583.1" class="koboSpan">, </span><i><span><span id="kobo.1584.1" class="koboSpan">β</span></span></i><span id="kobo.1585.1" class="koboSpan">, and </span><i><span><span id="kobo.1586.1" class="koboSpan">γ</span></span></i><span id="kobo.1587.1" class="koboSpan"> are the weights for the three measurements; </span><i><span><span id="kobo.1588.1" class="koboSpan">μ</span></span></i><sub><i><span id="kobo.1589.1" class="koboSpan">I</span></i><sub><span id="kobo.1590.1" class="koboSpan">1</span></sub></sub><span id="kobo.1591.1" class="koboSpan">, </span><i><span><span id="kobo.1592.1" class="koboSpan">μ</span></span></i><sub><i><span id="kobo.1593.1" class="koboSpan">I</span></i><sub><span id="kobo.1594.1" class="koboSpan">2</span></sub></sub><span id="kobo.1595.1" class="koboSpan">, </span><i><span><span id="kobo.1596.1" class="koboSpan">σ</span></span></i><sub><i><span id="kobo.1597.1" class="koboSpan">I</span></i><sub><span id="kobo.1598.1" class="koboSpan">1</span></sub></sub><span id="kobo.1599.1" class="koboSpan">, </span><i><span><span id="kobo.1600.1" class="koboSpan">σ</span></span></i><sub><i><span id="kobo.1601.1" class="koboSpan">I</span></i><sub><span id="kobo.1602.1" class="koboSpan">2</span></sub></sub><span id="kobo.1603.1" class="koboSpan">, and </span><i><span><span id="kobo.1604.1" class="koboSpan">σ</span></span></i><sub><i><span id="kobo.1605.1" class="koboSpan">I</span></i><sub><span id="kobo.1606.1" class="koboSpan">1</span></sub><i><span id="kobo.1607.1" class="koboSpan">I</span></i><sub><span id="kobo.1608.1" class="koboSpan">2</span></sub></sub><span id="kobo.1609.1" class="koboSpan"> are the local means, standard deviations, and correlation coefficient for images </span><i><span id="kobo.1610.1" class="koboSpan">I</span></i><sub><span id="kobo.1611.1" class="koboSpan">1</span></sub><span id="kobo.1612.1" class="koboSpan"> (</span><i><span id="kobo.1613.1" class="koboSpan">k, l</span></i><span id="kobo.1614.1" class="koboSpan">) and </span><i><span id="kobo.1615.1" class="koboSpan">I</span></i><sub><span id="kobo.1616.1" class="koboSpan">2</span></sub><span id="kobo.1617.1" class="koboSpan"> (</span><i><span id="kobo.1618.1" class="koboSpan">k, l</span></i><span id="kobo.1619.1" class="koboSpan">); and </span><i><span id="kobo.1620.1" class="koboSpan">C</span></i><sub><span id="kobo.1621.1" class="koboSpan">1</span></sub><span id="kobo.1622.1" class="koboSpan">, </span><i><span id="kobo.1623.1" class="koboSpan">C</span></i><sub><span id="kobo.1624.1" class="koboSpan">2</span></sub><span id="kobo.1625.1" class="koboSpan">, and </span><i><span id="kobo.1626.1" class="koboSpan">C</span></i><sub><span id="kobo.1627.1" class="koboSpan">3</span></sub><span id="kobo.1628.1" class="koboSpan"> are three variables to stabilize the division.</span></p>
<p class="AL-N"><span id="pg_63"><span id="kobo.1629.1" class="koboSpan">Refer to PSNR and SSIM functions in the scikit-image package for more details.</span><sup><a id="fn1x2-bk" href="chapter_2.xhtml#fn1x2"><span id="kobo.1630.1" class="koboSpan">1</span></a></sup><span id="kobo.1631.1" class="koboSpan"> Make sure you have changed the corresponding parameters of the SSIM function in scikit-image to match the implementation of (Wang et al., 2004). </span><span id="kobo.1631.2" class="koboSpan">Remember to normalize the images before reporting the PSNR and SSIM scores.</span></span></p>
<p class="AL"><span id="kobo.1632.1" class="koboSpan">b) Image preparation</span><br/><span id="kobo.1633.1" class="koboSpan">To test the performance of different deconvolution algorithms, we need to generate a blurry image using a known blur kernel. </span><span id="kobo.1633.2" class="koboSpan">The procedures are the following:</span></p>
<ul class="numbered">
<li class="NLF"><span id="kobo.1634.1" class="koboSpan">1. </span><span id="kobo.1634.2" class="koboSpan">Pick an image from the BSDS500 dataset, and convert it to gray scale.</span><sup><a id="fn2x2-bk" href="chapter_2.xhtml#fn2x2"><span id="kobo.1635.1" class="koboSpan">2</span></a></sup><span id="kobo.1636.1" class="koboSpan"> Crop a 256 × 256 region from the selected image. </span><span id="kobo.1636.2" class="koboSpan">Denote this image </span><i><span id="kobo.1637.1" class="koboSpan">I</span></i><span id="kobo.1638.1" class="koboSpan"> (</span><i><span id="kobo.1639.1" class="koboSpan">k, l</span></i><span id="kobo.1640.1" class="koboSpan">).</span></li>
<li class="NL"><span id="kobo.1641.1" class="koboSpan">2. </span><span id="kobo.1641.2" class="koboSpan">Perform 2D convolution (with zero padding) on the cropped image using an identity matrix of size 21 as the kernel, </span><i><span id="kobo.1642.1" class="koboSpan">h</span></i><span id="kobo.1643.1" class="koboSpan"> (</span><i><span id="kobo.1644.1" class="koboSpan">k, l</span></i><span id="kobo.1645.1" class="koboSpan">). </span><span id="kobo.1645.2" class="koboSpan">Remember to normalize the kernel to make sure that it sums up to 1 before convolution. </span><span id="kobo.1645.3" class="koboSpan">Denote the obtained image </span><i><span id="kobo.1646.1" class="koboSpan">I</span></i><sub><span id="kobo.1647.1" class="koboSpan">noiseless</span></sub><span id="kobo.1648.1" class="koboSpan"> (</span><i><span id="kobo.1649.1" class="koboSpan">k, l</span></i><span id="kobo.1650.1" class="koboSpan">).</span></li>
<li class="NLL"><span id="kobo.1651.1" class="koboSpan">3. </span><span id="kobo.1651.2" class="koboSpan">Calculate the standard deviation of </span><i><span id="kobo.1652.1" class="koboSpan">I</span></i><span id="kobo.1653.1" class="koboSpan"> (</span><i><span id="kobo.1654.1" class="koboSpan">k, l</span></i><span id="kobo.1655.1" class="koboSpan">) and add Guassian noise with zero mean and standard deviation of 0.01·STD(</span><i><span id="kobo.1656.1" class="koboSpan">I</span></i><span id="kobo.1657.1" class="koboSpan"> (</span><i><span id="kobo.1658.1" class="koboSpan">k, l</span></i><span id="kobo.1659.1" class="koboSpan">)) to </span><i><span id="kobo.1660.1" class="koboSpan">I</span></i><sub><span id="kobo.1661.1" class="koboSpan">noiseless</span></sub><span id="kobo.1662.1" class="koboSpan"> (</span><i><span id="kobo.1663.1" class="koboSpan">k, l</span></i><span id="kobo.1664.1" class="koboSpan">). </span><span id="kobo.1664.2" class="koboSpan">Denote the noisy image as </span><i><span id="kobo.1665.1" class="koboSpan">I</span></i><sub><span id="kobo.1666.1" class="koboSpan">noisy</span></sub><span id="kobo.1667.1" class="koboSpan"> (</span><i><span id="kobo.1668.1" class="koboSpan">k, l</span></i><span id="kobo.1669.1" class="koboSpan">).</span></li>
</ul>
<p class="RL-F"><span id="kobo.1670.1" class="koboSpan">  i. Blurry image</span><br/><span id="kobo.1671.1" class="koboSpan">Plot </span><i><span id="kobo.1672.1" class="koboSpan">I</span></i><span id="kobo.1673.1" class="koboSpan"> (</span><i><span id="kobo.1674.1" class="koboSpan">k, l</span></i><span id="kobo.1675.1" class="koboSpan">), </span><i><span id="kobo.1676.1" class="koboSpan">I</span></i><sub><span id="kobo.1677.1" class="koboSpan">noiseless</span></sub><span id="kobo.1678.1" class="koboSpan"> (</span><i><span id="kobo.1679.1" class="koboSpan">k, l</span></i><span id="kobo.1680.1" class="koboSpan">) and </span><i><span id="kobo.1681.1" class="koboSpan">I</span></i><sub><span id="kobo.1682.1" class="koboSpan">noisy</span></sub><span id="kobo.1683.1" class="koboSpan"> (</span><i><span id="kobo.1684.1" class="koboSpan">k, l</span></i><span id="kobo.1685.1" class="koboSpan">). </span><span id="kobo.1685.2" class="koboSpan">What are the sizes of </span><i><span id="kobo.1686.1" class="koboSpan">I</span></i><sub><span id="kobo.1687.1" class="koboSpan">noiseless</span></sub><span id="kobo.1688.1" class="koboSpan"> (</span><i><span id="kobo.1689.1" class="koboSpan">k, l</span></i><span id="kobo.1690.1" class="koboSpan">) and </span><i><span id="kobo.1691.1" class="koboSpan">I</span></i><sub><span id="kobo.1692.1" class="koboSpan">noisy</span></sub><span id="kobo.1693.1" class="koboSpan"> (</span><i><span id="kobo.1694.1" class="koboSpan">k, l</span></i><span id="kobo.1695.1" class="koboSpan">)?</span></p>
<p class="RL"><span id="kobo.1696.1" class="koboSpan"> ii. Metric baseline</span><br/><span id="kobo.1697.1" class="koboSpan">Briefly describe the differences between PSNR metric and SSIM metric. </span><span id="kobo.1697.2" class="koboSpan">Report PSNR(</span><i><span id="kobo.1698.1" class="koboSpan">I</span></i><span id="kobo.1699.1" class="koboSpan">, </span><i><span id="kobo.1700.1" class="koboSpan">I</span></i><sub><span id="kobo.1701.1" class="koboSpan">noiseless</span></sub><span id="kobo.1702.1" class="koboSpan">); SSIM(</span><i><span id="kobo.1703.1" class="koboSpan">I</span></i><span id="kobo.1704.1" class="koboSpan">, </span><i><span id="kobo.1705.1" class="koboSpan">I</span></i><sub><span id="kobo.1706.1" class="koboSpan">noiseless</span></sub><span id="kobo.1707.1" class="koboSpan">); PSNR(</span><i><span id="kobo.1708.1" class="koboSpan">I</span></i><span id="kobo.1709.1" class="koboSpan">, </span><i><span id="kobo.1710.1" class="koboSpan">I</span></i><sub><span id="kobo.1711.1" class="koboSpan">noisy</span></sub><span id="kobo.1712.1" class="koboSpan">); and SSIM(</span><i><span id="kobo.1713.1" class="koboSpan">I</span></i><span id="kobo.1714.1" class="koboSpan">, </span><i><span id="kobo.1715.1" class="koboSpan">I</span></i><sub><span id="kobo.1716.1" class="koboSpan">noisy</span></sub><span id="kobo.1717.1" class="koboSpan">). </span><span id="kobo.1717.2" class="koboSpan">You can crop the center 256 × 256 regions from </span><i><span id="kobo.1718.1" class="koboSpan">I</span></i><sub><span id="kobo.1719.1" class="koboSpan">noiseless</span></sub><span id="kobo.1720.1" class="koboSpan"> (</span><i><span id="kobo.1721.1" class="koboSpan">k, l</span></i><span id="kobo.1722.1" class="koboSpan">) and </span><i><span id="kobo.1723.1" class="koboSpan">I</span></i><sub><span id="kobo.1724.1" class="koboSpan">noisy</span></sub><span id="kobo.1725.1" class="koboSpan"> (</span><i><span id="kobo.1726.1" class="koboSpan">k, l</span></i><span id="kobo.1727.1" class="koboSpan">) when calculating PSNR and SSIM.</span></p>
<p class="AL-F"><span id="kobo.1728.1" class="koboSpan">c) Naive deconvolution</span><br/><span id="kobo.1729.1" class="koboSpan">Implement a function to conduct naive deconvolution. </span><span id="kobo.1729.2" class="koboSpan">The function should take the blurry observation, </span><i><span id="kobo.1730.1" class="koboSpan">J</span></i><span id="kobo.1731.1" class="koboSpan"> (</span><i><span id="kobo.1732.1" class="koboSpan">k, l</span></i><span id="kobo.1733.1" class="koboSpan">) and the blur kernel, </span><i><span id="kobo.1734.1" class="koboSpan">h</span></i><span id="kobo.1735.1" class="koboSpan"> (</span><i><span id="kobo.1736.1" class="koboSpan">k, l</span></i><span id="kobo.1737.1" class="koboSpan">) as the input parameters and return the recovered image, </span><i><span id="kobo.1738.1" class="koboSpan">Î</span></i><span id="kobo.1739.1" class="koboSpan"> (</span><i><span id="kobo.1740.1" class="koboSpan">k, l</span></i><span id="kobo.1741.1" class="koboSpan">). </span><span id="kobo.1741.2" class="koboSpan">The discrete Fourier transform functions in NumPy might be useful.</span><sup><a id="fn3x2-bk" href="chapter_2.xhtml#fn3x2"><span id="kobo.1742.1" class="koboSpan">3</span></a></sup></p>
<p class="RL-F"><span id="kobo.1743.1" class="koboSpan">  i. Naive deconvolution algorithm</span><br/><span id="kobo.1744.1" class="koboSpan">Apply your naive deconvolution algorithm to both </span><i><span id="kobo.1745.1" class="koboSpan">I</span></i><sub><span id="kobo.1746.1" class="koboSpan">noiseless</span></sub><span id="kobo.1747.1" class="koboSpan"> (</span><i><span id="kobo.1748.1" class="koboSpan">k, l</span></i><span id="kobo.1749.1" class="koboSpan">) and </span><i><span id="kobo.1750.1" class="koboSpan">I</span></i><sub><span id="kobo.1751.1" class="koboSpan">noisy</span></sub><span id="kobo.1752.1" class="koboSpan"> (</span><i><span id="kobo.1753.1" class="koboSpan">k, l</span></i><span id="kobo.1754.1" class="koboSpan">). </span><span id="pg_64"><span id="kobo.1755.1" class="koboSpan">Plot the recovered images, and report their PSNR and SSIM scores with </span><i><span id="kobo.1756.1" class="koboSpan">I</span></i><span id="kobo.1757.1" class="koboSpan"> (</span><i><span id="kobo.1758.1" class="koboSpan">k, l</span></i><span id="kobo.1759.1" class="koboSpan">). </span><span id="kobo.1759.2" class="koboSpan">Remember to crop the boundaries of the recovered images.</span></span></p>
<p class="RL"><span id="kobo.1760.1" class="koboSpan"> ii. Naive deconvolution results</span><br/><span id="kobo.1761.1" class="koboSpan">Why are the outputs of the preceding two cases different? </span><span id="kobo.1761.2" class="koboSpan">You need to derive the Fourier transform of the recovered images to answer this question.</span></p>
<p class="RL"><span id="kobo.1762.1" class="koboSpan">iii. Naive deconvolution analysis</span><br/><span id="kobo.1763.1" class="koboSpan">Express the recovered image from Wiener deconvolution in frequency domain, and implement your own Wiener filter function based on it.</span></p>
<p class="AL"><span id="kobo.1764.1" class="koboSpan">d) Wiener filter</span></p>
<p class="RL-F"><span id="kobo.1765.1" class="koboSpan">  i. Wiener filter algorithm</span><br/><span id="kobo.1766.1" class="koboSpan">Express the recovered image from Wiener deconvolution in frequency domain, and implement your own Wiener filter function based on it.</span></p>
<p class="RL"><span id="kobo.1767.1" class="koboSpan"> ii. Ideal Wiener filter results</span><br/><span id="kobo.1768.1" class="koboSpan">Apply your Wiener filter to </span><i><span id="kobo.1769.1" class="koboSpan">I</span></i><sub><span id="kobo.1770.1" class="koboSpan">noisy</span></sub><span id="kobo.1771.1" class="koboSpan"> (</span><i><span id="kobo.1772.1" class="koboSpan">k, l</span></i><span id="kobo.1773.1" class="koboSpan">). </span><span id="kobo.1773.2" class="koboSpan">Plot your recovered image, and report its PSNR and SSIM scores. </span><span id="kobo.1773.3" class="koboSpan">You can use the actual frequency-dependent SNR(</span><i><span><span id="kobo.1774.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1775.1" class="koboSpan">1</span></sub><span id="kobo.1776.1" class="koboSpan">, </span><i><span><span id="kobo.1777.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1778.1" class="koboSpan">2</span></sub><span id="kobo.1779.1" class="koboSpan">) in this question.</span></p>
<p class="RL"><span id="kobo.1780.1" class="koboSpan">iii. Power spectral density</span><br/><span id="kobo.1781.1" class="koboSpan">Normally, we do not have access to the frequency-dependent SNR(</span><i><span><span id="kobo.1782.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1783.1" class="koboSpan">1</span></sub><span id="kobo.1784.1" class="koboSpan">, </span><i><span><span id="kobo.1785.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1786.1" class="koboSpan">2</span></sub><span id="kobo.1787.1" class="koboSpan">) in real applications. </span><span id="kobo.1787.2" class="koboSpan">Therefore, people usually approximate the SNR(</span><i><span><span id="kobo.1788.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1789.1" class="koboSpan">1</span></sub><span id="kobo.1790.1" class="koboSpan">, </span><i><span><span id="kobo.1791.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1792.1" class="koboSpan">2</span></sub><span id="kobo.1793.1" class="koboSpan">) from a predefined function. </span><span id="kobo.1793.2" class="koboSpan">To explore how to estimate SNR(</span><i><span><span id="kobo.1794.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1795.1" class="koboSpan">1</span></sub><span id="kobo.1796.1" class="koboSpan">, </span><i><span><span id="kobo.1797.1" class="koboSpan">ω</span></span></i><sub><span id="kobo.1798.1" class="koboSpan">2</span></sub><span id="kobo.1799.1" class="koboSpan">), we first analyze the power spectrum of noise and real images. </span><span id="kobo.1799.2" class="koboSpan">Plot the power spectral density of </span><i><span id="kobo.1800.1" class="koboSpan">I</span></i><span id="kobo.1801.1" class="koboSpan"> (</span><i><span id="kobo.1802.1" class="koboSpan">k, l</span></i><span id="kobo.1803.1" class="koboSpan">) and your added noise </span><i><span id="kobo.1804.1" class="koboSpan">e</span></i><span id="kobo.1805.1" class="koboSpan"> (</span><i><span id="kobo.1806.1" class="koboSpan">k, l</span></i><span id="kobo.1807.1" class="koboSpan">) in log scale.</span><sup><a id="fn4x2-bk" href="chapter_2.xhtml#fn4x2"><span id="kobo.1808.1" class="koboSpan">4</span></a></sup><span id="kobo.1809.1" class="koboSpan"> Pick two other images with different scenes in the BSDS500 dataset, and plot these two images together with their log-scale spectral density.</span></p>
<p class="RL"><span id="kobo.1810.1" class="koboSpan">iv. SNR approximation</span><br/><span id="kobo.1811.1" class="koboSpan">On the basis of </span><a id="rfigE2-6" href="chapter_2.xhtml#figE2-6"><span id="kobo.1812.1" class="koboSpan">Figure E2.6</span></a><span id="kobo.1813.1" class="koboSpan">, describe the features of real images and noise. </span><span id="kobo.1813.2" class="koboSpan">Which function would you use to approximate SNR in this case?</span></p>
<div class="figure">
<span id="pg_65"><figure class="IMG"><a id="figE2-6"><span id="kobo.1814.1" class="koboSpan"><img class="img2" src="../images/FigureE2-6.png"/></span>
</a><figcaption><a id="figE2-6"></a><p class="CAP"><a id="figE2-6"><span class="FIGN"></span></a><a href="#rfigE2-6"><span id="kobo.1815.1" class="koboSpan">Figure E2.6</span></a> <span class="FIG"><span id="kobo.1816.1" class="koboSpan">Images of the real world and of noise.</span></span></p></figcaption>
</figure>
</span></div>
<p class="RL"><span id="kobo.1817.1" class="koboSpan">  v. Approximation results</span><br/><span id="kobo.1818.1" class="koboSpan">Plot the deconvolution result using the preceding SNR approximation, and report your PSNR and SSIM scores.</span></p>
<div class="footnotes">
<ol class="footnotes">
<li><p class="FN"><sup><a id="fn1x2" href="chapter_2.xhtml#fn1x2-bk"><span id="kobo.1819.1" class="koboSpan">1</span></a></sup> <a href="https://scikit-image.org/docs/dev/api/skimage.metrics.html"><span id="kobo.1820.1" class="koboSpan">https://</span><wbr/><span id="kobo.1821.1" class="koboSpan">scikit</span><wbr/><span id="kobo.1822.1" class="koboSpan">-image</span><wbr/><span id="kobo.1823.1" class="koboSpan">.org</span><wbr/><span id="kobo.1824.1" class="koboSpan">/docs</span><wbr/><span id="kobo.1825.1" class="koboSpan">/dev</span><wbr/><span id="kobo.1826.1" class="koboSpan">/api</span><wbr/><span id="kobo.1827.1" class="koboSpan">/skimage</span><wbr/><span id="kobo.1828.1" class="koboSpan">.metrics</span><wbr/><span id="kobo.1829.1" class="koboSpan">.html</span></a><span id="kobo.1830.1" class="koboSpan">.</span></p></li>
<li><p class="FN"><sup><a id="fn2x2" href="chapter_2.xhtml#fn2x2-bk"><span id="kobo.1831.1" class="koboSpan">2</span></a></sup> <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html"><span id="kobo.1832.1" class="koboSpan">https://</span><wbr/><span id="kobo.1833.1" class="koboSpan">www2</span><wbr/><span id="kobo.1834.1" class="koboSpan">.eecs</span><wbr/><span id="kobo.1835.1" class="koboSpan">.berkeley</span><wbr/><span id="kobo.1836.1" class="koboSpan">.edu</span><wbr/><span id="kobo.1837.1" class="koboSpan">/Research</span><wbr/><span id="kobo.1838.1" class="koboSpan">/Projects</span><wbr/><span id="kobo.1839.1" class="koboSpan">/CS</span><wbr/><span id="kobo.1840.1" class="koboSpan">/vision</span><wbr/><span id="kobo.1841.1" class="koboSpan">/grouping</span><wbr/><span id="kobo.1842.1" class="koboSpan">/resources</span><wbr/><span id="kobo.1843.1" class="koboSpan">.html</span></a><span id="kobo.1844.1" class="koboSpan">.</span></p></li>
<li><p class="FN"><sup><a id="fn3x2" href="chapter_2.xhtml#fn3x2-bk"><span id="kobo.1845.1" class="koboSpan">3</span></a></sup> <a href="https://docs.scipy.org/doc/numpy/reference/routines.fft.html"><span id="kobo.1846.1" class="koboSpan">https://</span><wbr/><span id="kobo.1847.1" class="koboSpan">docs</span><wbr/><span id="kobo.1848.1" class="koboSpan">.scipy</span><wbr/><span id="kobo.1849.1" class="koboSpan">.org</span><wbr/><span id="kobo.1850.1" class="koboSpan">/doc</span><wbr/><span id="kobo.1851.1" class="koboSpan">/numpy</span><wbr/><span id="kobo.1852.1" class="koboSpan">/reference</span><wbr/><span id="kobo.1853.1" class="koboSpan">/routines</span><wbr/><span id="kobo.1854.1" class="koboSpan">.fft</span><wbr/><span id="kobo.1855.1" class="koboSpan">.html</span></a><span id="kobo.1856.1" class="koboSpan">.</span></p></li>
<li><p class="FN"><sup><a id="fn4x2" href="chapter_2.xhtml#fn4x2-bk"><span id="kobo.1857.1" class="koboSpan">4</span></a></sup><span id="kobo.1858.1" class="koboSpan"> Remember to shift the zero-frequency component to the center of the spectrum by using fftshift in NumPy.</span></p></li>
</ol>
</div>
</div></section>
</section>

</body></html>